{
  "hash": "a1e5f5abcb1a6c41e5982354ab19b100",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"'Vibe Coding' my way into a RAG pipeline\"\nsubtitle: \"Retrieval-augmented generation with a little help from a friend.\"\nauthor: Gio Circo, Ph.D.\ndate: 2025-3-10\ncategories:\n  - Python\n  - Data Science Applications\nformat: \n    html:\n        self-contained: true\n        code-fold: false\n        mainfont: \"Roboto\"\n        section-divs: true\n        toc: true\n        title-block-banner: true\n        mermaid:\n            theme: neutral\ntheme: flatly\nimage: prompt-testing.png\n---\n\n\n*There are synthesized depictions of self-harm and suicide in this blog post.*\n\n## LLMs and RAG\n\nFor most people who are up-to-date in tech, large language models (LLMs) are nothing new. In fact, they are downright pervasive. One of the largest challenges with LLMs today is that they are very good at coming up with plausible-sounding answers, but less good at telling us *why* or *where* they got that answer from. Many times we have to cast a critical eye at LLM output and determine whether or not it is real or just a hallucination. From the beginning, it was suggested that we could augment a user's prompt with a set of relevent documents that could be used to verify the output of the LLM, or improve the prompt itself. For example, imagine we [wrote a prompt](https://chatgpt.com/share/67cf3c21-fd18-8007-a043-6eaec436e169) asking an LLM about [parking restrictions](https://newyork.public.law/laws/n.y._vehicle_and_traffic_law_section_1202) in New York State:\n\n> **Q:** Is it permitted to park between roadways on a highway according to New York State law?\\\n> **A:** According to New York State law, parking between roadways on a highway is not permitted, except where official signs indicate otherwise. Specifically, Section 1200 of the New York Vehicle and Traffic Law states that parking is prohibited in areas such as medians, between divided roadways, and other restricted zones unless otherwise posted.\n\nversus:\n\n> **Q:** Is it permitted to park between roadways on a highway according to New York State law? Review the following rules from the New York Vehicle and Traffic Law. Provide direct quotes to support your answer: *\\[full text of Vehicle & Traffic Law Section 1202\\]*\n>\n> **A:** No, it is not permitted according to New York Vehicle and Traffic Law. The law explicitly states:\n>\n> > *“No person shall stop, stand or park a vehicle... in the area between roadways of a divided highway, including crossovers, except in an emergency.”*\n>\n> This is found under Section 1(i) of the provided rules. The only exception is in the case of an emergency.\n\nThis is a bit of a contrived example, but the general idea here is that we can improve the LLMs by providing them relevant external information alongside their instructions. By including extra information this helps to guard against hallucinations, and can also give the user more guidance on why the LLM came to its specific answer. In our answer above, the LLM is technically correct in the first answer that parking is not permitted - but I think it invents a rule about official signs allowing otherwise. In the prompt containing the full-text of the relevent set of rules, we get a much shorter, cleaner response with the precise rule relevant to the question.\n\n## Coding Out a RAG Pipeline\n\nIn a [recent blog post](https://gmcirco.github.io/blog/posts/prompt-testing/prompt_testing.html) I walked through a step-by-step process of how to set up a A/B testing process for prompt refinement. I relied on data from a recent [DrivenData](https://www.drivendata.org/competitions/295/cdc-automated-abstraction/page/917/) competition that relied on youth suicide narrative reports from the [National Violent Death Reporting System](https://www.cdc.gov/nvdrs/resources/nvdrsCodingManual.pdf). I was pretty happy with the workflow I built out, but couldn't help but feel that I could improve it somehow by giving the LLM direct access to the relevant sections from the coding manual. Here is where RAG comes in! Rather than prompting the model with upwards of 200 pages of text, of which it might need less than 1 page, I could pass just the smallest subsections for each question.\n\n### My mental model\n\nThe way I envisioned this working was to process the RAG step separately by indexing the relevant sub-sections from section 5 of NVDRS coding manual. I would extract out the subsection chunks and then index them in vector database for retreval at the time of prompt creation. My prompt creator class adds the headers, instructions, and questions to the final prompt, and then we tack on the relevant rules from the vector database (see below):\n\n\n```{mermaid}\nflowchart LR\n    %% Improved node styling\n    classDef input fill:#c4e3f3,stroke:#5bc0de,stroke-width:2px,color:#31708f\n    classDef process fill:#d9edf7,stroke:#5bc0de,stroke-width:2px,color:#31708f\n    classDef database fill:#dff0d8,stroke:#5cb85c,stroke-width:2px,color:#3c763d\n    classDef output fill:#fcf8e3,stroke:#f0ad4e,stroke-width:2px,color:#8a6d3b\n    \n    %% Main components with better descriptions\n    A[\"NVDRS Manual<br/>(Source Document)\"] -->|\"Reference material\"| B\n    B[\"RAG Model<br/>(Retrieval System)\"] --> D\n    C[\"Narrative Text<br/>(Case Information)\"] -->|\"Contains: '...victim felt depressed..'\"| D\n    C --> E\n    \n    %% Database and outputs\n    D[(\"Vector Database<br/>(Knowledge Store)\")] -->|\"Retrieved: '5.3.4 Current depressed mood:'\"| F\n    E[\"Prompt Creator<br/>(Question Generator)\"] -->|\"Generates: Q1, Q2, Q3\"| F\n    \n    %% Final output\n    F[\"Final Prompt<br/>(For LLM Processing)\"]\n    \n    %% Apply styles\n    class A,C input\n    class B,E process\n    class D database\n    class F output\n\n```\n\n\nIn my mind, I figured I could come up with a quick and dirty solution by using regex to hit on key words in the narrative, and then use a semantic similarity model (like `SentenceTransformers`) to retrieve the top $n$ rules. For example, a narrative might have a section stating:\n\n> \"Victim had been feeling **depressed** and sad in the days leading up to the incident\"\n\nWe use regex to grab the relevant words around our matched word (here, **depressed**), encode them, and then retrieve rules from the vector database. In the last step we append these to our prompt before executing it.\n\nThere's just one problem - I've never done this before.\n\n### Vibe-Coding\n\nWhat is \"vibe coding\"? One of my favorite definitions comes from ex-OpenAI founder Andrej Karpathy:\n\n> \"There's a new kind of coding I call \"vibe coding\", where you fully give in to the vibes, embrace exponentials, and forget that the code even exists\"\n\nIn short, it represents a programmer's full surrender to the LLM, and taking what it gives back on good faith. When problems arrive, you just dig deeper and let the LLM guide you even further down the rabbit hole, trusting the process. I think the term is very funny - but there is a bit of truth to this. \"Vibe-Coding\" is sort of what I used to do early in grad school when I was trying to get some esoteric model running in R with virtually no background knowledge. To me, vibe-coding harkens back to the days of panicked copy-and-paste from a variety Stack Overflow posts.\n\nWith this in mind, I believe in sharing my work. Here's the [full conversation](https://claude.ai/share/9ae9a888-def0-4e61-96c0-6a795d5d4ad8) I used to set up the RAG framework. I had enough of an idea of what I wanted, but wanted to speed up the code required to get the document chunking and indexing\n\n## Testing the RAG Process\n\nSo what did all that get us? Well, with the help of Claude we got a set of four functions that^[If you are curious about the full code, you can look at my prompt-testing repo under my blog posts that contains the full set]:\n\n1.  Extract the relevant pages from the coding manual.\n2.  Chunk up the pages into subsections based on headers.\n3.  Encode these chunks using a `SentenceTransformers` model.\n4.  Save the embedded chunks and the section indices in a vector database.\n\nas well as two others:\n\n5. A function to query and retrieve results from the vector database.\n6. A function to append the results into a prompt-friendly text object.\n\nI took the code and made very slight adjustments (maybe 10% or less) and put them into their own .py file. I then created a separate file to perform all the steps and locally store the vector database in a cache folder:\n\n::: {#ffe52efc .cell execution_count=2}\n``` {.python .cell-code}\n\"Code to index rules from the NVDRS and store as vector store in cache\"\n\nfrom pypdf import PdfReader\nfrom src.rag import (\n    extract_pages,\n    chunk_by_subsections_with_codes,\n    encode_chunks,\n    create_vector_store,\n)\n\n# import the full nvdrs coding manual\n# we only need a subset of pages on circumstances\n# page 74 - 149\npage_min = 74\npage_max = 148\ncache_dir = \"cache/\"\n\nreader = PdfReader(\"reference/nvdrsCodingManual.pdf\")\n\n# extract pages, chunk subsections, then store in cache\n\npages_circumstances = extract_pages(reader, page_min, page_max)\nsection_circumstances = chunk_by_subsections_with_codes(pages_circumstances)\nsection_embeddings = encode_chunks(section_circumstances)\nindex, stored_chunks = create_vector_store(section_embeddings, cache_dir)\n```\n:::\n\n\nWith that done, the other adjustment I needed to do is add the ability to search the vector database and return the relevant codes based on key works in the narrative. What I did was set up a dict containing key words for each major question, and a query term to append to the retrieved text substring. So, for example, given a narrative like this:\n\n> \"Victim was at home and complained about feeling sad and depressed. Victim had been treated for ADHD and bipolar disorder and had reportedly not been taking his medications in the days preceeding\"\n\nPassing thie example narrative into our query function. We pass this narrative into a `search_vector_database` function that searches for regex hits, encodes matching narrative text, and then queries it against the vector database. We have several key word hits, here, so we get several hits. We take all of results from the vector database search and pass these into another function that prepares it for insertion to the prompt. Essentially the `create_prompt_rules` function adds a header for the section for coding rules, and organizes them in order. The code below shows a successful retreval for the DepressedMood variable:\n\n::: {#6f6b2d98 .cell execution_count=3}\n``` {.python .cell-code}\ntest_narrative = \"Victim was at home and complained about feeling sad and depressed. Victim had told his partner that he was thinking about taking his own life.\"\n\nval, matched_variables = search_vector_database(test_narrative, 1, \"cache/rules_index.faiss\", \"cache/rule_chunks.pkl\")\nPROMPT_RULES = create_prompt_rules(val, matched_variables)\n\nprint(PROMPT_RULES)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIf present, use the following rules to guide your coding of variables. Closely follow these instructions:\n    - Apply ONLY the rules relevant to the question\n    - If a rule is not relevant to the question, disregard it entirely\n    - Do NOT try and apply rules to questions where they are not closely relevant\n\n\n## RULES FOR DepressedMood:\nEvidence found: \"and complained about feeling sad and depressed. Victim had tol\"\n\nRULE 1 [Section 5.3.4]:\n5.3.4 Current depressed mood: CME/LE_DepressedMood \n \nDefinition:  \nVictim was perceived by self or others to be depressed at the time of the injury. \n \nResponse Options: \n0 No, Not Available, Unknown \n1 Yes \n \nDiscussion: \nOnly code this variable when the victim had a depressed mood at the time of injury. There does NOT \nneed to be a clinical diagnosis, and there does not need to be any indication that the depression directly \ncontributed to the death. Other words that can trigger coding this variable besides “depressed” are sad, \ndespondent, down, blue, low, unhappy, etc. Words that should not trigger coding this variable are \nagitated, angry, mad, anxious, overwrought, etc. \n \n If the victim has a known clinical history of depression but had no depressive symptoms at the time \nof the incident, this variable should NOT be selected. \n Depressed mood should not be inferred by the coder based on the circumstances (e.g., because the \nperson reports a bankruptcy); rather it must be noted in the record. \n \nManner of Death: All manners.  \n \n \n\n\n\n```\n:::\n:::\n\n\n## Adding it All Together\n\nNow that I had the LLM stuff mostly incorporated, all I needed to do is append this to my old LLM class. I added an extra parameter named `include_rag` that triggered the RAG process and appended it to the prompt:\n\n::: {#a4df51fb .cell execution_count=4}\n``` {.python .cell-code}\ndef standard_prompt_caching(\n        self,\n        header: str | list = None,\n        narrative: str | list = None,\n        body: str | list = None,\n        example_output: str | list = None,\n        footer: str | list = None,\n        include_rag: bool | list = False,\n        **kwargs\n    ) -> list:\n        \"\"\"Create multiple standard prompts based on all combinations of list elements.\n        This puts the narrative at the end to support OpenAI prompt caching.\n        \"\"\"\n\n        # Ensure all inputs are lists for consistent iteration\n        if include_rag:\n            val, matched_variables = search_vector_database(\n                narrative,\n                2,\n                \"cache/rules_index.faiss\",\n                \"cache/rule_chunks.pkl\",\n            )\n            rag = create_prompt_rules(val, matched_variables)\n            params = [body, example_output, rag, footer, header, narrative]\n        else:\n            params = [body, example_output, footer, header, narrative]\n        param_lists = [\n            [item] if not isinstance(item, list) else item for item in params\n        ]\n```\n:::\n\n\nHere's the crazy thing. It *works*. It works better than I expected. It looked like plausible code to me, but I had no idea if it would actually do what I envisioned. The code to do the chunking, embedding, and indexing took maybe under 30 minutes for me to read through, edit slightly, and execute.\n\n",
    "supporting": [
      "vibe-coding_files"
    ],
    "filters": [],
    "includes": {}
  }
}