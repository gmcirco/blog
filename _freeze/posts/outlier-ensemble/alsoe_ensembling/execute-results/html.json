{
  "hash": "6a6079622da05e3d57395b58cf9a0a6b",
  "result": {
    "markdown": "---\ntitle: \"The Power of Ensembles\"\nsubtitle: \"Adventures in outlier detection\"\nauthor: Gio Circo, Ph.D.\ndate: 2023-2-15\ncategories:\n  - Anomaly Detection\n  - Ensembles\nformat: \n    html:\n        self-contained: true\n        code-fold: true\n        mainfont: \"Roboto\"\n        section-divs: true\n        toc: true\n        title-block-banner: true\ntheme: flatly\nimage: \"comps.png\"\n---\n\nIt's no secret that ensemble methods are extremely powerful tools in statistical inference, data science, and machine learning. It's long been known that many \"imperfect\" models combined together can often perform better than any single model. \n\nFor example, in the [M5 forecasting competition](https://www.sciencedirect.com/science/article/pii/S0169207021001874) almost all of the top performers used some element of model averaging or ensembling. Indeed, the very foundations of some of the most commonly used tools in machine learning, like random forests and boosting, work by averging across many highly biased models to create a single more powerful model. The success of this method is relies on the fact that averaging across many high-variance models generally \nresults in a single, lower-variance model. This is most evident in the idea of the [\"wisdom of the crowd\"](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd), where large groups of individuals are often better at predicting something compared to a single expert. However, one area which hasn't received much attention is **outlier detection**. When we say \"outliers\" we're generally referring to observations that are exceptionally unusual compared to the rest of the sample. A rather consise definition by [Hawkins (1980)](https://link.springer.com/content/pdf/10.1007/978-94-015-3994-4.pdf) states:\n\n> \"An outlier is an observation which deviates so much from the other observations\nas to arouse suspicions that it was generated by a different mechanism.\"\n\nThis rather broad definition fits well with the general application of outlier detection. It can be used for identifying fraud in insurance or healthcare datasets, intrusion detection for computer networks, or flagging anomalies in time-series data - among many others. The specific challenge I want to address in this mini-blog is an ensembling approach for **unsupervised outlier detection**. This is doubly interesting because unsupervised learning presents many more issues compared to supervised learning. Below, I'll contrast some of these differences and then describe an interesting ensemble approach. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# library and data imports\nimport random\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom ensamble_funcs import ALSOe\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import StandardScaler\n\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\n\n# set plotting theme\ncparams = {\n            \"axes.spines.left\": False,\n            \"axes.spines.right\": False,\n            \"axes.spines.top\": False,\n            \"axes.spines.bottom\": False,\n            \"grid.linestyle\": \"--\"\n            }\n\nsns.set_style(\"whitegrid\", rc = cparams)\nsns.set_palette([\"#0077BB\",\"#EE7733\"])\n\n# define some helper functions\ndef eval_preds(yobs, ypred):\n    \"\"\" Print AUC and average precision\n    \"\"\"\n\n    auc = roc_auc_score(yobs, ypred)\n    pre = average_precision_score(yobs, ypred)\n\n    print(f'Roc:{np.round(auc,3)}') \n    print(f'Prn:{np.round(pre,3)}')\n\n# set seed\nrandom.seed(46098)\n\n# load data\ndata = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/6_cardio.npz\")\nX, y = data['X'], data['y']\n\n# Scale input features to mean 0, sd 1\nX = StandardScaler().fit_transform(X)\n\n# Train-test split\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y)\n```\n:::\n\n\n## Outlier Detection with Supervision\n\nTo start, let's look at an example using the `cardio` dataset sourced from the `pyod` [benchmark set](https://github.com/Minqi824/ADBench). In this case we have 1831 observations with 21 variables, of which about 9.6% of them are considered anomalous. These are conviently labeled for us, where a value of `1` indicates an anomalous reading. If we fit a simple random forest classifier we see that it is trivial to get a very high AUC on the test data (let's also not get ahead of ourselves here, as this is a toy dataset with a target that is quite easy to predict). Below we see an example of the fairly strong separation between the inliers and outliers. Our random forest works well in this case - giving us a test AUC of .99 and an average precision of .98. While this is an overly simple example, it does expose how easy some models can be (in many cases) when there is a definite target variable. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# fit a random forest classifier\nrf = RandomForestClassifier()\nrf.fit(Xtrain, ytrain)\n\n# extract the predictions, calculate AUC\nrf_preds = rf.predict_proba(Xtest)[:,1]\n\neval_preds(ytest, rf_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRoc:0.997\nPrn:0.977\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nsns.scatterplot(x = X[:,7], y = X[:,18], hue = y)\n(\n    plt.xlabel(\"Feature 7\"),\n    plt.ylabel(\"Feature 18\"),\n    plt.title(\"Cardio Scatterplot, Inliers and Outliers\")\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Scatterplot of inliers (0) and outliers (1), displaying strong separation](alsoe_ensembling_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\n## Unsupervised Outlier Detection\n\nLet's talk about *unsupervised* outlier detection. Unlike the situation above in an unsupervised setting we don't have the convenience of a set of labels identifying whether a given observation is anomalous or not. And because we're lacking this ground truth, it makes things a *lot* more complicated for choosing both our model(s) and the parameters for those model(s). Let's talk about why.\n\n### How do we select a *best* model?\n\nIn classic supervised learning we can choose a metric to optimize (say, root-mean squared error or log-loss), then fit a model which attempts to minimize that metric. In the simplest case, think about ordinary least squares. In that case we have a simple target of minimizing the sum of squared errors. We can validate the fit of the model by looking at evalution metrics (RMSE, R-Squared, standardized residuals).However, when we lack a way to identify if a given observation is anomalous or not we don't have any meaningful way to know if one model is doing better than another.\n\nYou might also be thinking about optimizing a specific parameter in a model (like the number of nearest neighbors $K$ in a K-nearest neighbors model) using some criterion that doesn't rely on a target variable (like the ['elbow' method](https://en.wikipedia.org/wiki/Elbow_method_(clustering))). However, optimizing this parameter doesn't guarantee that the model itself is useful. Simply put, we're often left groping around in the dark trying to decide what the optimal model or set of parameters is. \n\nLet's consider this example: Say we're looking at the same `cardio` dataset from above and trying to decide what unsupervised outlier detector we want to use. Maybe we're deciding between a distance-based one like exact K-nearest neighbors (KNN) or a density-based one like local outlier factor (LOF). Let's also say we're agnostic to parameter choices, so we stick with the default ones provided by `pyod`.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\n# initalize and fit using default params\nknn = KNN()\nlof = LOF()\n\nknn.fit(X)\nlof.fit(X)\n\n# extract the predictions, calculate AUC\nknn_pred = knn.decision_function(X)\n\neval_preds(y, knn_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRoc:0.686\nPrn:0.286\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# extract the predictions, calculate AUC\nlof_pred = lof.decision_function(X)\n\neval_preds(y, lof_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRoc:0.546\nPrn:0.154\n```\n:::\n:::\n\n\nHere we see that the KNN model performs better than the LOF model - however we didn't adjust any of the $K$ parameters for either model. Because, in practice, we can't see this, we don't know *a-priori* which model or set of parameters will work best in a given case. This is in stark contrast to our first attempt when we could simply focus on decreasing out-of-sample bias.\n\n## An Unsupervised Ensambling Approach\n\nSo, because we can't easily decrease bias (due to the lack of ground truth values) what are our options? Well, as we saw above, there is a large source of *variance* implicit in these models. This variance can come from multiple sources:\n\n1. **Choice of model**. There is considerable variance in how different models perform under different kinds of anomolies. For example, absent some evaluation metric how do you meaningfully choose between KNN, LOF, or one of many other methods. The [pyod benchmark page](https://pyod.readthedocs.io/en/latest/benchmark.html) shows that many models perform quite differently under different types of dimensionality and outlier proportion.\n2. **Choice of parameters**. Almost all anomaly detection models have added uncertaintly based on the choice of parameters. For example, in K-nearest neighbors we need to specify the parameter $K$. One-class support vector machines (SVM) are notoriously difficult to tune in part due to the number of parameters to choose (choice of kernel, polynomial degree, ect...).\n\nTherefore, in an unsupervised model our best option is to try to reduce the variance implicit in both the sources above. Rather than staking our whole model on a single choice of model, or a single set of parameters, we can *ensemble* over a wide number of choices to avoid the risk of choosing a catastrophically bad combination. Since we don't have access to ground truth, this ends up being the safest option (and as we will see, generally produces good results).\n\n### ALSO: A regression-based approach\n\nFor this specific post I'm going to focus on an unsupervised ensemble algothrim proposed by [Paulheim & Meusel (2015)](https://link.springer.com/article/10.1007/s10994-015-5507-y) and further discussed in [Aggarwal & Sathe (2017)](https://link.springer.com/book/10.1007/978-3-319-54765-7). The authors dub this method \"attribute-wise learning for scoring outliers\" or *ALSO*. The approach we are going to use extends the logic of the ALSO model to an ensemble-based approach (hence *ALSO-E*).\n\nLet's talk a bit about the logic here. The general idea of the algothrim is that we iteratively choose a target feature $X_j$ from the full set of features $X$. The chosen $j$ value is used as the target and the remaining $X - j$ features are used to predict $j$. We repeat this for all features in $X$, collecting the standardized model residuals at each step. We then average the residuals across all the models and use them to identify \"anomalous\" observations. In this case, more anomalous observations will likely be ones whose residuals are substantially larger than the rest of the sample. The beauty of this method is that for the modelling portion we can choose *any* base model for prediction (e.g. linear regression, random forests, etc...).\n\nTo avoid models that are very overfit, or have virtually no predictive ability, we define weights for each model using cross-validated RMSE. The goal here is to downweight models that have low predictive ability so they have a proportionally lower effect on the final outlier score. This is defined as\n\n$$w_k = 1 - min(1, RMSE(M_k))$$\n\nwhich simply means that models that perform worse than predicting the mean (which would give us an RMSE of 1) are weighted to 0, while a theoretically \"perfect\" model would be weighted 1. This gives us the added benefit of downweighting features that have little or no predictive value, which helps in cases when we might have one or more irrelevant variables. \n\n### Adding an E to ALSO\n\nThe base algothrim above fits $X$ models using all $n$ observations in the data. However, we can extend this model to an ensambling method by applying some useful statistical tools - namely variable subsambling. The idea proposed by Aggarwal & Sathe (2017) is to define a number of iterations (say, 100), and for each iteration randomly subsamble the data from between $min(1, \\frac{50}{n})$ and $min(1, \\frac{1000}{n})$. This means that each model is fit on a minimum of 50 observations and up to 1000 (or, $n$ if $n$ is less than 1000). In addition, we randomly choose a feature $j$ to be predicted. Combined, this ensambling approach makes a more efficient use of the available data and results in a more diverse set of models. \n\nTo my knowledge, there is no \"official\" implementation of the ALSO-E algothrim, and it is not present in any large libraries (e.g. pyod or sklearn). However the method is generic enough that it is not difficult to code from scrach. Using the notes available I implemented the method myself using a random forest regressor as the base detector. The code below defines a class with a `fit()` and `predict()` function. The `fit()` function handles all the subsampling and fits each sample on a very shallow random forest regressor. The `predict()` function does the work of getting the residuals and re-weighting them according to their CV-error. While my implementation is certainly not \"feature complete\", it's good enough to try out:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# code to fit an ALSOe anomaly detector\n\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nclass ALSOe():\n    \"\"\" Initializes a regression-based outlier detector\n    \"\"\"\n\n    def __init__(self, N = 100) -> None:\n\n        self.param_list = []\n        self.model_list = []\n        self.anom_list = []\n        self.wt = []\n    \n        self.N = N\n        self.std_scaler = StandardScaler()\n\n    def fit(self, data):\n        \"\"\" Fit an ensemble detector\n        \"\"\"\n\n        # standardize data\n        self.std_scaler = self.std_scaler.fit(X = data)\n        data = self.std_scaler.transform(data)\n\n        # fit N models\n        for i in range(0, self.N):\n\n            # define sample space\n            n = data.shape[0]\n            p = data.shape[1]\n            s = [min([n, 50]), min(n,1000)]\n\n            # draw s random samples from dataframe X\n            s1 = np.random.randint(low = s[0], high = s[1])\n            p1 = np.random.randint(low = 0, high = p)\n            ind = np.random.choice(n, size = s1, replace = False)\n\n            # define random y and X \n            df = data[ind]\n            y = df[:,p1]\n            X = np.delete(df, p1, axis=1)\n\n            # initalize RF regressor\n            rf = RandomForestRegressor(n_estimators=10)\n\n            # fit & predict\n            rf.fit(X, y)\n\n            # add fitted models & y param to list\n            self.model_list.append(rf)\n            self.param_list.append(p1)\n\n    def predict(self, newdata):\n\n        \"\"\" Get anomaly scores from fitted models\n        \"\"\"\n\n        # standardize data\n        newdata = self.std_scaler.transform(newdata)    \n\n        for i,j in zip(self.model_list, self.param_list):\n\n            # define X, y\n            y = newdata[:,j]\n            X = np.delete(newdata, j, axis=1)\n\n            # get predictions on model i, dropping feature j\n            yhat = i.predict(X)\n\n            # rmse\n            resid = np.sqrt(np.square(y - yhat))\n            resid = (resid - np.mean(resid)) / np.std(resid) \n\n            # compute and apply weights\n            cve = cross_val_score(i, X, y, cv=3, scoring='neg_root_mean_squared_error')\n            w = 1 - min(1, np.mean(cve)*-1)\n\n            resid = resid*w\n\n            # add weights and preds to lists\n            self.wt.append(w)\n            self.anom_list.append(resid)\n\n        # export results as min-max scaled\n        anom_score = np.array(self.anom_list).T\n        anom_score = np.mean(anom_score, axis = 1)\n\n        # rescale and export\n        anom_score = StandardScaler().fit_transform(anom_score.reshape(-1,1))\n        anom_score = anom_score.flatten()\n\n        return anom_score\n```\n:::\n\n\n#### Fitting the model\n\nWith all this in mind, fitting the actual model is quite simple. As stated above, the ALSO-E approach is largely parameter free, which means there isn't much for the user to worry about. Here we'll just initialize a model with 100 iterations, fit all of the random forest regressors, then get the weighted standardized residuals. This whole process can be condensed into basically 3 lines of code:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\n# Fit an ALSOe regression ensemble\n# using 100 random forests\nad = ALSOe(N = 100)\nad.fit(X)\n\n# extract predictions from the models and combine\n# the standardized outlier scores\nad_preds = ad.predict(X)\n```\n:::\n\n\n#### Evaluating the predictions\n\nNow that we have the predictions, we can look at the distribution of outlier scores. Below we see a histogram of the ensembled scores which, to recall, are rescaled to mean 0 and standard deviation 1. Therefore, the most anomalous observations will have large  positive values. Consistent with what we would expect to see, there is a long tail of large residuals which correspond to the outliers, while the bulk of the data corresponds to a mostly \"normal\" set of values centered around zero.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsns.histplot(x = ad_preds)\n(\n    plt.xlabel(\"Anomaly Score\"),\n    plt.ylabel(\"Observations\"),\n    plt.title(\"ALSO-E Anomaly Scores\")\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Histogram of anomaly scores. The characteristic long tail highlights potential anomalous observations](alsoe_ensembling_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nWe can evaluate the performance of our method by bootstrapping the original dataset 10 times, then running our model on each of the boostrap replicates. This is because there is bound to be some degree of randomness based on the chosen samples and variables for each iteration. Averaging over the bootstrap replicates helps give us some idea of how this model might perform \"in the wild\" so to speak. Below I define a little helper function to resample the dataset, fit the model, and then extract the relevant evaluation metrics. We then loop through the function and put the fit statistics in a set of lists. For evaluation we'll look at the averages for each set.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"false\"}\n# Bootstrap sample from base dataset and evaluate metrics\ndef boot_eval(df):\n    X, y = resample(df['X'], df['y'])\n\n    ad = ALSOe(N = 100)\n    ad.fit(X)\n    ad_preds = ad.predict(X)\n\n    auc = roc_auc_score(y, ad_preds)\n    pre = average_precision_score(y, ad_preds)\n\n    return [auc, pre]\n\n# run models\nroc_list = []\nprn_list = []\n\nfor i in range(10):\n    roc, prn = boot_eval(data)\n\n    roc_list.append(roc)\n    prn_list.append(prn)\n```\n:::\n\n\nShown below, we see we have a decent AUC and average precision score of about `0.71` and `0.26` respectively. While this is substantially lower than the supervised model, it is still better than the base KNN and LOF models above. The ensambling process also makes it easy because we don't have to specify any parameters other than the number of iterations to run. In testing, the default 100 works well as a starting point, and there aren't huge performance gains by increasing it substantially.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nprint(f'Avg. ROC: {np.round(np.mean(roc_list),3) }\\nAvg. Prn: {np.round(np.mean(prn_list),3)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAvg. ROC: 0.693\nAvg. Prn: 0.261\n```\n:::\n:::\n\n\n### Comparing performance across datasets\n\nWe can also evaluate its performance on a variety of other datasets. Here I randomly chose another four datasets from the [pyod benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html) page and compared its performance over 10 bootstrap resamplings to the other benchmarked methods in the `pyod` ecosystem. Looking at the results we see that we get median RocAUC scores of between .7 to .85 and average precision scores between .2 to .85. For an unsupervised model this isn't too bad, and largely falls within the range of other detectors.\n\nWe should note that while its performance is never the best, it is also never the *worst* either. For example: the `.707` we achieved on the `cardio` dataset is lower than some of the best methods (in this case, PCA and Cluster-Based LOF). However, we avoid extremely *bad* results like with Angle-Based Outlier Detection or LOF. This underscores our goals with the ensemble model: we prefer a more conservative model that tends to perform consistently across many types of anomalies. We also avoid issues related to choosing optimal parameters but simply ensambling over many detectors. In an unsupervised case this decrease in variance is especially desirable.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# load additional datasets\nd1 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/14_glass.npz\")\nd2 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/18_Ionosphere.npz\")\nd3 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/20_letter.npz\")\nd4 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/21_Lymphography.npz\")\n\ndlist = [d1,d2,d3,d4]\nmname = ['Cardio', 'Glass','Ionosphere','Letter','Lympho']\n\nroc_list_m = []\nprn_list_m = []\n\n# run models\nfor j in dlist:\n \n    for i in range(10):\n        roc, prn = boot_eval(j)\n\n        roc_list_m.append(roc)\n        prn_list_m.append(prn)\n\n\n# Plot evaluation metrics across datasets\nevaldf = pd.DataFrame({'RocAUC' : roc_list +roc_list_m, \n                       'Precision' : prn_list + prn_list_m,\n                       'Dataset': sorted([x for x in mname*10])})\\\n            .melt(id_vars = 'Dataset', var_name = 'Metric', value_name = 'Score')\n\n# facet plot across datasets\ng = sns.FacetGrid(evaldf, col = 'Metric', sharey = False, col_wrap=1, aspect = 2)\n(\n    g.map(sns.boxplot, 'Dataset','Score', order = mname),\n    g.fig.subplots_adjust(top=0.9),\n    g.fig.suptitle('ALSO-E Model Evaluation', fontsize=16)\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![ensemble models often are often not as good as the best method, but can achieve consistently decent performance.](alsoe_ensembling_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "alsoe_ensembling_files"
    ],
    "filters": [],
    "includes": {}
  }
}