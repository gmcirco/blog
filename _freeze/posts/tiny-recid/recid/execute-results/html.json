{
  "hash": "7273ef37871ff7008b19428152a84c10",
  "result": {
    "markdown": "---\ntitle: \"Don't Evaluate Your Model On a SMOTE Dataset\"\nsubtitle: \"Or: try this one weird trick to increase your AUC\"\nauthor: \"Gio Circo, Ph.D.\"\ndate: 2024-03-26\nformat: \n    html:\n        self-contained: false\n        code-fold: true\n        mainfont: \"Roboto\"\n        section-divs: true\n        toc: true\n        title-block-banner: true\ncategories:\n  - R\n  - Spatial Statistics\ntheme: flatly\n---\n\n\n## The Paper\n\nI recently found a paper published called \"[Advancing Recidivism Prediction for Male Juvenile Offenders: A Machine Learning Approach Applied to Prisoners in Hunan Province](https://books.google.com/books?id=HWD8EAAAQBAJ&newbks=0&printsec=frontcover&pg=PA184&hl=en&source=newbks_fb)\". In it, the authors make use of a very small recidivism data set focusing on youth in Hunan province, which originally appears in a 2017 PLOS ONE article here: \"[Predicting Reoffending Using the Structured Assessment of Violence Risk in Youth (SAVRY): A 5-Year Follow-Up Study of Male Juvenile Offenders in Hunan Province, China](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169251)\". \n\nThe authors of the new study explain how they can use machine learning to improve the prediction of recidivism (which, in of itself is a highly contentious topic). In general, it is a pretty harmless paper of virtually zero significance. They're using an absurdly tiny data set to test out machine learning models that only really work well when you are flush with data. However, a single line stuck out to me when I was scanning their paper:\n\n> \"The proposed ML models perform best on the oversampled dataset, as illustrated in Fig. 2.\"\n\nUh oh.\n\nLooking at the associated figure and table they show off some impressive metrics. Their random forest model has a precision and recall of 97%! On a recidivism prediction task this alone is highly suspicious. For example, in the competition I participated (and won in several categories), our model only averaged about .78 to .8. Why is theirs so good here? Well, it's all about that line above. Let's run through the data and explain:\n\n![](table_4.png)\n\n\n### The Data\n\nBelow, I pull  the original data set from the PLOS One paper (thanks to the authors for making it publically available!). There is a single missing value for education, which I impute to the median value. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(tidyverse)\nlibrary(pROC)\nlibrary(glmnet)\nlibrary(randomForest)\nlibrary(smotefamily)\n\nset.seed(978545)\n\n# load data\n# impute single missing value with median of education (8)\ndf <-\n  haven::read_sav(\"../../../data/savry.sav\") %>%\n  select(Reoffending, age, education, familyincome, contains(\"SAVRY\"), P1:P6) %>%\n  replace_na(list(education = 8)) %>%\n  mutate(across(familyincome:P6, as.factor))\n\n\nTEST_PROP = .2\nN = nrow(df)\n```\n:::\n\n\n## Doing it the Normal Way\n\nThe normal workflow in this case is to set up a test-train split, fit a model on\nthe training data set, then  evaluate the out-of-sample performance on the test\nset. Despite the very small sample size here I'll just follow the bog standard\napproach to illustrate. A 20% test set gives us: $246 - (246*.2) = 196.8$. So \njust under 200 cases to train on, which is really *quite* small.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# indices of train-test\ntest_size <- round(N*TEST_PROP)\ntest_idx <- sample(1:N, test_size)\ntrain_idx <- setdiff(1:N, test_idx)\n\n# set up y and X\n# one-hot encoding categorical vars\ny <- df$Reoffending\nX <- model.matrix(~ . - 1, data = df[-1])\n\n# test-train splits\ny_test = y[test_idx]\nX_test = X[test_idx,]\ny_train = y[train_idx]\nX_train = X[train_idx,]\n```\n:::\n\n\nDespite the authors of the previous paper using some boosting methods, this data\nis far, far too small to make use of those approaches usefully. Here, I'm just\nfitting a logistic regression with no regularization and a random forest with \n500 trees.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# fit multiple linear models\n# logit, no regularization & random forest\nfit_1_glm <- glmnet(X_train,y_train, family = \"binomial\", alpha = 0, lambda = 0)\nfit_1_rf <- randomForest(X_train ,as.factor(y_train))\n```\n:::\n\n\nNow we just evaluate the area under the curve:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\npred_1_glm <- as.numeric(predict(fit_1_glm, X_test, type = 'response'))\npred_1_rf <- as.numeric(predict(fit_1_rf, X_test, \"prob\")[,2])\n\n# get auc\nroc(y_test, pred_1_glm, quiet = T)$auc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.7482\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nroc(as.factor(y_test), pred_1_rf, quiet = T)$auc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.7197\n```\n:::\n:::\n\n\nSo about .74 for the logistic regression and .72 for the random forest. Not \ngreat, not terrible.\n\n## Doing it with SMOTE\n\nSo the argument with SMOTE is that training models on data sets with very large\nimbalances in positive vs. negative cases is that the models only learn from\nthe negative cases and not the positive ones. A good example might be a fraud\ndata set where you have 100,000 legitimate credit card transactions and only 500\ncases of fraud (so something like .5%). SMOTE is intended to help with training\na model by synthesizing a balanced data set where the ratio of positive to \nnegative cases are much closer to 50/50. Without going too much into it, this\nactually rarely solves and problems and often induces some.\n\nWhat I suspect the authors of this paper did is that they generated a SMOTE \ndata set with a balanced ratio of positive to negative cases, then created a \ntest-train split from *that* data set, and evaluated their metrics on a test \ndata set derived from the SMOTE model. That is very, *very* wrong.\n\n### Doing SMOTE the wrong way\n\nSo let's try it. I'll synthesize a SMOTE data set from the full set of cases, \nthen walk through the whole process using *only* the synthesized data. This\ncreates a data set with 126 new observations, and brings the balance of positive\nto negative cases to almost exactly 50/50 (rather than 25/75 in the original).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# create a smote dataset from the FULL dataset, then split\nsmote_df_full <- data.frame(X,y)\nsmote_model_full <- SMOTE(smote_df_full[-63], target = smote_df_full[63], dup_size = 2)\n\nX_smote <- smote_model_full$data[-63]\ny_smote <- as.numeric(smote_model_full$data$class)\n\ntable(y_smote)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ny_smote\n  0   1 \n183 189 \n```\n:::\n:::\n\n\nNow we just pull a test-train split on the SMOTE data, then fit the models.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# indices of train-test\ntest_idx_smote <- sample(1:N, test_size)\ntrain_idx_smote <- setdiff(1:N, test_idx_smote)\n\n# test-train splits\ny_test_smote = y_smote[test_idx_smote]\nX_test_smote = as.matrix(X_smote[test_idx_smote,])\ny_train_smote = y_smote[train_idx_smote]\nX_train_smote = as.matrix(X_smote[train_idx_smote,])\n\n# fit and evaluate models\nfit_2_glm_smote <- glmnet(X_train_smote,y_train_smote, family = \"binomial\", alpha = 0, lambda = 0)\nfit_2_rf_smote <- randomForest(X_train_smote ,as.factor(y_train_smote))\n\npred_2_glm_smote <- as.numeric(predict(fit_2_glm_smote, X_test_smote, type = 'response'))\npred_2_rf_smote <- as.numeric(predict(fit_2_rf_smote, X_test_smote, \"prob\")[,2])\n\n# get auc\nroc(y_test_smote, pred_2_glm_smote, quiet = T)$auc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.8408\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nroc(as.factor(y_test_smote), pred_2_rf_smote, quiet = T)$auc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.9765\n```\n:::\n:::\n\n\nWow! Look at that, we just increased our AUC for the random forest model from \n.72 to .97! But what if we do what we're supposed to and see how it works on real out-of \nsample data? \n\n### Doing SMOTE the (less) wrong way\n\nSame as above, except we create a SMOTE data set from our original training data\nand then we evaluate our model on the original *test* data set that is not \nsynthetically balanced.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsmote_df <- data.frame(X_train,y_train)\nsmote_model <- SMOTE(smote_df[-63], target = smote_df[63], dup_size = 2)\n\nX_smote <- smote_model$data[-63]\ny_smote <- as.numeric(smote_model$data$class)\n\nfit_3_glm_smote <- glmnet(X_smote,y_smote, family = \"binomial\", alpha = 0, lambda = 0)\nfit_3_rf_smote <- randomForest(X_smote ,as.factor(y_smote))\n\npred_3_glm_smote <- as.numeric(predict(fit_3_glm_smote, X_test, type = 'response'))\npred_3_rf_smote <- as.numeric(predict(fit_3_rf_smote, X_test, \"prob\")[,2])\n\nroc(y_test, pred_3_glm_smote, quiet = T)$auc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.7077\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nroc(as.factor(y_test), pred_3_rf_smote, quiet = T)$auc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.7335\n```\n:::\n:::\n\n\nOh.\n\n## Summary\n\nIn summary, if you wrongfully evaluate your model that was trained on a SMOTE \ndata set against a hold-out sample from that same SMOTE data your out-of-sample\nmetrics will be falsely confident. It is much easier to perform classification \non data that are artificially balanced. However, actually using these models in\nreal life entails data that almost never follow this.\n\nFinally, I don't mean to focus on these authors specifically. The analysis they \nare doing is with some good intentions, but is mostly misguided. The data here\nare mostly unsuited for examining more complex models and processes. In addition,\nwhat I see here is a common issue for many data analysts, which is why being \nthoughtful and careful at the start of your analysis is very important.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}