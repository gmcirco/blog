{
  "hash": "ebf62cd4c816d0b65d5dfcf9376c5836",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Going Back to (Bayesian) School\"\nsubtitle: \"Self-study with Regression and Other Stories\"\nauthor: \"Gio Circo, Ph.D.\"\ndate: 2024-8-05\nformat: \n    html:\n        self-contained: false\n        code-fold: true\n        mainfont: \"Roboto\"\n        section-divs: true\n        toc: true\n        title-block-banner: true\ncategories:\n  - R\n  - Bayesian\ntheme: flatly\nimage: duck.jpg\n---\n\n\n## Working with Bayesian Statistics\n\n\n\n\n\n## A Basic Regression Example\n\n### The Hibbs \"Bread and Peace\" Model\n\nThe data below is part of Douglas Hibb's [\"Bread and Peace\"](https://www.jstor.org/stable/pdf/30026466.pdf?casa_token=ndkpZ4OA8qsAAAAA:wmCzUl5_lrseqgvVh1QljGQcY4zt7RjL6qVVDpVx3FP7cH11_L_GYRP82RTjht1Q6jC8UpDiKVv97q1KAcvOzdj71tnb-YRpr_VKxlMiVoXrjuF0xzw) model of U.S. elections. The data below show the proportion of the 2-party vote share for the incumbent party against a measure of personal income growth during that party's tenure. As seen in the plot, there appears to be a fairly strong relationship between the economic status under the incumbent party and their share of the 2 party vote.\n\n::: panel-tabset\n\n## \"Bread and Peace\" Raw Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot1.1 <- \n  ggplot(election) +\n  geom_point(aes(x = growth, y = vote), size = 2, shape = 22, fill = '#004488', color = '#004488') +\n  labs(x = \"Growth in Personal Income\", y = \"Incumbent vote share\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 11))\nplot1.1\n```\n\n::: {.cell-output-display}\n![Growth in personal income is associated with higher vote shares for the incumbent party](bayes-study-1_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## \"Bread and Peace\" by Year\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot1.2 <- \n  ggplot(election) +\n  geom_text(aes(x = growth, y = vote, label = year), size = 3.5, fontface = 'bold', color = '#004488') +\n  labs(x = \"Growth in Personal Income\", y = \"Incumbent vote share\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 11))\nplot1.2\n```\n\n::: {.cell-output-display}\n![The 1980 election saw Jimmy Carter lose decsively to Ronald Regan amid a floundering economy.](bayes-study-1_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n:::\n\nTherefore, we have the simple regression equation:\n\n$$y = \\beta_0 + \\beta_{1}income + \\epsilon$$\nWhere the expected two party vote share $y$ is assumed to be a function of the growth in personal income growth $\\beta_1$ plus unmeasured residual error $\\epsilon$. So far this isn't any different than your bog-standard `lm(y~x)` type model. The difference is how a Bayesian model handles the model parameters. In short, while a frequentist model views parameters as \"fixed\", a Bayesian model views them as random variables modeled as draws from a posterior distribution. This posterior distribution is formed from a likelihood (the observed data), and a prior distribution. \n\nBecause this isn't meant to be a full introduction to Bayesian inference, feel free to review some good articles [here](http://www.stat.columbia.edu/~gelman/research/published/43586_2020_1_Author.pdfand), or [here](https://arxiv.org/pdf/2011.01808).\n\n### The Bayesian approach\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's regress!\nfit2_prior <- c(prior(normal(5, 2), class = 'b'))\n\n# model 1 has default (flat) priors on betas\n# 1 percentage point in growth is associated with ~ 3% increase in vote share\nfit1 <-\n  brm(vote ~ growth,\n      data = election,\n      family = \"gaussian\",\n      file = \"hibbs_fit1.Rdata\")\ntidy(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 8\n  effect   component group    term         estimate std.error conf.low conf.high\n  <chr>    <chr>     <chr>    <chr>           <dbl>     <dbl>    <dbl>     <dbl>\n1 fixed    cond      <NA>     (Intercept)     46.2      1.75     42.6      49.7 \n2 fixed    cond      <NA>     growth           3.08     0.762     1.54      4.57\n3 ran_pars cond      Residual sd__Observa…     4.05     0.813     2.82      5.97\n```\n\n\n:::\n\n```{.r .cell-code}\n# same model, with much tighter priors\n# assuming a mean effect of about 2.5% +- 1.5%\nfit2 <-\n  brm(\n    vote ~ growth,\n    data = election,\n    family = \"gaussian\",\n    prior = fit2_prior,\n    file = \"hibbs_fit2.Rdata\"\n  )\ntidy(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 8\n  effect   component group    term         estimate std.error conf.low conf.high\n  <chr>    <chr>     <chr>    <chr>           <dbl>     <dbl>    <dbl>     <dbl>\n1 fixed    cond      <NA>     (Intercept)     45.7      1.67     42.4      48.9 \n2 fixed    cond      <NA>     growth           3.30     0.716     1.90      4.75\n3 ran_pars cond      Residual sd__Observa…     4.01     0.818     2.83      5.97\n```\n\n\n:::\n:::\n\n\n## Predictions for a new value of `x`\n\nWe can create a prediction for `y` given a new value of `x`. For example, what would the expected vote share be, given the growth in personal income was 2%? Here we can access the posterior distribution for a prediction of `y`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# what is the predicted vote share given a 2% growth rate?\n# 52%, but with a pretty big margin of error\nnewgrowth = 2.0\nnewprobs = c(.025, .25, .75, 0.975)\n\npred1 <- posterior_predict(fit1, newdata = data.frame(growth=newgrowth))\n\nypred = mean(pred1)\nypred_quantile = quantile(pred1, newprobs)\n```\n:::\n\n\nThis prediction itself is constructed as a distribution from the posterior, which gives us a range of values for the prediction. Below we see the mean prediction for incumbent vote share where $y=2$ is 52.3, but this could plausibly range between as low as 44 or as high as 61.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data.frame(x = pred1)) + geom_density(aes(x = x), linewidth = 1, color = '#DDAA33', fill = '#DDAA33', alpha = .2) +\n  labs(x = \"Predicted Vote Share (y=2)\", y = \"Probability Density\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 11))\n```\n\n::: {.cell-output-display}\n![](bayes-study-1_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the 50% and 95% credible intervals for the point estimate of 2% growth\nplot1.1 +\n  annotate(geom = \"linerange\", x = newgrowth, ymin = ypred_quantile[2], ymax = ypred_quantile[3], color = '#DDAA33',  linewidth = 1) +\n  annotate(geom = \"linerange\", x = newgrowth, ymin = ypred_quantile[1], ymax = ypred_quantile[4], color = '#DDAA33') +\n  annotate(geom = \"point\", x = newgrowth, y = ypred, fill = '#DDAA33', color = 'white', stroke = 2, size = 3, shape = 21)\n```\n\n::: {.cell-output-display}\n![](bayes-study-1_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe can also access simulations for the intercepts and slopes of each model.\n\n::: panel-tabset\n\n## Flat prior\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_posterior(fit1)\n```\n\n::: {.cell-output-display}\n![](bayes-study-1_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Informative prior\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_posterior(fit2)\n```\n\n::: {.cell-output-display}\n![](bayes-study-1_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n:::",
    "supporting": [
      "bayes-study-1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}