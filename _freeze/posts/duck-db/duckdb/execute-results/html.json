{
  "hash": "793408a25a78c0429016630a24fd2c55",
  "result": {
    "markdown": "---\ntitle: \"Dear Crime Analysts: Why You Should Use SQL Inside of R\"\nsubtitle: \"Using duckDB in R to speed up analysis\"\nauthor: \"Gio Circo, Ph.D.\"\ndate: 2024-7-23\nformat: \n    html:\n        self-contained: false\n        code-fold: false\n        mainfont: \"Roboto\"\n        section-divs: true\n        toc: true\n        title-block-banner: true\ncategories:\n  - R\n  - SQL\ntheme: flatly\n---\n\n\n## The One Big Thing I *Didn't* Learn In Grad School\n\n\n\n\n\nWhen I was in grad school working on my Ph.D. I learned a lot about math,\nstatistics, research methods, and experimental design (among a LOT of other things).\nFor a good part of my time as a grad student I also worked doing crime analysis at\nthe Detroit Police Department for [Ceasefire](https://www.tandfonline.com/doi/abs/10.1080/24751979.2020.1827938) and [Project Green Light](https://link.springer.com/article/10.1007/s11292-019-09404-y).\nHowever, looking back, I realize one skill I never learned, that has become invaluable\ntoday, is something I never would have guessed: SQL. Yes, *that* SQL. \n\nFor my academic friends who aren't in the know, SQL stands for \"Structured Query\nLanguage\" and is the number one way that analysts interface with data stored in \ndatabases. SQL is great because it is a fast and efficient way to pull data out\nof very large and complex tables. In addition it doesn't require you to read\nan entire table into memory. For reference, at my day job I typically work with \nmedical claims data tables with *billions* of records. It is simply not \npossible (nor recommended) to work with the entire table in-memory.\n\nDuring grad school my typical workflow was to try and load a single large data\ntable into R and work with it directly, or manually break it into smaller .csv\nfiles. Not only is this highly inefficient, it also makes it difficult to \nreplicate the workflow later. I think being able to work with large complex \ndatasets is increasingly important for researchers who want to take control of\ntheir workflow. \n\n## DuckDB and R\n\nThere are a lot of different ways to interface with SQL. In earlier projects\nI've used a SQLite database to manage a very large dataset and then query it \nfrom R. However, this approach requires you to create a `.sqlite` database and\nadds a bit of up-front work. Often times I might just have one or two very large\ntables where this approach is a bit overkill. For example, working with raw \nNIBRS data entails only a few important tables (victim, offender, offense) but\neach table is far too large to work with directly. \n\n[DuckDB](https://duckdb.org/) is a great option here because it has a ton of \nvery useful functions that allow you to read directly from a `.csv` or other type of \nfile (JSON, Parquet,etc...). In addition this can work directly in `R` using a\n[client API](https://duckdb.org/docs/api/r). For an academic, we often only have simple tables \nlike these to work with and so having an option that we can easily integrate into \nour workflow is really appealing.\n\n\n### Setting it up\n\nAs an example, I have some crime data from Detroit that I used for a project\na few years back. The size of this file is large (although not *that* large). \nHowever it is big enough that it might be reasonable to pull only a subset of\nthe data into memory at a time. Here's a perfect use-case for duckDB. Below,\nI start by loading the `duckdb` library and setting up the working directory\nof the file location `dir` as a string. This will make it a bit cleaner to read \nand pass in to our queries when we start working.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(sf)\n\nwd <- \"../Documents/blog/data/\"\nfile_name <- \"crime.csv\"\ndir <- paste0(wd,file_name)\n```\n:::\n\n\nWe then set up a connection object via `dbConnect` and use `duckdb()` as the \nconnector. After we do that all we need to do is built a SQL string and pass it \nin. The script below reads all the columns (`SELECT *`) from the table listed in\nthe directory `crime.csv` and pulls only the top 10 rows. And this builds the connection and executes the query:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon = dbConnect(duckdb())\ndbGetQuery(con, sprintf(\"SELECT * FROM read_csv('%s') LIMIT 10\", dir))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         category       offense       crno       date           address\n1           FRAUD FRAUD (OTHER) 1501070062 2015-01-07     02600 E 8MILE\n2  STOLEN VEHICLE VEHICLE THEFT 1501310051 2015-01-31     15300 VAUGHAN\n3           FRAUD FRAUD (OTHER) 1503160171 2015-02-01     10000 GEORGIA\n4  STOLEN VEHICLE VEHICLE THEFT 1502020211 2015-02-02 14900 E JEFFERSON\n5           FRAUD FRAUD (OTHER) 1504090191 2015-02-02    20500 SAN JUAN\n6         ASSAULT  INTIMIDATION 1502090035 2015-02-09    14400 FREELAND\n7  STOLEN VEHICLE VEHICLE THEFT 1502240058 2015-02-23   15700 KENTFIELD\n8           FRAUD FRAUD (OTHER) 1502240128 2015-02-24   19300 PINEHURST\n9           FRAUD FRAUD (OTHER) 1502270221 2015-02-25      00100 SEWARD\n10          FRAUD FRAUD (OTHER) 1502270182 2015-02-25    19100 KEYSTONE\n        lon     lat     yr_mon crime_type\n1  -83.0759 42.4466 2015-01-01   property\n2  -83.2380 42.4021 2015-01-01   property\n3  -83.0042 42.3961 2015-02-01   property\n4  -82.9388 42.3746 2015-02-01   property\n5  -83.1480 42.4453 2015-02-01   property\n6  -83.1846 42.3945 2015-02-01   disorder\n7  -83.2407 42.4055 2015-02-01   property\n8  -83.1670 42.4333 2015-02-01   property\n9  -83.0786 42.3748 2015-02-01   property\n10 -83.0510 42.4341 2015-02-01   property\n```\n:::\n:::\n\n\nVoila! As an aside, if you are more familiar with `dplyr`'s syntax, the equivalent\ncode would be. This is a bit less verbose, but requires you to read in the *entire*\ntable before selecting just the top 10 rows. It is vastly less efficient and\nslow in cases where the table sizes become very large.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_csv(dir) %>%\n  slice(1:10)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 321983 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): category, offense, address, crime_type\ndbl  (3): crno, lon, lat\ndate (2): date, yr_mon\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 9\n   category  offense   crno date       address   lon   lat yr_mon     crime_type\n   <chr>     <chr>    <dbl> <date>     <chr>   <dbl> <dbl> <date>     <chr>     \n 1 FRAUD     FRAUD … 1.50e9 2015-01-07 02600 … -83.1  42.4 2015-01-01 property  \n 2 STOLEN V… VEHICL… 1.50e9 2015-01-31 15300 … -83.2  42.4 2015-01-01 property  \n 3 FRAUD     FRAUD … 1.50e9 2015-02-01 10000 … -83.0  42.4 2015-02-01 property  \n 4 STOLEN V… VEHICL… 1.50e9 2015-02-02 14900 … -82.9  42.4 2015-02-01 property  \n 5 FRAUD     FRAUD … 1.50e9 2015-02-02 20500 … -83.1  42.4 2015-02-01 property  \n 6 ASSAULT   INTIMI… 1.50e9 2015-02-09 14400 … -83.2  42.4 2015-02-01 disorder  \n 7 STOLEN V… VEHICL… 1.50e9 2015-02-23 15700 … -83.2  42.4 2015-02-01 property  \n 8 FRAUD     FRAUD … 1.50e9 2015-02-24 19300 … -83.2  42.4 2015-02-01 property  \n 9 FRAUD     FRAUD … 1.50e9 2015-02-25 00100 … -83.1  42.4 2015-02-01 property  \n10 FRAUD     FRAUD … 1.50e9 2015-02-25 19100 … -83.1  42.4 2015-02-01 property  \n```\n:::\n:::\n\n\n### Other Tricks - Aggregations and Plotting\n\n\nOf course SQL is a very robust scripting language that allows for both simple and \ncomplex operations. We can do any kind of reporting and aggregations. For example, \nif we wanted some basic information about crime at the year-month level we could\ndo:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqu = \n  \"SELECT\n      crime_type,\n      yr_mon,\n      COUNT(crime_type) AS N\n  FROM\n      read_csv('%s')\n  GROUP BY\n      crime_type,\n      yr_mon\n  ORDER BY\n      crime_type,\n      yr_mon\"\n\ntab <- dbGetQuery(con, sprintf(qu, dir))\nhead(tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  crime_type     yr_mon    N\n1   disorder 2015-01-01 1518\n2   disorder 2015-02-01 1447\n3   disorder 2015-03-01 1797\n4   disorder 2015-04-01 1884\n5   disorder 2015-05-01 2125\n6   disorder 2015-06-01 1839\n```\n:::\n:::\n\n\nThis performs the group-by and counts out of memory and then moves the aggregated\ntable right into R as a dataframe. And since the result is just a dataframe, we can \npipe it directly into a ggplot visualization, like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbGetQuery(con, sprintf(qu, dir)) %>%\n  ggplot() +\n  geom_line(aes(x = yr_mon, y = N, color = crime_type), linewidth = 1) +\n  facet_wrap(~crime_type, scales = \"free\", ncol = 2) +\n  labs(x = \"Year-Month\", y = \"Count\") +\n  scale_color_manual(values = c('#004488', '#DDAA33', '#BB5566')) +\n  theme_minimal() +\n  theme(legend.position = 'none')\n```\n\n::: {.cell-output-display}\n![](duckdb_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nSimilarly, this applies to other functions like creating spatial objects. What\nif we wanted to plot only the violent crimes from the first month of 2015?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqu =\n  \"SELECT *\n  FROM \n      read_csv('%s')\n  WHERE \n      crime_type = 'violent'\n  AND\n      datepart('year', yr_mon) = 2015\n  AND\n      datepart('month', yr_mon) = 1\n\"\n\nst_as_sf(dbGetQuery(con, sprintf(qu, dir)),\n         coords = c('lon', 'lat'),\n         crs = 4326) %>%\n  ggplot() +\n  geom_sf(shape = 21, alpha = .2, fill = '#BB5566') +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](duckdb_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis is pretty cool too, because we have access to all the base functions available in duckDB. For example, there are a bunch of [date handling functions](https://duckdb.org/docs/sql/functions/date.html) that make these types of queries a lot easier than other base SQL languages. Here, the `datepart` function lets us split up date objects very easily within SQL.\n\n\n",
    "supporting": [
      "duckdb_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}