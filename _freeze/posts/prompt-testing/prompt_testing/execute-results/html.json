{
  "hash": "f98b4ae3048b40cd5c67f93fdb37690c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"An A/B Testing Approach to Prompt Refinement\"\nsubtitle: \"Testing a text extraction model using ChatGPT\"\nauthor: Gio Circo, Ph.D.\ndate: 2025-2-24\ncategories:\n  - Python\n  - Data Science Applications\nformat: \n    html:\n        self-contained: true\n        code-fold: false\n        mainfont: \"Roboto\"\n        section-divs: true\n        toc: true\n        title-block-banner: true\n        mermaid:\n            theme: neutral\ntheme: flatly\nimage: prompt-testing.png\n---\n\n\n\n\n\n*There are synthesized depictions of self-harm and suicide in this blog post.*\n\n## What's up with Prompts?\n\nI am still a relative novice to large-language models (LLMs) when it comes to non-trivial tasks (like asking ChatGPT to summarize an email). Applying these models for complex real-world tasks is not nearly as simple. In fact, I think applying LLMs to solve business-problems is still new enough that I am not sure how many people can claim bonda-fide expertise here. Regardless, in my work I've been increasingly asked to use LLMs to automate the processing of large volumes of free-form text and extract structured output. Luckily, this is one task that LLMs are actually well-suited for, unlike many of the very silly attempts to plug it in where it is clearly *not* useful.\n\nIn my ongoing work, I have learned a few things. One of the biggest is that LLMs are *very* sensitive to the prompt they are given. Often, it can be a matter of wording, question structure, or even where the prompt is [placed relative to the input](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips). A few of the LLM companies, like Anthropic, even have some interesting suggestions: like structuring long documents with XML tagging to block off important sections. Outside of some of these more esoteric suggestions, I've found that common good practice for creating LLM prompts generally follows these rules:\n\n1.  Be *extremely* specific\n2.  Spell out tasks using bullets or numbered points\n3.  Provide examples\n\nThis could be as simple as the difference between:\n\n> \"Summarize this legal document\"\n\nor\n\n> \"Summarize this housing contract in 3 to 5 sentences. Include a section with numbered bullet points for the costs.\"\n\nOne gives much more latitude to the LLM to do whatever it thinks is right, compared to the other which has structure and key instructions. Generally you will get better results the less you let the LLM fill in the blanks on its own.\n\n## Setting up a testing framework\n\nWith that in mind, let's talk a bit about how I think about prompting.\n\n### My \"mental model\" of prompting\n\nWhen I think of a prompt, I tend to conceptualize it as containing a few key pieces:\n\n1.  A **role** which defines the key tasks of the LLM and provides it some structure to its intended job.\n2.  A **header** which contains some initial instructions guiding the next steps in the prompt.\n3.  A **body** which has the bulk of the instructions, text, and other materials that make up the primary tasks of the prompt.\n4.  An **example** of what the intended output is supposed to look like. In a structured text extraction task, this would be the format of the json or other file I want created.\n5.  An optional **footer** that contains some final instructions. Often times I have found these useful to provide a list of things I definitively do NOT want the LLM to do.\n\n### A python framework\n\nApplying this to an automated workflow in python can be pretty easily actually. Since we're just working with text prompts, all I need to do is create a system that concats all the necessary pieces together in a way that creates a full prompt. Below I set up a class to help test multiple prompt versions in a somewhat quicker fashion than creating each one individually. My goal here is to have the ability to test many different prompts, in different configurations, without having to manually copy-and-paste these examples and store them. To accomplish this, I first built out a small prompt creation class:\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\"}\nfrom itertools import product\n\nclass Prompt:\n    def __init__(self):\n        pass\n\n    def prompt_concat(self, text_list):\n        \"\"\"Concat a list of text, dropping None values\"\"\"\n        output_text = \"\\n\"  .join(filter(None, text_list))\n        output_text += \"\\n\"\n\n        return output_text\n\n    def standard_prompt(\n        self,\n        header: str | list = None,\n        narrative: str | list = None,\n        body: str | list = None,\n        example_output: str | list = None,\n        footer: str | list = None,\n        **kwargs\n    ) -> list:\n        \"\"\"Create multiple standard prompts based on all combinations of list elements.\"\"\"\n        \n        # Ensure all inputs are lists for consistent iteration\n        params = [header, narrative, body, example_output, footer]\n        param_lists = [[item] if not isinstance(item, list) else item for item in params]\n        \n        # unpack params, then pass to concat\n        prompt_combinations = product(*param_lists)\n        prompts = [self.prompt_concat(combination) for combination in prompt_combinations]\n        \n        return prompts\n    \n    def standard_prompt_caching(self,\n        header: str | list = None,\n        narrative: str | list = None,\n        body: str | list = None,\n        example_output: str | list = None,\n        footer: str | list = None,\n        **kwargs\n    ) -> list:\n        \"\"\"Create multiple standard prompts based on all combinations of list elements.\n        This puts the narrative at the end to support OpenAI prompt caching.\n        \"\"\"\n        \n        # Ensure all inputs are lists for consistent iteration\n        params = [body, example_output, footer, header, narrative]\n        param_lists = [[item] if not isinstance(item, list) else item for item in params]\n        \n        # unpack params, then pass to concat\n        prompt_combinations = product(*param_lists)\n        prompts = [self.prompt_concat(combination) for combination in prompt_combinations]\n        \n        return prompts\n    \n    def unstructured_prompt(self, prompt_text_list: list[str])-> str:\n        \"\"\"Create an unstructured prompt, given a list of text\"\"\"\n        return self.prompt_concat([prompt_text_list])\n```\n:::\n\n\n\nAll this class really does is take text strings and pastes them together using my mental framework. The trick here is that you can pass in a list for any of the parameters, and then we use `itertools.product()` to create all the possible combinations. For example, passing in 2 versions of a header, body, and example would give you $2^3=8$ different combinations of the prompts. Also, here I add a \"narrative\" field which will serve as the input text for each text narrative we will be extracting data from.\n\n## Classifying Youth Suicide Narratives\n\nThe example problem here is based on a competition that was hosted by the CDC last year on [DrivenData](https://www.drivendata.org/competitions/295/cdc-automated-abstraction/). The stated goal of the contest was to extract structured information from free-text narratives derived from police and medical examiner reports. The free text reports look something like this simulated example below:\n\n*\\[Simulated Example\\]*\n\n> V was a YYYY. V was discovered deceased at home from a self-inflicted gunshot wound to the head. No medical history was documented in the report. According to V's close friend and coworker, V had struggled with periods of severe anxiety but had never spoken about self-harm. V's friend mentioned that several years ago, V had driven recklessly after a personal loss, but it was unclear whether it was an intentional act. On the night of the incident, V had been drinking and sent a message to a relative expressing regret and affection for them and other family members. Toxicology results confirmed the presence of alcohol. No additional details regarding the event were available.\n\nThe contest required taking the short narratives and extracting 24 different variables based on definitions from the [national violent death reporting system](https://www.cdc.gov/nvdrs/resources/nvdrscodingmanual.pdf?CDC_AAref_Val=https://www.cdc.gov/violenceprevention/pdf/nvdrs/nvdrsCodingManual.pdf). Most of these are [binary indicators](https://www.drivendata.org/competitions/295/cdc-automated-abstraction/page/917/) about specific behaviors observed in the narrative. Therefore, we need to create a prompt that will instruct our LLM to read the narratives, extract features based on the rules for each of the 24 features, then return the output back to us in a format that we can use for scoring.\n\n### Prompt creation\n\nAs a demonstration of what this testing can look like, my idea is to run 4 different prompt versions using a sample of narratives and evaluate whether we can observe any improvement in performance based solely on the prompt. As a default the body and example output of my primary prompt looks like this:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nBODY1 = \"\"\"\nINSTRUCTIONS:\n    Closely follow these instructions:\n        - For each variable below return a 0 for 'no' and a 1 for 'yes' unless otherwise stated.\n        - If more than two answers are available, return ONE of the numbered values.\n        - Rely ONLY on information available in the narrative. Do NOT extrapolate.\n        - Return a properly formatted json object where the keys are the variables and the values are the numeric labels.\n        - Do NOT return anything other than the label. Do NOT include any discussion or commentary.\n\nVARIABLES:   \n    DepressedMood: The person was perceived to be depressed at the time\n    MentalIllnessTreatmentCurrnt: Currently in treatment for a mental health or substance abuse problem\n    HistoryMentalIllnessTreatmnt: History of ever being treated for a mental health or substance abuse problem\n    SuicideAttemptHistory: History of attempting suicide previously\n    SuicideThoughtHistory: History of suicidal thoughts or plans\n    SubstanceAbuseProblem: The person struggled with a substance abuse problem. This combines AlcoholProblem and SubstanceAbuseOther from the coding manual\n    MentalHealthProblem: The person had a mental health condition at the time\n    DiagnosisAnxiety: The person had a medical diagnosis of anxiety\n    DiagnosisDepressionDysthymia: The person had a medical diagnosis of depression\n    DiagnosisBipolar: The person had a medical diagnosis of bipolar\n    DiagnosisAdhd: The person had a medical diagnosis of ADHD\n    IntimatePartnerProblem: Problems with a current or former intimate partner appear to have contributed\n    FamilyRelationship: Relationship problems with a family member (other than an intimate partner) appear to have contributed\n    Argument: An argument or conflict appears to have contributed\n    SchoolProblem: Problems at or related to school appear to have contributed\n    RecentCriminalLegalProblem: Criminal legal problem(s) appear to have contributed\n    SuicideNote: The person left a suicide note\n    SuicideIntentDisclosed: The person disclosed their thoughts and/or plans to die by suicide to someone else within the last month\n    DisclosedToIntimatePartner: Intent was disclosed to a previous or current intimate partner\n    DisclosedToOtherFamilyMember: Intent was disclosed to another family member\n    DisclosedToFriend: Intent was disclosed to a friend\n    InjuryLocationType: The type of place where the suicide took place.\n        - 1: House, apartment\n        - 2: Motor vehicle (excluding school bus and public transportation)\n        - 3: Natural area (e.g., field, river, beaches, woods)\n        - 4: Park, playground, public use area\n        - 5: Street/road, sidewalk, alley\n        - 6: Other\n    WeaponType1: Type of weapon used \n        - 1: Blunt instrument\n        - 2: Drowning\n        - 3: Fall\n        - 4: Fire or burns\n        - 5: Firearm\n        - 6: Hanging, strangulation, suffocation\n        - 7: Motor vehicle including buses, motorcycles\n        - 8: Other transport vehicle, eg, trains, planes, boats\n        - 9: Poisoning\n        - 10: Sharp instrument\n        - 11: Other (e.g. taser, electrocution, nail gun)\n        - 12: Unknown\n\"\"\"\n\nEXAMPLE_OUTPUT1 = \"\"\"\nEXAMPLE OUTPUT:\n{\n    \"DepressedMood\": 1,\n    \"MentalIllnessTreatmentCurrnt\": 0,\n    \"HistoryMentalIllnessTreatmnt\": 0,\n    \"SuicideAttemptHistory\": 0,\n    \"SuicideThoughtHistory\": 0,\n    \"SubstanceAbuseProblem\": 1,\n    \"MentalHealthProblem\": 0,\n    \"DiagnosisAnxiety\": 0,\n    \"DiagnosisDepressionDysthymia\": 0,\n    \"DiagnosisBipolar\": 0,\n    \"DiagnosisAdhd\": 1,\n    \"IntimatePartnerProblem\": 0,\n    \"FamilyRelationship\": 0,\n    \"Argument\": 1,\n    \"SchoolProblem\": 1,\n    \"RecentCriminalLegalProblem\": 0,\n    \"SuicideNote\": 1,\n    \"SuicideIntentDisclosed\": 0,\n    \"DisclosedToIntimatePartner\": 0,\n    \"DisclosedToOtherFamilyMember\": 0,\n    \"DisclosedToFriend\": 0,\n    \"InjuryLocationType\": 1,\n    \"WeaponType1\": 5\n}\n\"\"\"\n```\n:::\n\n\n\nThis prompt is basically a copy-paste from the instructions in the contest, with an example json object to ensure the LLM knows what to output. This is probably the absolute minimum you would need to get some useful output. That being said, there are at least two areas I think we can improve this baseline prompt:\n\n1.  Add detailed descriptions to each feature\n2.  Add \"few shot\" examples\n\nThe first one will have us spell out a bit more closely what we actually mean for each variable. For example we might turn this:\n\n> DepressedMood: The person was perceived to be depressed at the time\n\ninto this:\n\n> DepressedMood: The person was perceived to be depressed at the time - 1: Specific signs of depression were noted in the narrative (e.g., sad, withdrawn, hopeless) - 0: No mention of depressive symptoms\n\nThe first one is vague, and gives the LLM a lot of freedom to guess what \"perceived to be depressed\" means. In contrast, the second one asks only to mark it if there were specific mentions of depresion noted in the narrative.\n\nWe can also add \"few shot\" examples to the prompt. This just means we provide the LLM with a few examples of a narrative and output to give it a better idea of what we want. Here is a synthesized example of what this might look like using a simulated narrative:\n\n*\\[Simulated Example\\]*\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\"}\nEXAMPLE_OUTPUT2 = \"\"\"\nHere is an example narrative and expected output:\n\nEXAMPLE NARRATIVE 1:\nOfficers responded at 0745 hours to a report of a self-inflicted gunshot wound. The V was found in a bedroom with two acquaintances present, a gunshot wound to the left temple, and no exit wound. The V’s girlfriend stated he had been struggling with anxiety and depression, especially with the anniversary of his cousin’s suicide approaching. Before pulling the trigger, the V said, “You won’t believe me until I do it.” A .380 caliber firearm, a handwritten note, alcohol, and unidentified substances were found at the scene.\n\nEXAMPLE OUTPUT 1:\n{\n    \"DepressedMood\": 1,\n    \"MentalIllnessTreatmentCurrnt\": 0,\n    \"HistoryMentalIllnessTreatmnt\": 1,\n    \"SuicideAttemptHistory\": 0,\n    \"SuicideThoughtHistory\": 1,\n    \"SubstanceAbuseProblem\": 1,\n    \"MentalHealthProblem\": 1,\n    \"DiagnosisAnxiety\": 1,\n    \"DiagnosisDepressionDysthymia\": 1,\n    \"DiagnosisBipolar\": 0,\n    \"DiagnosisAdhd\": 0,\n    \"IntimatePartnerProblem\": 0,\n    \"FamilyRelationship\": 0,\n    \"Argument\": 0,\n    \"SchoolProblem\": 0,\n    \"RecentCriminalLegalProblem\": 0,\n    \"SuicideNote\": 1,\n    \"SuicideIntentDisclosed\": 1,\n    \"DisclosedToIntimatePartner\": 1,\n    \"DisclosedToOtherFamilyMember\": 0,\n    \"DisclosedToFriend\": 0,\n    \"InjuryLocationType\": 1,\n    \"WeaponType1\": 6\n}\n\"\"\"\n```\n:::\n\n\n\nIf you want to see all of the propmpt versions, you can look at the script under `src.prompts.py` in the git repo for this post.\n\n### Running through ChatGPT\n\nTo test all variants of these prompts, I took a sample of 200 cases from the 4000 original narratives, and oversampled from rare categories to ensure I had at least a few examples for each variable (if you are curious, you can look at my `uid_weighting.R` script for how I did this). Using these 200 cases, I wanted to test the 4 different prompt types by running them through a ChatGPT LLM. Most of the work is handled using the code below, where I take each sample narrative, construct 4 prompts, then append them to a json request template. All of these 200 examples are appended into a jsonlist object, then bulk run in a batch using the OpenAI API. I use one of the cheaper models here, `4.0 mini`, which is probably fine for testing purposes. I should also note that tokens are *cheap*. In a batch run with 1.7 million tokens, I paid 13 cents for the input tokens and 6 cents for the output tokens. I actually messed up the prompt caching for this test, but if I did it correctly it would be 50% cheaper.\n\nOne important note - I specify the output must come in the form of a json object by adding `\"response_format\": { \"type\": \"json_object\" }`. This is ChatGPT's \"json mode\", which is quite handy for this. Oftentimes the LLM might hallucinate an output that is not a valid json and break the workflow.\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"false\"}\nfor row in narratives.iterrows():\n\n    # grab the unique id and text\n    single_narrative = row[1]\n    id = single_narrative[\"uid\"]\n    txt = single_narrative[\"NarrativeLE\"]\n\n    prompt_input = {\n        \"header\": HEADER1,\n        \"narrative\": txt,\n        \"body\": [BODY1, BODY2],\n        \"example_output\": [EXAMPLE_OUTPUT1, EXAMPLE_OUTPUT2],\n        \"footer\": None,\n    }\n\n    # create a prompt, pass in the text narrative\n    prompt_versions = prompt_creator.standard_prompt_caching(**prompt_input)\n\n    version_num = 0\n    for prompt in prompt_versions:\n        # now append to list\n        json_list.append(\n            {\n                \"custom_id\": f\"{id}_{version_num}\",\n                \"method\": \"POST\",\n                \"url\": \"/v1/chat/completions\",\n                \"body\": {\n                    \"model\": \"gpt-4o-mini\",\n                    \"messages\": [\n                        {\"role\": \"system\", \"content\": ROLE},\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    \"max_tokens\": 500,\n                    \"response_format\": { \"type\": \"json_object\" },\n                },\n            }\n        )\n        version_num += 1\n```\n:::\n\n\n\nAfter passing the jsonlist it takes about an hour to run using the cheapest batch service. We can then download the output jsons and parse our results!\n\n## Results\n\nTo evaluate the results we use the same [metrics provided by the contest](https://github.com/drivendataorg/youth-mental-health-runtime/blob/main/src/scoring.py): a macro-weighted f1 score for the binary variables, and a micro-weighted f1 score for the multi-class categorical variables.\n\nBelow we see that relative to the baseline prompt (model 0) adding descriptions and adding examples had a positive impact on both the f1 score and accuracy in the aggregate. The model with the highest score was the one with both detailed variable descriptions and 3 scored examples. Overall, all of the models did pretty well given how little tweaking I did. The more interesting question is looking at how much quality really was impacted by different prompting styles relative to the general randomness we typically expect out of an LLM.\n\n| Prompt Version                        | F1 Score | Accuracy |\n|---------------------------------------|---------:|---------:|\n| 0: Baseline                           |    0.763 |    0.824 |\n| 1: Add 3 few-shot examples            |    0.775 |    0.832 |\n| 2: Add more descriptions to features  |    0.778 |    0.837 |\n| 3: Add few-shot and more descriptions |    0.781 |    0.839 |\n\nA solution: remember analysis of variance from your stats 101 course? We can actually use it here to see if there if there is non-zero variation in f1 score attributable to a change in prompts, relative to the variation in question type. Looking below we see, unsurprisingly, almost all the variation is explainable by the question type (meaning that differences in f1 scores are mostly based on the question type).\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nres <- with(results, aov(f1score ~ as.factor(model_ver) + as.factor(feature)))\nbroom::tidy(res) %>% kable(digits=3)\n```\n\n::: {.cell-output-display}\n\n\n|term                 | df| sumsq| meansq| statistic| p.value|\n|:--------------------|--:|-----:|------:|---------:|-------:|\n|as.factor(model_ver) |  3| 0.004|  0.001|     4.054|    0.01|\n|as.factor(feature)   | 22| 0.584|  0.027|    76.276|    0.00|\n|Residuals            | 66| 0.023|  0.000|        NA|      NA|\n\n\n:::\n:::\n\n\n\nIf we break this down by question type and plot them out, the results are a bit clearer. Below I have the questions ordered based on their variance in f1 scores, so that questions that changed more often based on the prompt are nearer the top. Interestingly, a few questions see a large improvement from the baseline prompt to the more advanced one. `DepressedMood` has an f1 score of .5 on the original prompt, which increases to about .65 on the final prompt. We see similar results with `SchoolProblem`, `FamilyRelationship`, and `Argument` as well. Questions that were already doing quite good see virtually no change - like `WeaponType1`. The LLM has a very easy time identifying the weapon, because it is almost always clearly disclosed in the narrative (and is very often a firearm).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](prompt_testing_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n## Summary\n\nTo wrap up this very long blog post, I have a few things to note:\n\n1.  Writing good prompts isn't hard - but requires structuring questions in a way to ensure you get what you expect to see.\n2.  Setting up a testing environment can help automate comparisons of many different prompts.\n3.  Principled testing can help reduce manual prompt testing down the road.\n\nWe're still in uncharted waters, comparatively speaking. Things on the LLM front are moving fast!",
    "supporting": [
      "prompt_testing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}