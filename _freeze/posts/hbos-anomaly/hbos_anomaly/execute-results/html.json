{
  "hash": "e08f5933a589ddc59b67d7c815e454d7",
  "result": {
    "markdown": "---\ntitle: \"Building an Outlier Ensemble from 'Scratch'\"\nsubtitle: \"Part 3: Histogram-based anomaly detector\"\nauthor: \"Gio Circo, Ph.D.\"\ndate: 2023-5-14\nformat: \n    html:\n        self-contained: false\n        code-fold: true\n        mainfont: \"Roboto\"\n        section-divs: true\n        toc: true\n        title-block-banner: true\nbibliography: refs.bib\ntheme: flatly\nimage: \"hist.png\"\n---\n\n\n\n\n## Part 3: The histogram-based anomaly detector\n\nThis is the third part of a 3-part series. In the first two posts I described\nhow I built a [principal components analysis anomaly detector](https://gmcirco.github.io/blog/posts/pca-anomaly/pca_anomaly.html) and\na k-nearest neighbors anomaly detector as components for a ensemble model. This\nthird post will discuss the last piece, which is a histogram-based anomaly\ndetector.\n\n1. \"Soft\" principal components anomaly detector\n2. K-nearest neighbors anomaly detector\n3. **Isolation forest or histogram-based anomaly detector**\n\n## Building a Histogram-Based Outlier Detector\n\n### Defining sub-space density\n\nThe core of the idea behind a histogram-based outlier detector is that it is a method to efficiently explore subspaces of the data by binning observations into discrete groups, then weighting each bin inversely by the number of observations (more on this in a moment). To start, we can provide a quick example showing how we can use histograms to partition the data into bins. Below, I create two histograms for the features representing stay length and average cost per-stay.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define breaks\nh1 <- hist(df$stay_len, breaks = 5, plot = FALSE)\nh2 <- hist(df$cost_per_stay, breaks = 5, plot = FALSE)\n\n# append to dataframe\nhdf <- df %>%\n  mutate(d1 = findInterval(stay_len, h1$breaks),\n         d2 = findInterval(cost_per_stay, h2$breaks),\n         space = paste0(d1,\"-\",d2))\n\n# Create a data frame with a grid of values for the predictor variables\ngrid <- expand.grid(stay_len = seq(min(hdf$stay_len), max(hdf$stay_len), length.out = 10),\n                    cost_per_stay = seq(min(hdf$cost_per_stay), max(hdf$cost_per_stay), length.out = 10)) %>%\n  mutate(d1 = findInterval(stay_len, h1$breaks),\n         d2 = findInterval(cost_per_stay, h2$breaks),\n         space = paste0(d1,\"-\",d2)) %>%\n    mutate(space = ifelse(space %in% hdf$space, space, NA)) %>%\n    fill(space)\n```\n:::\n\n\nIf we plot each of these histograms, we can observe that most values concentrate in a few bins, while a small number of values are in more sparsely-populated bins. Obviously this shows us that the majority of stay lengths are between 0-10 days, and the average cost per-stay is around $4,000.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nplot(h1, main = \"Stay Length\", xlab = \"Days\", col = '#004488', border = 'white')\nplot(h2, main = \"Cost Per Stay\", xlab = \"Cost\", col = '#004488', border = 'white')\n```\n\n::: {.cell-output-display}\n![A histogram's bins are proportional to the number of observations.](hbos_anomaly_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe can plot this in 2 dimensions to see how the feature space distribution is subdivided based on histogram bins. As we would expect, the majority of observations fall into a few regions, while potential outliers exist in much more sparsely populated bins. This is actually fairly similar to a decision tree, where we classify observations based on a set of rules. For example, the lone observation on the far right of the plot is in a region where `stay_length >= 44` and `cost_per_stay >= 3367` and `cost_per_stay <= 4837`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_tile(data = grid, aes(x = stay_len, y = cost_per_stay, fill = space), alpha = .6) +\n  geom_point(data = df, aes(x = stay_len, y = cost_per_stay), color = '#004488') +\n  labs(x = 'Stay Length', y = 'Cost Per Stay') +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n```\n\n::: {.cell-output-display}\n![2D histogram partitioning of observations. Colored regions represent different bin partitions. Note the sparse regions at the top left and lower right quadrants.](hbos_anomaly_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Scoring observations\n\nWith this in mind, we are essentially going to do the above, but in $d$ dimensions (where $d$ is the number of input features). To give each observation an anomaly score, we will follow a [very simple scoring mechanism](https://www.goldiges.de/publications/HBOS-KI-2012.pdf) proposed by the original authors of the method where:\n\n$$HBOS(p) = \\sum^d_{i=0}log(\\frac{1}{hist_i(p)})$$\nWhich states that the histogram-based anomaly score is the sum of the log of inverse histogram densities. More simply, for each feature $d$ we compute a histogram density, and each observation is scored based on the inverse of its bin density [@goldstein2012histogram]. This means observations in sparsely populated bins receive higher scores, and vice-versa. One of the trade-offs here is that we have to assume feature independence (which is a tenuous assumption in a lot of cases), but even violations of this might not be *too* bad.\n\n### A brief aside: choosing the optimal bin size\n\nOne challenge with this approach is that before we calculate histogram densities we need to define the number of bins for our histograms ahead of time. Now, one simple method might just be to choose a very rough rule-of-thumb (e.g. the \"Sturges\" rule of $1+log2(N)$) or to just choose a constant number like 5 or 10. A more principled way, however, would be to derive the optimal number of bins based on some properties of the input data.\n\nThere are a lot of proposed options out here, but the one that makes a lot of sense to me (and, incidentally, is also used in the [pyod](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/hbos.html) implementation of this function) is to iteratively fit histograms, calculate a [penalized maximum likelihood](http://www.numdam.org/item/10.1051/ps:2006001.pdf) estimate for each histogram $D$, and then select the number of bins corresponding to the maximum likelihood estimate [@birge2006many]. A rough R implementation of this is shown below:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n  # internal function: compute optimal bins\nopt_bins <- function(X, upper_bound = 15)\n  {\n    \n    epsilon = 1\n    n <- length(X)\n    \n    # maximum likelihood estimate for bin\n    maximum_likelihood <- array(0, dim = c(upper_bound - 1, 1))\n    \n    # rule of thumb for upper bound\n    if (is.null(upper_bound)) {\n      upper_bound <- as.integer(sqrt(length(X)))\n    }\n    \n    for (i in seq_along(1:(upper_bound - 1))) {\n      b <- i + 1\n      histogram <- hist(X, breaks = b, plot = FALSE)$counts\n      \n      maximum_likelihood[i] <-\n        sum(histogram * log(b * histogram /  n + epsilon) - (b - 1 + log(b) ^ 2.5))\n    }\n    \n    return(which.max(maximum_likelihood))\n  }\n```\n:::\n\n\nSo, running this for the first feature `stay_len`, we get:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nopt_bins(df$stay_len)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5\n```\n:::\n:::\n\n\n### Building the detector\n\nWith the issue of histogram bins out of the way, we can procede with the rest of the model. The last bit is really quite simple. For each feature $d$ we compute the optimal number of bins (using the function we just defined above). We then build a histogram for that feature and identify which points fall within each bin. We then score each point according to the formula above, which is the log of the inverse histogram density (making outlying observations have correspondingly *higher* anomaly scores). The last thing we do after running this algorithm over all $d$ features is to scale their scores (here, I use min-max normalization) and sum them together. The code to do this is below:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# run HBOS\n  for(d in 1:d){\n    \n    h <- hist(X[,d], breaks = opt_bins(X[,d]), plot = FALSE)\n    fi <- findInterval(X[,d], h$breaks)\n    \n    hbos[[d]] <- log(1/h$density[fi])\n  }\n  \n  # minmax scale feature vectors\n  hbos <- lapply(hbos, function(x){(x- min(x)) /(max(x)-min(x))})\n  \n  # return score\n  return(apply(do.call(cbind, hbos), 1, sum))\n```\n:::\n\n\n## Running the Model\n\nNow we're ready to run everything. For simplicity, I wrap all this code into a single `adHBOS` function that contains the optimal histogram binning and the scoring (see: @sec-hbos). For flagging anomalies we will just identify the highest 5% (a rough, but arguably acceptable heuristic).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nX <- df[,2:7]\n\ndf$anom <-  adHBOS(X)\ndf$flag <- ifelse(df$anom >= quantile(df$anom, .95),1,0)\n```\n:::\n\n\nIf we look at a histogram of we see most scores are low, while the outliers are clearly visible on the right-hand side.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_histogram(aes(x = anom), bins = 10, color = 'white', fill = '#004488')+\n  labs(x = \"Anomaly Score\", y = \"Observations\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![Histogram-based anomaly scores. More anomalous observations have higher scores](hbos_anomaly_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nComparing this to our earlier plot we see:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_tile(data = grid, aes(x = stay_len, y = cost_per_stay, fill = space), alpha = .6) +\n  geom_point(data = df, aes(x = stay_len, y = cost_per_stay), color = '#004488') +\n  geom_point(data = df[df$flag == 1,], aes(x = stay_len, y = cost_per_stay, color = '#BB5566'), size = 2) +\n  labs(x = 'Stay Length', y = 'Cost Per Stay') +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n```\n\n::: {.cell-output-display}\n![Plotting inliers (blue) and outliers (red) in 2D space.](hbos_anomaly_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nAs we expect, most of the observations that are flagged as outliers reside in bins with few other observations. This is pretty consistent with the other two methods we used before (PCA and KNN anomaly detectors). One specific advantage of the HBOS method is that it is very fast for even large datasets. However, with higher levels of dimensionality it is very likely that the assumption of feature independence is tenuous at best. Other methods, like the isolation forest can often perform better in higher dimensions. However, the simplicity of the method makes it easy to explain, which can be a benefit in many cases!\n\n### HBOS Anaomaly Detector: Example Function{#sec-hbos}\n\nHere's a minimal working example of the procedure above. As we build our ensemble, we'll come back to this function later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a principal components anomaly detector\nadHBOS <- function(X, ub = 15){\n  \n  # scale input features, define list to hold scores\n  X <- scale(X)\n  j <- dim(X)[2]\n  hbos <- vector(\"list\",j)\n  \n  # internal function: compute optimal bins\n  opt_bins <- function(X, upper_bound = ub)\n  {\n    \n    epsilon = 1\n    n <- length(X)\n    \n    # maximum likelihood estimate for bin\n    maximum_likelihood <- array(0, dim = c(upper_bound - 1, 1))\n    \n    # rule of thumb for upper bound\n    if (is.null(upper_bound)) {\n      upper_bound <- as.integer(sqrt(length(X)))\n    }\n    \n    for (i in seq_along(1:(upper_bound - 1))) {\n      b <- i + 1\n      histogram <- hist(X, breaks = b, plot = FALSE)$counts\n      \n      maximum_likelihood[i] <-\n        sum(histogram * log(b * histogram /  n + epsilon) - (b - 1 + log(b) ^ 2.5))\n    }\n    \n    return(which.max(maximum_likelihood))\n  }\n  \n  # run HBOS\n  for(j in 1:j){\n    \n    h <- hist(X[,j], breaks = opt_bins(X[,j]), plot = FALSE)\n    fi <- findInterval(X[,j], h$breaks)\n    \n    hbos[[j]] <- log(1/h$density[fi])\n  }\n  \n  # minmax scale feature vectors\n  hbos <- lapply(hbos, function(x){(x- min(x)) /(max(x)-min(x))})\n  \n  # return score\n  return(apply(do.call(cbind, hbos), 1, sum))\n  \n}\n```\n:::",
    "supporting": [
      "hbos_anomaly_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}