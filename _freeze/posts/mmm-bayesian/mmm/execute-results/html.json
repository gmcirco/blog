{
  "hash": "41c98b437990fee4723d8e5fb5bdf372",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"An Outsider's Perspective On Media Mix Modelling\"\nsubtitle: \"A Bayesian Approach to MMM\"\nauthor: Gio Circo, Ph.D.\ndate: 2024-3-18\ncategories:\n  - R\n  - Bayesian Statistics\nformat: \n    html:\n        self-contained: true\n        code-fold: false\n        mainfont: \"Roboto\"\n        section-divs: true\n        toc: true\n        title-block-banner: true\nimage: mmm.jpg\ntheme: flatly\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Media Mix Modelling\n\nI'm trying something a bit new this time. Typically how I learn is that I see something interesting (either in a blog post, an academic article, or through something a co-worker is working on). I'll then try and work through the problem via code on my own to see how I can make it work. It's not always perfect, but it gets me started. Today I'm going to go out of my comfort zone and try my hand at **Media Mix Modelling** (MMM).\n\nIn general, the stated goal of MMM is to determine the optimal distribution of advertising money, given $n$ different venues. For instance, this could determine how much to spend on internet, TV, or radio advertising given the costs of running ads and the expected return for each venue. Typically this is done using a regression to try and parse out the effect of each type of advertising net of many other factors (e.g. seasonal and trend effects, costs of the product, demand, etc...).\n\n### Getting some data\n\nGetting reliable open-source data for MMM is actually a bit more difficult than you think. There are a number of very trivial simulated datasets scattered about on places like Kaggle, but these aren't terribly useful. I was able to find a strange mostly undocumented set of data from a git repo [here](https://github.com/jamesrawlins1000/Market-mix-modelling-data). Per the author, the data purports:\n\n> \"...data contain information on demand, sales, supply, POS data, advertisiment expenditure and different impressions recorded across multiple channels for calculating the advertising campaign effectiveness such as Mobile SMS, Newspaper Ads, Radio, TV, Poster, Internet etc.in the form of GRP (Gross Rating Point) in Shenzhen city\"\n\nGood enough for me.\n\n## The Adstock Function\n\nThe paper [Bayesian Methods for Media Mix Modeling with Carryover and Shape Effects](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf) is a pretty clear and concise introduction to media mix modelling. This paper is a pretty good introduction into many of the issues related to MMM. One of the biggest issues is the need to transform the ad spend variables to better represent how they behave in real life. For example, we don't necessarily expect an ad campaign to have an *instantaneous* lift, nor do we expect its effect to end immediately either. Hence, the need to model appropriate [carryover effects]().\n\nTo do this I'll borrow a function [Kylie Fu](https://medium.com/@kyliefu/implementation-of-the-advertising-adstock-theory-in-r-62c2cc4b82fd) to calculate the weights for the delayed adstock function. The goal here is to define a function that can transform the ad spend variables to reflect our belief that they have delays of decay, delays of peak effect, and an upper maximum carryover effect. Below the function takes a vector of ad spend data and creates the transformation given the parameters `lambda`, `theta`, and `L`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nDelayedSimpleAdstock <- function(advertising, lambda, theta, L){\n  N <- length(advertising)\n  weights <- matrix(0, N, N)\n  for (i in 1:N){\n    for (j in 1:N){\n      k = i - j\n      if (k < L && k >= 0){\n        weights[i, j] = lambda ** ((k - theta) ** 2)\n      }\n    }\n  }\n  \n  adstock <- as.numeric(weights %*% matrix(advertising))\n  \n  return(adstock)  \n}\n```\n:::\n\n\n\n### Setting up an adstock transformation\n\nNow we can choose the parameters for the adstock transformation. Ideally, we want a transformation that captures the decay of the advertising program (`lambda`), its delayed peak onset (`theta`), and its maximum effect duration (`L`). With a bit of simulation we can see what each parameter does across a value of different lags. The goal here is to have a function that matches what we believe the actual effect of ad spending looks like for different media regions (e.g. TV, radio, internet). Below, we can see that increasing `lambda` increases the decay of the effect up, while varying `theta` sets the peak onset of the ad campaign to later lags. The value of `L` simply sets the maximum effect to a specific lag.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# set up grid of params to iterate over\n\nx <- c(1, rep(0, 15))\nlambda = seq(0,1, by = .1)\ntheta = seq(0,10, by = 1)\nL = seq(1,12, by = 1)\n```\n:::\n\n\n\n::: panel-tabset\n## Lambda\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mmm_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## Theta\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mmm_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n## L\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mmm_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n:::\n\nBased on a visual assesment I just chose an adstock function with a lambda of .8 (suggesting moderate decay of the initial ad effect), a theta of 2 (implying a peak onset of 2 weeks), and an L of 13 which is a rule-of-thumb that makes the maximum effect quite large.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Adstock function (Lambda = .8, theta = 2, L = 13)](mmm_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nThe code below applies our adstock function to each of the spend variables. For simplicity here I am making the assumption that all of the modes have similar adstock functions, but this can (and should) vary per modality based on expert prior information. We convert the daily data (which is quite noisy) to a more commonly utilized weekly format. We then limit the focus of our analysis to a 4-year time span.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# setup weekly data\n# setup weekly data\nmmm_weekly_data <-\n  mmm_raw %>%\n  mutate(date = as.Date(DATE, \"%m/%d/%Y\"),\n         year = year(date),\n         month = month(date),\n         week = week(date)) %>%\n  select(\n    date,\n    year,\n    month,\n    week,\n    sales = `SALES ($)`,\n    spend_sms = `Advertising Expenses (SMS)`,\n    spend_news = `Advertising Expenses(Newspaper ads)`,\n    spend_radio = `Advertising Expenses(Radio)`,\n    spend_tv = `Advertising Expenses(TV)`,\n    spend_net = `Advertising Expenses(Internet)`,\n    demand = DEMAND,\n    supply = `POS/ Supply Data`,\n    price = `Unit Price ($)`\n  ) %>%\n  filter(year %in% 2014:2017)\n\n\nweekly_spend <-\n  mmm_weekly_data %>%\n  group_by(year, month, week) %>%\n  summarise(across(sales:spend_net, sum), across(demand:price, mean), .groups = 'drop') %>%\n  mutate(index = 1:nrow(.))\n\n\n# Apply transformation to advertising variables, scale dollar values to per $1,000\nX <-\n  weekly_spend %>%\n  mutate(across(spend_sms:spend_net,~ DelayedSimpleAdstock(.,lambda = .8,theta = 2,L = 13)),\n         across(spend_sms:price, function(x) x/1e3),\n         trend = 1:nrow(.)/nrow(.))\n```\n:::\n\n\n\n### Setting up the model\n\nBefore we fit the model we can plot out the primary variables of interest, along with our dependent variable `sales`. Looking below we can see a few potential issues. One which should jump out immediately is that there is a *very* high correlation between several of our ad spend categories. For example, the correlation between TV spending and news spending is almost 1. In the case of MMM this is a common problem, which makes unique identification of the effect of ad spends much more difficult. More troubling, by just eyeballing these plots there doesn't seem to be a terribly strong relationship between *any* of the advertising venues and sales.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(X[, c('sales',\n           'spend_sms',\n           'spend_news',\n           'spend_radio',\n           'spend_tv',\n           'spend_net')], col = '#004488')\n```\n\n::: {.cell-output-display}\n![Pairwise relationships between sales ~ ad venues](mmm_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nNor do the pairwise correlations seem to be very high either (in fact, they are very nearly zero). Regardless, we'll continue by fitting a simple set of models.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nround(cor(X[, c('sales',\n           'spend_sms',\n           'spend_news',\n           'spend_radio',\n           'spend_tv',\n           'spend_net')]),2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            sales spend_sms spend_news spend_radio spend_tv spend_net\nsales        1.00     -0.01      -0.04       -0.03    -0.04     -0.07\nspend_sms   -0.01      1.00       0.89        0.91     0.90     -0.06\nspend_news  -0.04      0.89       1.00        0.86     1.00      0.30\nspend_radio -0.03      0.91       0.86        1.00     0.87     -0.09\nspend_tv    -0.04      0.90       1.00        0.87     1.00      0.28\nspend_net   -0.07     -0.06       0.30       -0.09     0.28      1.00\n```\n\n\n:::\n:::\n\n\n\n## Fitting A Model\n\nOne of the biggest challenges with MMM is that many of the model coefficients including the advertising venues, are likely to be *very* highly correlated. For example, the advertising spend on TV ads is almost perfectly correlated with the spend on news ads. We can set some moderately strong priors on the ad spend coefficients to ensure that the estimates don't explode due to multicollinearity. Here I'm just placing a `normal(0, .5)` prior which is still pretty wide for a coefficient on the logrithmic scale.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# set up priors\nbprior <- c(prior(normal(0,.5), class = \"b\", coef = \"spend_sms\"),\n            prior(normal(0,.5), class = \"b\", coef = \"spend_news\"),\n            prior(normal(0,.5), class = \"b\", coef = \"spend_radio\"),\n            prior(normal(0,.5), class = \"b\", coef = \"spend_tv\"),\n            prior(normal(0,.5), class = \"b\", coef = \"spend_net\"))\n\n# just intercept\nbrm_fit_0 <- brm(log(sales) ~ 1, data = X,\n                 chains = 4,\n                 cores = 4,\n                 family = gaussian(),\n                 file = \"C:/Users/gioc4/Documents/blog/data/brms_models/mmm_fit0.Rdata\")\n\n# no random effects, linear trend effect\nbrm_fit_1 <- brm(log(sales) ~                 \n                   demand +\n                   supply +\n                   price +\n                   as.factor(month) +\n                   as.factor(week) +\n                   trend +\n                   spend_sms +\n                   spend_news +\n                   spend_radio +\n                   spend_tv +\n                   spend_net,\n                 data = X,\n                 chains = 4,\n                 cores = 4,\n                 prior = bprior,\n                 family = gaussian(),\n                 file = \"C:/Users/gioc4/Documents/blog/data/brms_models/mmm_fit1.Rdata\")\n\n# random effects for month+week\n# smoothing spline for trend\nbrm_fit_2 <- brm(log(sales) ~                 \n                   demand +\n                   supply +\n                   price +\n                   s(trend) +\n                   spend_sms +\n                   spend_news +\n                   spend_radio +\n                   spend_tv +\n                   spend_net +\n                   (1|month) +\n                   (1|week),\n                 data = X,\n                 chains = 4,\n                 cores = 4,\n                 prior = bprior,\n                 family = gaussian(),\n                 control = list(adapt_delta = .9),\n                 file = \"C:/Users/gioc4/Documents/blog/data/brms_models/mmm_fit2.Rdata\")\n```\n:::\n\n\n\n### Model Evaluation\n\nNow we can evaluate the models. Here, I evaluate a model with random effects for month and week, and a smoothing spline for the trend component against a fixed effects model with a linear trend, and a \"null\" model with just an intercept. The model with a smooth trend spline appears to beat out the linear trend, which makes sense given the non-linear bump in sales observed in the raw data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# evaluate models using leave-out-out criterion\n# looks like smoothing spline is slightly better\nloo_eval <- loo(brm_fit_0, brm_fit_1, brm_fit_2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Found 12 observations with a pareto_k > 0.7 in model 'brm_fit_1'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Found 15 observations with a pareto_k > 0.7 in model 'brm_fit_2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nloo_eval$diffs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          elpd_diff se_diff\nbrm_fit_2    0.0       0.0 \nbrm_fit_1  -16.2       8.3 \nbrm_fit_0 -291.7      18.9 \n```\n\n\n:::\n:::\n\n\n\nWe can (and should) also check out the predictions from the model using a posterior predictive check. In Bayesian terms what this means is we take a sample of draws from our fitted model and compare them against the observed data. If our model is capturing the process well, the predicted values should generally follow the observed process. Below we see that our model does a fairly decent job.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npp_check(brm_fit_2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Posterior predictive check, smoothing spline model](mmm_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nFinally, we can look at some of the model coefficients\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(brm_fit_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log(sales) ~ demand + supply + price + s(trend) + spend_sms + spend_news + spend_radio + spend_tv + spend_net + (1 | month) + (1 | week) \n   Data: X (Number of observations: 197) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(strend_1)     1.53      0.73     0.54     3.35 1.00     1739     2509\n\nMultilevel Hyperparameters:\n~month (Number of levels: 12) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.86      0.22     0.55     1.38 1.00     1413     2226\n\n~week (Number of levels: 53) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.79      0.09     0.64     1.00 1.00      721     1142\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      13.88      0.48    12.94    14.83 1.00     2447     3039\ndemand          0.01      0.03    -0.06     0.07 1.00     4078     3029\nsupply          0.19      0.04     0.12     0.26 1.00     4860     3017\nprice           2.88      0.99     0.83     4.86 1.00     4064     2908\nspend_sms       0.12      0.17    -0.22     0.45 1.00     5927     3202\nspend_news     -0.01      0.51    -1.01     0.99 1.00     6425     2879\nspend_radio     0.18      0.14    -0.10     0.46 1.00     4534     2610\nspend_tv       -0.02      0.02    -0.05     0.01 1.00     5170     2835\nspend_net       0.00      0.00    -0.00     0.01 1.00     4820     3243\nstrend_1       -2.42      1.55    -5.84     0.34 1.00     2711     2636\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.15      0.01     0.13     0.17 1.00     1381     2410\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n### Model Predictions\n\nAnd here's the estimated lift for Radio by week. While there is definitely some lift, it is pretty tiny here\n\n::: column-page\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# get predictions \nX_no_radio <- X %>% mutate(spend_radio= 0)\n\npred1 <- predict(brm_fit_2)\npred1_no_radio <- predict(brm_fit_2, newdata = X_no_radio)\n\npred_dataframe <-\n  cbind.data.frame(week = 1:197,\n                   obs = pred1[, 1],\n                   pred = pred1_no_radio[, 1]) %>%\n  pivot_longer(-week) %>%\n  group_by(week) %>%\n  mutate(diff = value - lead(value, 1))\n\n# predicted (all) vs predicted (no radio)\nggplot(pred_dataframe) +\n  geom_line(aes(x = week, y = value, color = name)) +\n  theme_bw() +\n  scale_color_manual(values = c(\"#0077BB\", \"#EE7733\")) +\n  labs(y = \"(log) Sales\", x = \"Week\") +\n  theme(legend.position = 'none')\n```\n\n::: {.cell-output-display}\n![](mmm_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# predicted lift from radio\npred_dataframe %>%\n  na.omit() %>%\n  ggplot() +\n  geom_line(aes(x = week, y = diff)) +\n  theme_bw() +\n  labs(y = \"(log) Sales\", x = \"Week\")\n```\n\n::: {.cell-output-display}\n![](mmm_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n:::\n\n\n:::\n\nIf we average the estimated mean lift across the entire time frame we get an additional value of about 2%.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean((pred1[,1] - pred1_no_radio[,1])/pred1[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02156586\n```\n\n\n:::\n:::\n\n\n\nAt this point there is a *lot* of additional work that can be done. Most applied uses of MMM apply some optimization algorithms to determine the best ad spend mix given a fixed budget. The data I have here isn't really good enough to delve any deeper into - but its important to note that fitting the model is really only the beginning.\n\n## In Closing: My Take\n\nMy biggest problem with Mixed Media Modelling is that it seems like it is easy to implement badly, but much harder to do well. Not only do you have to appropriately model an adstock function for your advertising venues, you also have very high correlation between your variables. This, in turn, makes the choice of model specification even more important because the coefficients with be *highly* sensitive. Personally, a true experiment or even quasi-experiment would be preferable to this - although I'm all too aware that this is often impossible. Like everything there is no magic bullet, and choosing one approach over another will always introduce trade offs.\n",
    "supporting": [
      "mmm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}