{
  "hash": "058b06e4a1b4386ced749fa49ce036f6",
  "result": {
    "markdown": "---\ntitle: \"Anomaly Detection for Time Series\"\nsubtitle: \"Applying a PCA anomaly detector\"\nauthor: \"Gio Circo, Ph.D.\"\ndate: 2023-4-24\nformat: \n    html:\n        self-contained: false\n        code-fold: true\n        mainfont: \"Roboto\"\n        section-divs: true\n        toc: true\n        title-block-banner: true\ntheme: flatly\nimage: \"power.png\"\n---\n\n\n\n\n## An Unsupervised Approach to Time Series Anomalies\n\nIdentifying outliers in time series is one of the more common applications for unsupervised anomaly detection. Some of the most common examples come from network intrusion detection, mechanical processes, and other types of high-volume streaming data. Of course, there are just as many proposed ways of identifying outliers from the simple (basic Z-scores) to the complex (convolutional neural networks). There are also some approaches that rely on more conventional tabular approaches. Rob Hyndman proposed a few approaches [here](https://robjhyndman.com/papers/icdm2015.pdf) and [here](https://arxiv.org/ftp/arxiv/papers/1908/1908.04000.pdf) showing how many high-volume time series can be compressed into a tabular dataset. The general idea is that you can decompose many time series into tabular observations by creating a large variety of features describing each series. \n\n### Featurizing a time series dataset\n\nThe data we'll use is on [hourly power usage](https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption) for a large power company (American Electric Power). From this dataset we can perform some basic aggregation (to ensure that all timestamped values are on the same day-hour), then separate each set of 24 hours into their individual days. The goal here is to make it easier to look at hours within each day. The code below does a bit of this processing. Of course, working with dates is still always a pain, despite the improvements in `R` libraries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read data, convert to zoo\nelec <- read_csv(\"AEP_hourly.csv\") %>%\n  group_by(Datetime) %>%\n  summarise(AEP_MW = sum(AEP_MW)) %>%\n  filter(year(Datetime) %in% 2017)\n\nelec_ts <- zoo(x = elec$AEP_MW, order.by = elec$Datetime, frequency = 24)\n\n# Split the hourly time series into daily time series\ndaily_ts_list <- split(elec_ts, as.Date(index(elec_ts)))\n\n# Extract the first 24 observations of each daily time series\n# dropping days with missing values\ndaily_24_ts_list <- lapply(daily_ts_list, function(x) {\n  if (length(x) >= 24) {\n    return(x[1:24])\n  } else {\n    return(NA)\n  }\n})\n\n# Convert from list to dataframe\ndaily_24_ts_list <- purrr::discard(daily_24_ts_list, ~any(is.na(.)))\n```\n:::\n\n\nAfter converting the list of values to a data frame, we can proceed with the featurization. As we said before, we can use the [tsfeatures](https://pkg.robjhyndman.com/tsfeatures/articles/tsfeatures.html) library to decompose each day's hourly values into a single observation. We can see this creates a data frame with 17 features, which correspond to various measures, including: autocorrelation, seasonality, entropy and other ad-hoc measures of time series behavior. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Convert from list to dataframe, extract TS features\ndaily_24_ts_list <- purrr::discard(daily_24_ts_list, ~ any(is.na(.)))\n\n# create time series features using `tsfeatures`\ndf <- daily_24_ts_list %>%\n  tsfeatures(\n    features = c(\n      \"acf_features\",\n      \"stl_features\",\n      \"entropy\",\n      \"lumpiness\",\n      \"stability\",\n      \"max_level_shift\"\n    )\n  ) %>%\n  select(-nperiods,-seasonal_period)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(df, width = 65)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 364\nColumns: 13\n$ x_acf1      <dbl> 0.8139510, 0.9318718, 0.9123398, 0.9173901,…\n$ x_acf10     <dbl> 1.771656, 2.081091, 1.868137, 1.889912, 1.7…\n$ diff1_acf1  <dbl> 0.6284980, 0.6181949, 0.6534759, 0.6194560,…\n$ diff1_acf10 <dbl> 1.4141518, 0.7771648, 0.6910469, 1.0491669,…\n$ diff2_acf1  <dbl> 0.3426413, 0.2082615, 0.3164917, 0.2786098,…\n$ diff2_acf10 <dbl> 0.5009215, 0.2550913, 0.4214296, 0.2577332,…\n$ trend       <dbl> 0.7256036, 0.9374949, 0.9377550, 0.9478148,…\n$ spike       <dbl> 1.946527e-04, 4.715699e-06, 6.706533e-06, 4…\n$ linearity   <dbl> 1.8002798, 3.6310069, 3.3908752, 4.1128389,…\n$ curvature   <dbl> 0.4988011, -1.2265007, -2.0788821, -0.66100…\n$ e_acf1      <dbl> 0.6719203, 0.6507832, 0.6513875, 0.6636410,…\n$ e_acf10     <dbl> 1.4588480, 1.0862781, 0.9523931, 1.1326925,…\n$ entropy     <dbl> 0.3585884, 0.5225069, 0.4962494, 0.3090901,…\n```\n:::\n:::\n\n\n### Principal components anomaly detector\n\nAfter doing this, we can proceed as a normal tabular data problem. The PCA anomaly detector that was detailed in [an earlier post](https://gmcirco.github.io/blog/posts/pca-anomaly/pca_anomaly.html#pca-anaomaly-detector-example-function) is an easy plug in here and is a natural fit for the problem. We have a lot of highly correlated measures that likely share a large amount of variance across a few dimensions. We can then weight the lower-variance dimensions *higher* to identify anomalous series. We'll use the $\\chi^2$ distribution to derive a p-value, which we can then threshold for flagging outliers.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Perform anomaly detection\nanom <- adPCA(df)\np = sapply(anom, pchisq, df=ncol(df), ncp = mean(anom), lower.tail=F)\n```\n:::\n\n\n### Results\n\nWe can see which series were flagged by the model by highlighting the series which were flagged at the $p < .01$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# flag observations at p < 0.01\nelec_plot <- elec %>%\n  mutate(date = as.Date(format(Datetime, \"%Y-%m-%d\")),\n         hour = hour(Datetime)) %>%\n  left_join(scored_data) %>%\n  mutate(flag = ifelse(p < 0.01,1,0))\n\nggplot(data = elec_plot, aes(x = Datetime, y = AEP_MW, group = date)) +\n  geom_line(color = '#004488', alpha = .125) +\n  geom_line(data = elec_plot[elec_plot$flag == 1,], color = '#BB5566') +\n  labs(x = \"Date\", y = \"Megawatt Hours\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![Anamolous days are highlighted in red. Note the unusually high spike in late 2017.](pca_ts_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWe can also see what these anomalous series look like compared to the other series on a hourly basis. This plot clearly shows one series with a unusual early-morning spike, and several series with flatter trajectories compared to more normally expected seasonality - in particular, they are days with low power consumption in the afternoon when consumption is usually at its highest. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = elec_plot, aes(x = hour, y = AEP_MW, group = date)) +\n  geom_line(color = '#004488', alpha = .075) +\n  geom_line(data = elec_plot[elec_plot$flag == 1,], color = '#BB5566', size = .7) +\n  labs(x = \"Hour of Day\", y = \"Megawatt Hours\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![Anomalous days often have flatter curves and dip during high-load hours of the day.](pca_ts_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nArguably this isn't an ideal approach because each sub-series is only comprised of 24 observations. That means reliably identifying seasonality via the `stl_features` is questionable. In addition, this approach loses some information that comes from day-to-day correlations. It would probably be worthwhile testing this approach against something like [STL decomposition](https://otexts.com/fpp2/stl.html).",
    "supporting": [
      "pca_ts_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}