{
  "hash": "e33c3e8d72c8513b11cba5e291826fed",
  "result": {
    "markdown": "---\ntitle: \"Building an Outlier Ensemble from 'Scratch'\"\nsubtitle: \"Part 2: K-nearest neighbors anomaly detector\"\nauthor: \"Gio Circo, Ph.D.\"\ndate: 2023-4-25\nformat: \n    html:\n        self-contained: false\n        code-fold: true\n        mainfont: \"Roboto\"\n        section-divs: true\n        toc: true\n        title-block-banner: true\n        draft: true\ntheme: flatly\nimage: \"house.png\"\n---\n\n\n\n\n## Part 2: The K-nearest neighbor anomaly detector\n\nThis is the second part of a 3-part series. In the previous post I talked a bit \nabout my desire to work on building the pieces of an outlier ensemble from \n\"scratch\" (e.g. mostly base R code with some helpers). In the first post I talked\nabout my approach building a [principal components analysis anomaly detector](https://gmcirco.github.io/blog/posts/pca-anomaly/pca_anomaly.html). In\nthis post I'll work on the K-nearest neighbors anomaly detector using the same\nbase data.\n\nTo date, the three parts of the ensemble contain:\n\n1. \"Soft\" principal components anomaly detector\n2. **K-nearest neighbors anomaly detector**\n3. Isolation forest or histogram-based anomaly detector\n\n## Creating the KNN anomaly detector\n\n### Defining distance\n\nIn a way, the K-nearest neighbors anomaly detector is incredibly simple. To compute the anomalousness of a single point we measure its distance to its $k$ nearest neighbors. We then use either the maximum or average distance among those $k$ points as its anomaly score. However, there is some additional complexity here regarding the choice of $k$ in an unsupervised setting - but we'll get to that in a moment.\n\nOne issue is that computing all pairs of nearest neighbors has $O(N^2)$ time complexity. However, we only need to know the number of nearest neighbors up to our value of $k$. Therefore, we can avoid computing nearest unnecessary distances by applying more efficient algorithms - like [k-d trees](https://en.wikipedia.org/wiki/K-d_tree). In the case for $k$ nearest neighbors the time complexity is $O(N * log(N)$. The `RANN` package in R does this fairly efficiently. We'll use the same data as in the [previous post](https://gmcirco.github.io/blog/posts/pca-anomaly/pca_anomaly.html) for this example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# scale input attributes\nX <- df[, 2:7]\nX <- scale(X)\n\n# compute NN distance between all points\n# set n neighbors\nk = 5\n\n# compute k nearest neighbor distances\n# using kd-trees\nd <- RANN::nn2(X, k = k+1)\nd <- d[[2]][,1:k+1]\n```\n:::\n\n\nYou will notice that we set $k$ to $k+1$ to avoid calculating the nearest-neighbor distance to the each point itself (which is always zero). The `nn2` package gives us the Euclidean nearest-neighbor distances for each point arranged from nearest to farthest. For example if we look at the top 3 rows of the distance matrix we see:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd[1:3,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 0.1457822 0.3123632 0.3311984 0.3641609 0.3726815\n[2,] 0.5261689 0.6887312 0.9636189 1.0087124 1.0097114\n[3,] 0.2874466 0.3044159 0.3723676 0.4139428 0.4251863\n```\n:::\n:::\n\n\nWhich gives us the standardized (Z-score) distance to the $k$ nearest neighbor of point $i$. Now all we need to do is decide on how we will summarize this distance. \n\n## Distance Measures\n\nWe have a few options for distance measures \n\n\n::: {.cell}\n\n```{.r .cell-code}\nanom_max <- apply(d, 1, max)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nanom_mean <- adKNN(d, k = 20, method = \"mean\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nscored_data <- data.frame(df,anom_mean)\n\nflag <- scored_data$anom_mean >= quantile(scored_data$anom_mean, .95)\n\nggplot() +\n  geom_point(data = scored_data, aes(x = stay_len, y = diff), color = '#004488', size = 2, alpha = .25) +\n  geom_point(data = scored_data[flag,], aes(x = stay_len, y = diff), color = '#BB5566', size = 2.5) +\n  labs(x = \"Stay Length\", y = \"Avg. Payment Difference\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![](knn_anomaly_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### KNN Anaomaly Detector: Example Function\n\nHere's a minimal working example of the procedure above. As we build our ensemble, we'll come back to this function later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a principal components anomaly detector\nadKNN <- function(X, k = 5, method = 'max'){\n  \n  # compute k nearest neighbor distances\n  # using kd-trees\n  d <- RANN::nn2(X, k = k+1)\n  d <- d[[2]][,1:k+1]\n  \n  # aggregate scores\n  if(method == 'max')\n    anom <- apply(d, 1, max)\n  else if(method == 'mean')\n    anom <- apply(d, 1, mean)\n  else\n    print(\"Function not found\")\n  \n  return(anom)\n  \n}\n```\n:::",
    "supporting": [
      "knn_anomaly_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}