---
title: "An Outsider's Perspective On Media Mix Modelling"
subtitle: "A Bayesian Approach to MMM"
author: Gio Circo, Ph.D.
date: 2024-3-18
categories:
  - R
  - Bayesian Statistics
format: 
    html:
        self-contained: true
        code-fold: false
        mainfont: "Roboto"
        section-divs: true
        toc: true
        title-block-banner: true
theme: flatly
---

```{r}
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(lubridate)
library(brms)

# load data
mmm_raw <-
  read_csv("C:/Users/gioc4/Documents/blog/data/MMM_data.csv")

DelayedSimpleAdstock <- function(advertising, lambda, theta, L){
  N <- length(advertising)
  weights <- matrix(0, N, N)
  for (i in 1:N){
    for (j in 1:N){
      k = i - j
      if (k < L && k >= 0){
        weights[i, j] = lambda ** ((k - theta) ** 2)
      }
    }
  }
  
  adstock <- as.numeric(weights %*% matrix(advertising))
  
  return(adstock)  
}
```

## Media Mix Modelling

I'm trying something a bit new this time. Typically how I learn is that I see something interesting (either in a blog post, an academic article, or through something a co-worker is working on). I'll then try and work through the problem via code on my own to see how I can make it work. It's not always perfect, but it gets me started. Today I'm going to go out of my comfort zone and try my hand at **Media Mix Modelling** (MMM).

In general, the stated goal of MMM is to determine the optimal distribution of advertising money, given $n$ different venues. For instance, this could determine how much to spend on internet, TV, or radio advertising given the costs of running ads and the expected return for each venue. Typically this is done using a regression to try and parse out the effect of each type of advertising net of many other factors (e.g. seasonal and trend effects, costs of the product, demand, etc...).

### Getting some data

Getting reliable open-source data for MMM is actually a bit more difficult than you think. There are a number of very trivial simulated datasets scattered about on places like Kaggle, but these aren't terribly useful. I was able to find a strange mostly undocumented set of data from a git repo [here](https://github.com/jamesrawlins1000/Market-mix-modelling-data). Per the author, the data purports:

> "...data contain information on demand, sales, supply, POS data, advertisiment expenditure and different impressions recorded across multiple channels for calculating the advertising campaign effectiveness such as Mobile SMS, Newspaper Ads, Radio, TV, Poster, Internet etc.in the form of GRP (Gross Rating Point) in Shenzhen city"

Good enough for me.

## The Adstock Function

The paper [Bayesian Methods for Media Mix Modeling with Carryover and Shape Effects](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf) is a pretty clear and concise introduction to media mix modelling. This paper is a pretty good introduction into many of the issues related to MMM. One of the biggest issues is the need to transform the ad spend variables to better represent how they behave in real life. For example, we don't necessarily expect an ad campaign to have an *instantaneous* lift, nor do we expect its effect to end immediately either. Hence, the need to model appropriate [carryover effects]().

To do this I'll borrow a function [Kylie Fu](https://medium.com/@kyliefu/implementation-of-the-advertising-adstock-theory-in-r-62c2cc4b82fd) to calculate the weights for the delayed adstock function. The goal here is to define a function that can transform the ad spend variables to reflect our belief that they have delays of decay, delays of peak effect, and an upper maximum carryover effect. Below the function takes a vector of ad spend data and creates the transformation given the parameters `lambda`, `theta`, and `L`.

```{r}
#| code-fold: false
#| eval: false
DelayedSimpleAdstock <- function(advertising, lambda, theta, L){
  N <- length(advertising)
  weights <- matrix(0, N, N)
  for (i in 1:N){
    for (j in 1:N){
      k = i - j
      if (k < L && k >= 0){
        weights[i, j] = lambda ** ((k - theta) ** 2)
      }
    }
  }
  
  adstock <- as.numeric(weights %*% matrix(advertising))
  
  return(adstock)  
}

```

### Setting up an adstock transformation

Now we can choose the parameters for the adstock transformation. Ideally, we want a transformation that captures the decay of the advertising program (`lambda`), its delayed peak onset (`theta`), and its maximum effect duration (`L`). With a bit of simulation we can see what each parameter does across a value of different lags. The goal here is to have a function that matches what we believe the actual effect of ad spending looks like for different media regions (e.g. TV, radio, internet). Below, we can see that increasing `lambda` increases the decay of the effect up, while varying `theta` sets the peak onset of the ad campaign to later lags. The value of `L` simply sets the maximum effect to a specific lag.

```{r}
#| code-fold: false
#| echo: true

# set up grid of params to iterate over

x <- c(1, rep(0, 15))
lambda = seq(0,1, by = .1)
theta = seq(0,10, by = 1)
L = seq(1,12, by = 1)

```

::: panel-tabset
## Lambda

```{r}
#| echo: false

x <- c(1, rep(0, 15))
lambda = seq(0,1, by = .1)
theta = seq(0,10, by = 1)
L = seq(1,12, by = 1)

## A
par(mfrow = c(3, 4), mai = c(.5, 0.3, 0.2, 0.2))
for (i in lambda) {
  plot(
    DelayedSimpleAdstock(
      x,
      lambda = i,
      theta = 1,
      L = 13
    ),
    lwd = 2,
    col = "#004488",
    type = 'l',
    xlab = "",
    main = paste0("Lambda:", i)
  )
  title(xlab="Weeks", line=1.8, cex.lab=1)
}
```

## Theta

```{r}
#| echo: false
par(mfrow = c(3, 4), mai = c(.5, 0.3, 0.2, 0.2))
for (j in theta) {
  plot(
    DelayedSimpleAdstock(
      x,
      lambda = .8,
      theta = j,
      L = 13
    ),
    lwd = 2,
    col = "#BB5566",
    type = 'l',
    xlab = "",
    main = paste0("Theta:", j)
  )
  title(xlab="Weeks", line=1.8, cex.lab=1)
}
```

## L

```{r}
#| echo: false
par(mfrow = c(3, 4), mai = c(.5, 0.3, 0.2, 0.2))
for (k in L) {
  plot(
    DelayedSimpleAdstock(
      x,
      lambda = .8,
      theta = 2,
      L = k
    ),
    lwd = 2,
    col = "#DDAA33",
    type = 'l',
    xlab = "",
    main = paste0("L:", k)
  )
  title(xlab="Weeks", line=1.8, cex.lab=1)
}
```
:::

Based on a visual assesment I just chose an adstock function with a lambda of .8 (suggesting moderate decay of the initial ad effect), a theta of 2 (implying a peak onset of 2 weeks), and an L of 13 which is a rule-of-thumb that makes the maximum effect quite large.

```{r}
#| echo: false
#| fig-cap: Adstock function (Lambda = .8, theta = 2, L = 13)

par(mfrow = c(1,1))
plot(
  DelayedSimpleAdstock(
    x,
    lambda = .8,
    theta = 2,
    L = 13
  ),
  lwd = 2,
  col = "#DDAA33",
  type = 'l',
  xlab = "Weeks",
  ylab = "Adstock",
  main = "Adstock Function\n",
)
```

The code below applies our adstock function to each of the spend variables. For simplicity here I am making the assumption that all of the modes have similar adstock functions, but this can (and should) vary per modality based on expert prior information. We convert the daily data (which is quite noisy) to a more commonly utilized weekly format. We then limit the focus of our analysis to a 4-year time span.

```{r}
#| code-fold: true

# setup weekly data
# setup weekly data
mmm_weekly_data <-
  mmm_raw %>%
  mutate(date = as.Date(DATE, "%m/%d/%Y"),
         year = year(date),
         month = month(date),
         week = week(date)) %>%
  select(
    date,
    year,
    month,
    week,
    sales = `SALES ($)`,
    spend_sms = `Advertising Expenses (SMS)`,
    spend_news = `Advertising Expenses(Newspaper ads)`,
    spend_radio = `Advertising Expenses(Radio)`,
    spend_tv = `Advertising Expenses(TV)`,
    spend_net = `Advertising Expenses(Internet)`,
    demand = DEMAND,
    supply = `POS/ Supply Data`,
    price = `Unit Price ($)`
  ) %>%
  filter(year %in% 2014:2017)


weekly_spend <-
  mmm_weekly_data %>%
  group_by(year, month, week) %>%
  summarise(across(sales:spend_net, sum), across(demand:price, mean), .groups = 'drop') %>%
  mutate(index = 1:nrow(.))


# Apply transformation to advertising variables, scale dollar values to per $1,000
X <-
  weekly_spend %>%
  mutate(across(spend_sms:spend_net,~ DelayedSimpleAdstock(.,lambda = .8,theta = 2,L = 13)),
         across(spend_sms:price, function(x) x/1e3),
         trend = 1:nrow(.)/nrow(.))
```

### Setting up the model

Before we fit the model we can plot out the primary variables of interest, along with our dependent variable `sales`. Looking below we can see a few potential issues. One which should jump out immediately is that there is a *very* high correlation between several of our ad spend categories. For example, the correlation between TV spending and news spending is almost 1. In the case of MMM this is a common problem, which makes unique identification of the effect of ad spends much more difficult. More troubling, by just eyeballing these plots there doesn't seem to be a terribly strong relationship between *any* of the advertising venues and sales.

```{r}
#| fig-cap: Pairwise relationships between sales ~ ad venues
plot(X[, c('sales',
           'spend_sms',
           'spend_news',
           'spend_radio',
           'spend_tv',
           'spend_net')], col = '#004488')
```

Nor do the pairwise correlations seem to be very high either (in fact, they are very nearly zero). Regardless, we'll continue by fitting a simple set of models.

```{r}
#| code-fold: true
round(cor(X[, c('sales',
           'spend_sms',
           'spend_news',
           'spend_radio',
           'spend_tv',
           'spend_net')]),2)
```

## Fitting A Model

One of the biggest challenges with MMM is that many of the model coefficients including the advertising venues, are likely to be *very* highly correlated. For example, the advertising spend on TV ads is almost perfectly correlated with the spend on news ads. We can set some moderately strong priors on the ad spend coefficients to ensure that the estimates don't explode due to multicollinearity. Here I'm just placing a `normal(0, .5)` prior which is still pretty wide for a coefficient on the logrithmic scale.

```{r}
#| code-fold: true

# set up priors
bprior <- c(prior(normal(0,.5), class = "b", coef = "spend_sms"),
            prior(normal(0,.5), class = "b", coef = "spend_news"),
            prior(normal(0,.5), class = "b", coef = "spend_radio"),
            prior(normal(0,.5), class = "b", coef = "spend_tv"),
            prior(normal(0,.5), class = "b", coef = "spend_net"))

# just intercept
brm_fit_0 <- brm(log(sales) ~ 1, data = X,
                 chains = 4,
                 cores = 4,
                 family = gaussian(),
                 file = "C:/Users/gioc4/Documents/blog/data/brms_models/mmm_fit0.Rdata")

# no random effects, linear trend effect
brm_fit_1 <- brm(log(sales) ~                 
                   demand +
                   supply +
                   price +
                   as.factor(month) +
                   as.factor(week) +
                   trend +
                   spend_sms +
                   spend_news +
                   spend_radio +
                   spend_tv +
                   spend_net,
                 data = X,
                 chains = 4,
                 cores = 4,
                 prior = bprior,
                 family = gaussian(),
                 file = "C:/Users/gioc4/Documents/blog/data/brms_models/mmm_fit1.Rdata")

# random effects for month+week
# smoothing spline for trend
brm_fit_2 <- brm(log(sales) ~                 
                   demand +
                   supply +
                   price +
                   s(trend) +
                   spend_sms +
                   spend_news +
                   spend_radio +
                   spend_tv +
                   spend_net +
                   (1|month) +
                   (1|week),
                 data = X,
                 chains = 4,
                 cores = 4,
                 prior = bprior,
                 family = gaussian(),
                 control = list(adapt_delta = .9),
                 file = "C:/Users/gioc4/Documents/blog/data/brms_models/mmm_fit2.Rdata")

```

### Model Evaluation

Now we can evaluate the models. Here, I evaluate a model with random effects for month and week, and a smoothing spline for the trend component against a fixed effects model with a linear trend, and a "null" model with just an intercept. The model with a smooth trend spline appears to beat out the linear trend, which makes sense given the non-linear bump in sales observed in the raw data.

```{r}
#| code-fold: true

# evaluate models using leave-out-out criterion
# looks like smoothing spline is slightly better
loo_eval <- loo(brm_fit_0, brm_fit_1, brm_fit_2)
loo_eval$diffs
```

We can (and should) also check out the predictions from the model using a posterior predictive check. In Bayesian terms what this means is we take a sample of draws from our fitted model and compare them against the observed data. If our model is capturing the process well, the predicted values should generally follow the observed process. Below we see that our model does a fairly decent job.

```{r}
#| code-fold: true
#| fig-cap: "Posterior predictive check, smoothing spline model"
pp_check(brm_fit_2)
```

Finally, we can look at some of the model coefficients

```{r}
summary(brm_fit_2)
```

### Model Predictions

And here's the estimated lift for Radio by week. While there is definitely some lift, it is pretty tiny here

::: column-page
```{r}
#| code-fold: true
#| layout-ncol: 2


# get predictions 
X_no_radio <- X %>% mutate(spend_radio= 0)

pred1 <- predict(brm_fit_2)
pred1_no_radio <- predict(brm_fit_2, newdata = X_no_radio)

pred_dataframe <-
  cbind.data.frame(week = 1:197,
                   obs = pred1[, 1],
                   pred = pred1_no_radio[, 1]) %>%
  pivot_longer(-week) %>%
  group_by(week) %>%
  mutate(diff = value - lead(value, 1))

# predicted (all) vs predicted (no radio)
ggplot(pred_dataframe) +
  geom_line(aes(x = week, y = value, color = name)) +
  theme_bw() +
  scale_color_manual(values = c("#0077BB", "#EE7733")) +
  labs(y = "(log) Sales", x = "Week") +
  theme(legend.position = 'none')

# predicted lift from radio
pred_dataframe %>%
  na.omit() %>%
  ggplot() +
  geom_line(aes(x = week, y = diff)) +
  theme_bw() +
  labs(y = "(log) Sales", x = "Week")
  
```
:::

If we average the estimated mean lift across the entire time frame we get an additional value of about 2%.

```{r}
mean((pred1[,1] - pred1_no_radio[,1])/pred1[,1])
```

At this point there is a *lot* of additional work that can be done. Most applied uses of MMM apply some optimization algorithms to determine the best ad spend mix given a fixed budget. The data I have here isn't really good enough to delve any deeper into - but its important to note that fitting the model is really only the beginning.

## In Closing: My Take

My biggest problem with Mixed Media Modelling is that it seems like it is easy to implement badly, but much harder to do well. Not only do you have to appropriately model an adstock function for your advertising venues, you also have very high correlation between your variables. This, in turn, makes the choice of model specification even more important because the coefficients with be *highly* sensitive. Personally, a true experiment or even quasi-experiment would be preferable to this - although I'm all too aware that this is often impossible. Like everything there is no magic bullet, and choosing one approach over another will always introduce trade offs.
