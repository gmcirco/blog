---
title: "'Vibe Coding' my way into a RAG pipeline"
subtitle: "Retrieval-augmented generation with a little help from a friend."
author: Gio Circo, Ph.D.
date: 2025-3-10
categories:
  - Python
  - Data Science Applications
format: 
    html:
        self-contained: true
        code-fold: false
        mainfont: "Roboto"
        section-divs: true
        toc: true
        title-block-banner: true
        mermaid:
            theme: neutral
theme: flatly
image: vibes.png
---

```{python}
#| include: false
%cd C:\Users\gioc4\Documents\blog\gcirco_blog\posts\prompt-testing

import pandas as pd
import json 
from src.rag import create_prompt_rules, search_vector_database

narratives = pd.read_csv("C:\\Users\\gioc4\\Documents\\blog\\gcirco_blog\\posts\\prompt-testing\\data\\train_narratives_sample_200.csv")

# load original prompts and output

with open("C:\\Users\\gioc4\\Documents\\blog\\gcirco_blog\\posts\\prompt-testing\\json\\output_2025-03-10.jsonl") as f:
    prompts = [json.loads(line) for line in f]


with open("C:\\Users\\gioc4\\Documents\\blog\\gcirco_blog\\posts\\prompt-testing\\json\\batch_67cf174680b08190b7e5b5412c4d5d3b_output.jsonl") as f:
    output = [json.loads(line) for line in f]



```

*There are synthesized depictions of self-harm and suicide in this blog post.*

## LLMs and RAG

For most people who are up-to-date in tech, large language models (LLMs) are nothing new. In fact, they are downright pervasive. One of the largest challenges with LLMs today is that they are very good at coming up with plausible-sounding answers, but less good at telling us *why* or *where* they got that answer from. Many times we have to cast a critical eye at LLM output and determine whether or not it is real or just a hallucination. From the beginning, it was suggested that we could augment a user's prompt with a set of relevent documents that could be used to verify the output of the LLM, or improve the prompt itself. For example, imagine we [wrote a prompt](https://chatgpt.com/share/67cf3c21-fd18-8007-a043-6eaec436e169) asking an LLM about [parking restrictions](https://newyork.public.law/laws/n.y._vehicle_and_traffic_law_section_1202) in New York State:

> **Q:** Is it permitted to park between roadways on a highway according to New York State law?\
> **A:** According to New York State law, parking between roadways on a highway is not permitted, except where official signs indicate otherwise. Specifically, Section 1200 of the New York Vehicle and Traffic Law states that parking is prohibited in areas such as medians, between divided roadways, and other restricted zones unless otherwise posted.

versus:

> **Q:** Is it permitted to park between roadways on a highway according to New York State law? Review the following rules from the New York Vehicle and Traffic Law. Provide direct quotes to support your answer: *\[full text of Vehicle & Traffic Law Section 1202\]*
>
> **A:** No, it is not permitted according to New York Vehicle and Traffic Law. The law explicitly states:
>
> > *“No person shall stop, stand or park a vehicle... in the area between roadways of a divided highway, including crossovers, except in an emergency.”*
>
> This is found under Section 1(i) of the provided rules. The only exception is in the case of an emergency.

This is a bit of a contrived example, but the general idea here is that we can improve the LLMs by providing them relevant external information alongside their instructions. By including extra information this helps to guard against hallucinations, and can also give the user more guidance on why the LLM came to its specific answer. In our answer above, the LLM is technically correct in the first answer that parking is not permitted - but I think it invents a rule about official signs allowing otherwise. In the prompt containing the full-text of the relevent set of rules, we get a much shorter, cleaner response with the precise rule relevant to the question.

## Coding Out a RAG Pipeline

In a [recent blog post](https://gmcirco.github.io/blog/posts/prompt-testing/prompt_testing.html) I walked through a step-by-step process of how to set up a A/B testing process for prompt refinement. I relied on data from a recent [DrivenData](https://www.drivendata.org/competitions/295/cdc-automated-abstraction/page/917/) competition that relied on youth suicide narrative reports from the [National Violent Death Reporting System](https://www.cdc.gov/nvdrs/resources/nvdrsCodingManual.pdf). I was pretty happy with the workflow I built out, but couldn't help but feel that I could improve it somehow by giving the LLM direct access to the relevant sections from the coding manual. Here is where RAG comes in! Rather than prompting the model with upwards of 200 pages of text, of which it might need less than 1 page, I could pass just the smallest subsections for each question.

### My mental model

The way I envisioned this working was to process the RAG step separately by indexing the relevant sub-sections from section 5 of NVDRS coding manual. I would extract out the subsection chunks and then index them in vector database for retreval at the time of prompt creation. My prompt creator class adds the headers, instructions, and questions to the final prompt, and then we tack on the relevant rules from the vector database (see below):

```{mermaid}
flowchart LR
    %% Improved node styling
    classDef input fill:#c4e3f3,stroke:#5bc0de,stroke-width:2px,color:#31708f
    classDef process fill:#d9edf7,stroke:#5bc0de,stroke-width:2px,color:#31708f
    classDef database fill:#dff0d8,stroke:#5cb85c,stroke-width:2px,color:#3c763d
    classDef output fill:#fcf8e3,stroke:#f0ad4e,stroke-width:2px,color:#8a6d3b
    
    %% Main components with better descriptions
    A["NVDRS Manual<br/>(Source Document)"] -->|"Reference material"| B
    B["RAG Model<br/>(Retrieval System)"] --> D
    C["Narrative Text<br/>(Case Information)"] -->|"Contains: '...victim felt depressed..'"| D
    C --> E
    
    %% Database and outputs
    D[("Vector Database<br/>(Knowledge Store)")] -->|"Retrieved: '5.3.4 Current depressed mood:'"| F
    E["Prompt Creator<br/>(Question Generator)"] -->|"Generates: Q1, Q2, Q3"| F
    
    %% Final output
    F["Final Prompt<br/>(For LLM Processing)"]
    
    %% Apply styles
    class A,C input
    class B,E process
    class D database
    class F output

```

In my mind, I figured I could come up with a quick and dirty solution by using regex to hit on key words in the narrative, and then use a semantic similarity model (like `SentenceTransformers`) to retrieve the top $n$ rules. For example, a narrative might have a section stating:

> "Victim had been feeling **depressed** and sad in the days leading up to the incident"

We use regex to grab the relevant words around our matched word (here, **depressed**), encode them, and then retrieve rules from the vector database. In the last step we append these to our prompt before executing it.

There's just one problem - I've never done this before.

### Vibe-Coding

What is "vibe coding"? One of my favorite definitions comes from ex-OpenAI founder Andrej Karpathy:

> "There's a new kind of coding I call "vibe coding", where you fully give in to the vibes, embrace exponentials, and forget that the code even exists"

In short, it represents a programmer's full surrender to the LLM, and taking what it gives back on good faith. When problems arrive, you just dig deeper and let the LLM guide you even further down the rabbit hole, trusting the process. I think the term is very funny - but there is a bit of truth to this. "Vibe-Coding" is sort of what I used to do early in grad school when I was trying to get some esoteric model running in R with virtually no background knowledge. To me, vibe-coding harkens back to the days of panicked copy-and-paste from a variety Stack Overflow posts.

With this in mind, I believe in sharing my work. Here's the [full conversation](https://claude.ai/share/9ae9a888-def0-4e61-96c0-6a795d5d4ad8) I used to set up the RAG framework. I had enough of an idea of what I wanted, but wanted to speed up the code required to get the document chunking and indexing

## Testing the RAG Process

So what did all that get us? Well, with the help of Claude we got a set of four functions that[^1]:

[^1]: If you are curious about the full code, you can look at my prompt-testing repo under my blog posts that contains the full set

1.  Extract the relevant pages from the coding manual.
2.  Chunk up the pages into subsections based on headers.
3.  Encode these chunks using a `SentenceTransformers` model.
4.  Save the embedded chunks and the section indices in a vector database.

as well as two others:

5.  A function to query and retrieve results from the vector database.
6.  A function to append the results into a prompt-friendly text object.

I took the code and made some adjustments (maybe 10-15% or less) and then put them into their own .py file under my src folder. I then created a separate file `index_rules.py` to perform all the steps and locally store the vector database in a cache folder:

```{python}
#| eval: false

"Code to index rules from the NVDRS and store as vector store in cache"

from pypdf import PdfReader
from src.rag import (
    extract_pages,
    chunk_by_subsections_with_codes,
    encode_chunks,
    create_vector_store,
)

# import the full nvdrs coding manual
# we only need a subset of pages on circumstances
# page 74 - 149
page_min = 74
page_max = 148
cache_dir = "cache/"

reader = PdfReader("reference/nvdrsCodingManual.pdf")

# extract pages, chunk subsections, then store in cache

pages_circumstances = extract_pages(reader, page_min, page_max)
section_circumstances = chunk_by_subsections_with_codes(pages_circumstances)
section_embeddings = encode_chunks(section_circumstances)
index, stored_chunks = create_vector_store(section_embeddings, cache_dir)
```

With that done, the other adjustment I needed to make was to add the ability to query the vector database and return relevant coding rules based on matching key words in the narrative. What I did was set up a dict containing key words for each major question, and a query term to append to the retrieved text substring. So, for example, given a narrative like this:

> "Victim was at home and complained about feeling sad and depressed. Victim had been treated for ADHD and bipolar disorder and had reportedly not been taking his medications in the days preceeding"

We would use a regex pattern to match 30 characters on either side of a matching keyword in the dict of keywords (one selected here below):

```{python}
#| eval: false

keyterms = {
    "DepressedMood": {
        "Terms": [
            "depressed",
            "depressive symptoms",
            "sad",
            "unhappy",
            "low mood",
            "feeling down",
            "persistent sadness",
            "major depression",
            "melancholy",
            "hopeless",
            "despair",
            "gloomy",
            "emotional distress",
            "tearful",
            "loss of interest",
            "worthlessness",
            "self-loathing",
        ],
        "Query": "Coding rules for DepressedMood",
    }

```

We then just loop through the dict of keywords and collect all the hits.

To illustrate: passing this example narrative into our `search_vector_database` performs the steps of searching for all regex hits, encodes the matching narrative text, and then queries it against the vector database.  We take all of results from the vector database search and pass these into another function that prepares it for insertion to the prompt. The `create_prompt_rules` function adds a header for the section for coding rules, and organizes them in order of section header. The code below shows a successful retreval for the DepressedMood variable:

```{python}
test_narrative = "Victim was at home and complained about feeling sad and depressed. Victim had told his partner that he was thinking about taking his own life."

val, matched_variables = search_vector_database(test_narrative, 1, "cache/rules_index.faiss", "cache/rule_chunks.pkl")
PROMPT_RULES = create_prompt_rules(val, matched_variables)

print(PROMPT_RULES)
```

## Adding it All Together

Now that I had the LLM stuff mostly incorporated, all I needed to do is append this new RAG workflow to my old LLM class. I added an extra parameter named `include_rag` that triggered the RAG process and appended it to the prompt if specified by the user:

```{python}
#| eval: false

def standard_prompt_caching(
        self,
        header: str | list = None,
        narrative: str | list = None,
        body: str | list = None,
        example_output: str | list = None,
        footer: str | list = None,
        include_rag: bool | list = False,
        **kwargs
    ) -> list:
        """Create multiple standard prompts based on all combinations of list elements.
        This puts the narrative at the end to support OpenAI prompt caching.
        """

        # Ensure all inputs are lists for consistent iteration
        if include_rag:
            val, matched_variables = search_vector_database(
                narrative,
                2,
                "cache/rules_index.faiss",
                "cache/rule_chunks.pkl",
            )
            rag = create_prompt_rules(val, matched_variables)
            params = [body, example_output, rag, footer, header, narrative]
        else:
            params = [body, example_output, footer, header, narrative]
        param_lists = [
            [item] if not isinstance(item, list) else item for item in params
        ]
```

The final result looks like this, which is structurally almost identical to the non-RAG version I did before - in fact, the only real change is adding `include_rag = True` to the LLM class parameters. This is all the code that processes the queries and passes them on to the OpenAI API:

```{python}
#| eval: false
#| code-fold: true

import pandas as pd
import json
from datetime import datetime

from openai import OpenAI
from src.prompts import HEADER1, BODY2, EXAMPLE_OUTPUT2
from src.prompt_creation import Prompt

client = OpenAI()
run_date = datetime.now().strftime("%Y-%m-%d")

# set up prompt
prompt_creator = Prompt()
ROLE = """You are a mental health expert reviewing law enforcement narratives of youth suicide incidents. 
Your task is to label variables relating to the incident. Closely review the following instructions. Read 
the provided narrative and then add labels corresponding to the variables into the described JSON format. 
Do NOT deviate from the instructions. If coding rules are present you may use them to guide your analysis. 
Do NOT rely solely on the rules.
"""

# load suicide narratives and labels
narratives = pd.read_csv("data/train_narratives_sample_200.csv")
labels = pd.read_csv("data/train_labels_sample_200.csv")


# Execute 1 version of prompt with RAG
json_list = []

for row in narratives.iterrows():

    # grab the unique id and text
    single_narrative = row[1]
    id = single_narrative["uid"]
    txt = single_narrative["NarrativeLE"] + single_narrative["NarrativeCME"]

    prompt_input = {
        "header": HEADER1,
        "narrative": txt,
        "body": BODY2,
        "example_output": EXAMPLE_OUTPUT2,
        "footer": None,
        "include_rag": True
    }

    # create a prompt, pass in the text narrative
    prompt_versions = prompt_creator.standard_prompt_caching(**prompt_input)

    version_num = 0
    for prompt in prompt_versions:
        # now append to list
        json_list.append(
            {
                "custom_id": f"{id}_{version_num}",
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": "gpt-4o-mini",
                    "messages": [
                        {"role": "system", "content": ROLE},
                        {"role": "user", "content": prompt},
                    ],
                    "max_tokens": 500,
                    "response_format": { "type": "json_object" },
                },
            }
        )
        version_num += 1


with open(f"json/output_{run_date}.jsonl", "w") as outfile:
    for entry in json_list:
        json.dump(entry, outfile)
        outfile.write("\n")

# upload batch to openai
batch_input_file = client.files.create(
    file=open(f"json/output_{run_date}.jsonl", "rb"), purpose="batch"
)

batch_input_file_id = batch_input_file.id
client.batches.create(
    input_file_id=batch_input_file_id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
    metadata={"description": "Batch Testing 1 prompt x 200 examples with caching and RAG"},
)


```


### Reviewing the output

Finally we can review the input and output from this. Here is the full un-edited prompt with the RAG element added. The RAG section is near the end of the prompt, just above full narrative text:

*[Warning: Depictions of Self-Harm]*

<details>
<summary>Full RAG Input Prompt</summary>
```{python}
#| echo: false
print(prompts[58]['body']['messages'][1]['content'])
```
</details>

And the output:

<details>
<summary>JSON Output</summary>
```{python}
#| echo: false
print(output[58]['response']['body']['choices'][0]['message']['content'])
```
</details>

I didn't ask for any reasoning or citations for the rules provided. Here I just wanted the raw JSON output - but it would be trivial to add another section and ask for specific citations when the rules provided were applied. Looking at the retreval part I am actually quite surprised how well it was able to pull the relevant rules based on keyword hits.

## My Take

To be honest, I was surprised that most of the Claude-generated code worked as well as it did. I had to make few substantial changes. The code to do the chunking, embedding, and indexing took maybe under 30 minutes for me to read through, edit slightly, and execute. Adding these functions into my existing workflow took under and hour - so maybe 90 minutes total from prompt to working RAG proof-of-concept. To be honest, totally wild. I could have figured this out on my own, but this was like half a day of work, compared to the week it would take me to do it solely by scratch.

Here's the rub - I think "vibe coding" can be helpful to jumpstart a project from nothing to a workable proof-of-concept. I do NOT think it is a good idea to rely 100% on AI-generated code without knowing what it is actually doing. Personally, I think its a good idea to get a working POC and then pick apart all of the functions and steps the LLM generated to understand whats happening. Blind coding can certainly be a "vibe" but I don't think it is sustainable for real-world production-level code.
