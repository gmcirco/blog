---
title: "'Vibe Coding' my way into a RAG pipeline"
subtitle: "Retrieval-augmented generation with a little help from a friend."
author: Gio Circo, Ph.D.
date: 2025-3-14
categories:
  - Python
  - Data Science Applications
format: 
    html:
        self-contained: true
        code-fold: false
        mainfont: "Roboto"
        section-divs: true
        toc: true
        title-block-banner: true
        mermaid:
            theme: neutral
theme: flatly
image: vibes.png
---

```{python}
#| include: false
%cd C:\Users\gioc4\Documents\blog\gcirco_blog\posts\prompt-testing

import pandas as pd
import json 
from src.rag import create_prompt_rules, search_vector_database

narratives = pd.read_csv("C:\\Users\\gioc4\\Documents\\blog\\gcirco_blog\\posts\\prompt-testing\\data\\train_narratives_sample_200.csv")

# load original prompts and output

with open("C:\\Users\\gioc4\\Documents\\blog\\gcirco_blog\\posts\\prompt-testing\\json\\output_2025-03-10.jsonl") as f:
    prompts = [json.loads(line) for line in f]


with open("C:\\Users\\gioc4\\Documents\\blog\\gcirco_blog\\posts\\prompt-testing\\json\\batch_67cf174680b08190b7e5b5412c4d5d3b_output.jsonl") as f:
    output = [json.loads(line) for line in f]



```

## LLMs and RAG

For most people who are up-to-date in tech, large language models (LLMs) are nothing new. In fact, they are downright pervasive. One of the largest challenges with LLMs today is that they are very good at coming up with plausible-sounding answers, but less good at telling us *why* or *where* they got that answer from. It's often good practice to cast a critical eye at LLM output and determine whether or not it is real or just a hallucination. These concerns have led to suggestions that we can augment a user's prompt with a set of reference documents that can be used to verify the output of the LLM, or to improve the prompt itself. For example, imagine we [wrote a prompt](https://chatgpt.com/share/67cf3c21-fd18-8007-a043-6eaec436e169) asking an LLM about [parking restrictions](https://newyork.public.law/laws/n.y._vehicle_and_traffic_law_section_1202) in New York State:

> **Q:** Is it permitted to park between roadways on a highway according to New York State law?
> **A:** According to New York State law, parking between roadways on a highway is not permitted, except where official signs indicate otherwise. Specifically, Section 1200 of the New York Vehicle and Traffic Law states that parking is prohibited in areas such as medians, between divided roadways, and other restricted zones unless otherwise posted.

versus:

> **Q:** Is it permitted to park between roadways on a highway according to New York State law? Review the following rules from the New York Vehicle and Traffic Law. Provide direct quotes to support your answer: *\[full text of Vehicle & Traffic Law Section 1202\]*
>
> **A:** No, it is not permitted according to New York Vehicle and Traffic Law. The law explicitly states:
>
> > *“No person shall stop, stand or park a vehicle... in the area between roadways of a divided highway, including crossovers, except in an emergency.”*
>
> This is found under Section 1(i) of the provided rules. The only exception is in the case of an emergency.

This is a bit of a contrived example, but the general idea is that we can improve the LLMs performance by providing relevant external information alongside the standard instructions. Including this extra information helps to guard against hallucinations, and also gives the user more guidance on why the LLM came to its specific answer. In the example above, the LLM is *technically* correct in the first answer that parking is not permitted - but I think it invents a rule about official signs allowing otherwise. This is probably speculation on the part of the LLM. In the prompt containing the full-text of the relevent set of rules, we obtain a shorter, cleaner response with the precise rule relevant to the question - including the citation to the exact section and subsection.

### Retrieval-augmented generation

The example I've described above is essentially RAG or Retrieval-augmented generation. RAG is another one of those buzzwords that enters the conversation every time "AI" comes up. I wrote this blog post, in part, to demystify it for my readers (all 3 of your) and myself. When you boil down the workflow, RAG is essentially an additional step of retrieving relevant information from a database or document store before generating a response. Instead of relying solely on a model’s pre-trained knowledge, the RAG step pulls in context-specific data relevant to the question. This approach is particularly useful when dealing with rapidly changing information or domain-specific knowledge. 

## Coding Out a RAG Pipeline

In a [recent blog post](https://gmcirco.github.io/blog/posts/prompt-testing/prompt_testing.html) I walked through a step-by-step process of how to set up a A/B testing process for prompt refinement. I relied on data from a recent [DrivenData](https://www.drivendata.org/competitions/295/cdc-automated-abstraction/page/917/) competition that used youth suicide narrative reports from the CDC's [National Violent Death Reporting System](https://www.cdc.gov/nvdrs/resources/nvdrsCodingManual.pdf). I was pretty happy with the workflow I built out, but couldn't help but feel that I could improve it somehow. The NVDRS has nearly 300 page coding manual with detailed instructions for each variable type. For example, section 5.5.9 describes the rules of how to code a death that is the result of a gang-related crime:

> 5.5.9 Gang-related: CME/LE_GangRelated  
> **Definition**  
> Definitions for gang-related homicide can vary by law enforcement agency or CME and tend to capture  
> deaths that are classified as gang-motivated (i.e., the motive of the incident was gang-related) or had  
> suspected involvement of a gang member (i.e., a gang member was a suspect or victim in the incident).  
> This variable captures both types of gang-related deaths reported by agencies.  
>  
> **Response Options**  
> - **0** No, Not available, Unknown  
> - **1** Yes, gang motivated  
> - **2** Yes, suspected gang member involvement  
> - **3** Yes, gang-related not otherwise specified  
> - **4** Organized crime including motorcycle gangs, mafia, and drug cartels  

There are nearly a hundred such rules in this manual. However, it doesn't really make sense to try and pass the entire manual into our prompt. Even if the context window could handle the full size, when you are paying for tokens it makes sense to minimize the prompt to only the necessary bits. So what if we just wanted to augment each prompt with *only* the references that are relevant for the narrative being coded? This is where RAG comes in! 

### Envisioning a RAG pipeline

The way I envisioned this working was to process the RAG step separately by first indexing the relevant sub-sections from section 5 of NVDRS coding manual, which contains all information about incident circumstances. I would extract out the subsection chunks and then index them in vector database for retreval at the time of prompt creation. My prompt creator class already adds the headers, instructions, and questions to the final prompt, so all we need to add is the additional step of then tacking on the relevant rules from the vector database (see below):

```{mermaid}
flowchart LR
    %% Improved node styling
    classDef input fill:#c4e3f3,stroke:#5bc0de,stroke-width:2px,color:#31708f
    classDef process fill:#d9edf7,stroke:#5bc0de,stroke-width:2px,color:#31708f
    classDef database fill:#dff0d8,stroke:#5cb85c,stroke-width:2px,color:#3c763d
    classDef output fill:#fcf8e3,stroke:#f0ad4e,stroke-width:2px,color:#8a6d3b
    
    %% Main components with better descriptions
    A["NVDRS Manual<br/>(Source Document)"] -->|"Reference material"| B
    B["RAG Model<br/>(Retrieval System)"] --> D
    C["Narrative Text<br/>(Case Information)"] -->|"Contains: '...victim felt depressed..'"| D
    C --> E
    
    %% Database and outputs
    D[("Vector Database<br/>(Knowledge Store)")] -->|"Retrieved: '5.3.4 Current depressed mood:'"| F
    E["Prompt Creator<br/>(Question Generator)"] -->|"Generates: Q1, Q2, Q3"| F
    
    %% Final output
    F["Final Prompt<br/>(For LLM Processing)"]
    
    %% Apply styles
    class A,C input
    class B,E process
    class D database
    class F output

```

In my mind, I figured I could come up with a quick and dirty solution by using regex to hit on key words in each narrative, and then use a semantic similarity model (like `SentenceTransformers`) to retrieve the top $n$ rules from the vector database. For example, a narrative might have a section stating:

> "Victim had been feeling **depressed** and sad in the days leading up to the incident"

Using regex we can grab the relevant words around our matched word (here, **depressed**), encode them, and then retrieve rules from the vector database. In the last step we append these to our prompt before executing it.

There's just one problem - I've never done this before.

### Vibe-Coding

What is "vibe coding"? One of my favorite definitions comes from ex-OpenAI founder Andrej Karpathy:

> "There's a new kind of coding I call "vibe coding", where you fully give in to the vibes, embrace exponentials, and forget that the code even exists"

In short, it represents a programmer's full surrender to the LLM, and taking what it gives back on good faith. When problems arrive, you just dig deeper and let the LLM guide you even further down the rabbit hole, trusting the process. I think the term is very funny - but there is a bit of truth to this. "Vibe-coding" is sort of what I used to do early in grad school when I was trying to get some esoteric model running in R with virtually no background knowledge. To me, vibe-coding harkens back to the days of panicked copy-and-paste from a variety Stack Overflow posts.

With this in mind, I believe in sharing my work. Here's the [full conversation](https://claude.ai/share/9ae9a888-def0-4e61-96c0-6a795d5d4ad8) I used to set up the RAG framework. I had enough of an idea of what I wanted, but wanted to speed up the processing of generating code required to get the document chunking and indexing working.

## Testing the RAG Process

So what did all that get us? Well, with the help of Claude we got a set of four functions that[^1]:

[^1]: If you are curious about the code, you can look at my [prompt-testing repo](https://github.com/gmcirco/blog/tree/master/posts/prompt-testing/src) under my blog posts that contains the full set of classes and functions.

1.  Extract the relevant pages from the coding manual.
2.  Chunk up the pages into subsections based on headers.
3.  Encode these chunks using a `SentenceTransformers` model.
4.  Save the embedded chunks and the section indices in a vector database.

as well as two others:

5.  A function to query and retrieve results from the vector database.
6.  A function to append the results into a prompt-friendly text object.

I took the LLM-generated code and made some adjustments (maybe 10-15% or less) and then put them into their own .py file under my src folder. I then created a separate file `index_rules.py` to perform all the steps and locally store the vector database in a cache folder:

```{python}
#| eval: false

"Code to index rules from the NVDRS and store as vector store in cache"

from pypdf import PdfReader
from src.rag import (
    extract_pages,
    chunk_by_subsections_with_codes,
    encode_chunks,
    create_vector_store,
)

# import the full nvdrs coding manual
# we only need a subset of pages on circumstances
# page 74 - 149
page_min = 74
page_max = 148
cache_dir = "cache/"

reader = PdfReader("reference/nvdrsCodingManual.pdf")

# extract pages, chunk subsections, then store in cache

pages_circumstances = extract_pages(reader, page_min, page_max)
section_circumstances = chunk_by_subsections_with_codes(pages_circumstances)
section_embeddings = encode_chunks(section_circumstances)
index, stored_chunks = create_vector_store(section_embeddings, cache_dir)
```

With that done, the other adjustment I needed to make was to add the ability to query the vector database and return relevant coding rules based on matching key words in the narrative. What I did was set up a dict containing key words for each major question, and a query term to append to the retrieved text substring. So, for example, given a narrative like this:

> "Victim was at home and complained about feeling sad and depressed. Victim had been treated for ADHD and bipolar disorder and had reportedly not been taking his medications in the days preceeding"

We would use a regex pattern to match 30 characters on either side of a matching keyword in the dict of keywords (one selected here below):

```{python}
#| eval: false

keyterms = {
    "DepressedMood": {
        "Terms": [
            "depressed",
            "depressive symptoms",
            "sad",
            "unhappy",
            "low mood",
            "feeling down",
            "persistent sadness",
            "major depression",
            "melancholy",
            "hopeless",
            "despair",
            "gloomy",
            "emotional distress",
            "tearful",
            "loss of interest",
            "worthlessness",
            "self-loathing",
        ],
        "Query": "Coding rules for DepressedMood",
    }

```

We then just loop through the dict of keywords and collect all the hits.

To illustrate: passing this example narrative into the `search_vector_database` performs the steps of searching for all regex hits, encodes the matching narrative text, and then queries it against the vector database. It then takes all of the results from the vector database search and passes these into another function that prepares it for insertion to the prompt. The `create_prompt_rules` function adds a header for the section for coding rules, and organizes them in order of section header. The code below shows a successful retreval for the `DepressedMood` variable:

```{python}
test_narrative = "Victim was at home and complained about feeling sad and depressed. Victim had told his partner that he was thinking about taking his own life."

val, matched_variables = search_vector_database(test_narrative, 1, "cache/rules_index.faiss", "cache/rule_chunks.pkl")
PROMPT_RULES = create_prompt_rules(val, matched_variables)

print(PROMPT_RULES)
```

## Adding it All Together

Now that I had the LLM stuff mostly incorporated, all I needed to do is append this new RAG workflow to my old LLM class. I added an extra parameter named `include_rag` that triggers the RAG process and appended it to the prompt if specified by the user:

```{python}
#| eval: false

def standard_prompt_caching(
        self,
        header: str | list = None,
        narrative: str | list = None,
        body: str | list = None,
        example_output: str | list = None,
        footer: str | list = None,
        include_rag: bool | list = False,
        **kwargs
    ) -> list:
        """Create multiple standard prompts based on all combinations of list elements.
        This puts the narrative at the end to support OpenAI prompt caching.
        """

        # Ensure all inputs are lists for consistent iteration
        if include_rag:
            val, matched_variables = search_vector_database(
                narrative,
                2,
                "cache/rules_index.faiss",
                "cache/rule_chunks.pkl",
            )
            rag = create_prompt_rules(val, matched_variables)
            params = [body, example_output, rag, footer, header, narrative]
        else:
            params = [body, example_output, footer, header, narrative]
        param_lists = [
            [item] if not isinstance(item, list) else item for item in params
        ]
```

The final result looks like this, which is structurally almost identical to the non-RAG version I did before. In fact, the only real change is adding `include_rag = True` to the LLM class parameters. This is all the code that processes the queries and passes them on to the OpenAI API:

```{python}
#| eval: false
#| code-fold: true

import pandas as pd
import json
from datetime import datetime

from openai import OpenAI
from src.prompts import HEADER1, BODY2, EXAMPLE_OUTPUT2
from src.prompt_creation import Prompt

client = OpenAI()
run_date = datetime.now().strftime("%Y-%m-%d")

# set up prompt
prompt_creator = Prompt()
ROLE = """You are a mental health expert reviewing law enforcement narratives of youth suicide incidents. 
Your task is to label variables relating to the incident. Closely review the following instructions. Read 
the provided narrative and then add labels corresponding to the variables into the described JSON format. 
Do NOT deviate from the instructions. If coding rules are present you may use them to guide your analysis. 
Do NOT rely solely on the rules.
"""

# load suicide narratives and labels
narratives = pd.read_csv("data/train_narratives_sample_200.csv")
labels = pd.read_csv("data/train_labels_sample_200.csv")


# Execute 1 version of prompt with RAG
json_list = []

for row in narratives.iterrows():

    # grab the unique id and text
    single_narrative = row[1]
    id = single_narrative["uid"]
    txt = single_narrative["NarrativeLE"] + single_narrative["NarrativeCME"]

    prompt_input = {
        "header": HEADER1,
        "narrative": txt,
        "body": BODY2,
        "example_output": EXAMPLE_OUTPUT2,
        "footer": None,
        "include_rag": True
    }

    # create a prompt, pass in the text narrative
    prompt_versions = prompt_creator.standard_prompt_caching(**prompt_input)

    version_num = 0
    for prompt in prompt_versions:
        # now append to list
        json_list.append(
            {
                "custom_id": f"{id}_{version_num}",
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": "gpt-4o-mini",
                    "messages": [
                        {"role": "system", "content": ROLE},
                        {"role": "user", "content": prompt},
                    ],
                    "max_tokens": 500,
                    "response_format": { "type": "json_object" },
                },
            }
        )
        version_num += 1


with open(f"json/output_{run_date}.jsonl", "w") as outfile:
    for entry in json_list:
        json.dump(entry, outfile)
        outfile.write("\n")

# upload batch to openai
batch_input_file = client.files.create(
    file=open(f"json/output_{run_date}.jsonl", "rb"), purpose="batch"
)

batch_input_file_id = batch_input_file.id
client.batches.create(
    input_file_id=batch_input_file_id,
    endpoint="/v1/chat/completions",
    completion_window="24h",
    metadata={"description": "Batch Testing 1 prompt x 200 examples with caching and RAG"},
)


```

### Reviewing the output

Finally we can review the input and output from this. Here is the full un-edited prompt with the RAG element added. The RAG section is near the end of the prompt, just above full narrative text:

*\[Warning: Depictions of Self-Harm\]*

<details>

<summary>Full RAG Input Prompt</summary>

```{python}
#| echo: false
print(prompts[58]['body']['messages'][1]['content'])
```

</details>

And the output:

<details>

<summary>JSON Output</summary>

```{python}
#| echo: false
print(output[58]['response']['body']['choices'][0]['message']['content'])
```

</details>

I didn't ask for any reasoning or citations for the rules provided. Here I just wanted the raw JSON output - but it would be trivial to add another section and ask for specific citations when the rules provided were applied. Looking at the retrieval part I am actually quite surprised how well it was able to pull the relevant rules based on keyword hits.

## My Take

To be honest, I was surprised that most of the Claude-generated code worked as well as it did. I had to make few substantial changes. The code to do the chunking, embedding, and indexing took maybe under 30 minutes for me to read through, edit slightly, and execute. Adding these functions into my existing workflow took under and hour - so maybe 90 minutes total from prompt to working RAG proof-of-concept. To be honest, totally wild. I could have figured this out on my own, but this was like half a day of work, compared to the week it would take me to do it solely by scratch.

Here's the rub - I think "vibe coding" can be helpful to jump start a project from nothing to a workable proof-of-concept. I do NOT think it is a good idea to rely 100% on AI-generated code without knowing what it is actually doing. Personally, I think its a good idea to get a working POC and then pick apart all of the functions and steps the LLM generated to understand whats happening. Blind coding can certainly be a "vibe" but I don't think it is sustainable for real-world production-level code.