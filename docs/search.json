[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m Gio Circo, a Ph.D. in criminology and former assistant professor. I currently work at Gainwell technologies as a data scientist. My work primarily focuses on causal inference, Bayesian statistics, and anomaly detection.\nI’m big into food, wine, and spirits - especially cocktails."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gio Circo, Ph.D.",
    "section": "",
    "text": "Part 1: Principal components anomaly detector\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nCar Crashes in Austin\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nAdventures in outlier detection\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nGio Circo, Ph.D\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html",
    "href": "posts/austin-vehicle/spatial_merging.html",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\n\n\n# DATA SOURCES AND INFO\n# -----------------------------#\n# https://data.austintexas.gov\n#   vehicle crashes: 'Transportation-and-Mobility/Vision-Zero-Crash-Report-Data-Crash-Level-Records'\n#   traffic cameras: 'Transportation-and-Mobility/Traffic-Cameras/'\n#   austin council map: 'dataset/Boundaries-City-of-Austin-Council-Districts'\n\n# Select a specific Austin Council District and year\n# see: https://maps.austintexas.gov/GIS/CouncilDistrictMap/\ncnl <- 3\nyr <- 2022\n\n\n# DATA LOADING\n# -----------------------------#\n\n# Get Austin shapefile, pull only the district we need\naustin <- st_read(\"C:/Users/gioc4/Documents/blog/data/austin_city.shp\", quiet = TRUE) %>%\n  st_transform(crs = 32614) %>%\n  filter(council_di %in% cnl)\n\n# Read traffic camera data & vehicle crash data\n# Limit crashes to a specific year, conver to spatial\ncamera <- st_read(\"C:/Users/gioc4/Documents/blog/data/traffic_camera.shp\", quiet = TRUE) %>%\n  filter(camera_sta == \"TURNED_ON\") %>%\n  distinct(geometry, .keep_all = TRUE) %>%\n  st_transform(crs = 32614) %>%\n  mutate(camera_X = st_coordinates(.)[,1],\n         camera_Y = st_coordinates(.)[,2])\n\ncrash <- read_csv(unz(\"C:/Users/gioc4/Documents/blog/data/austin_crash.zip\",\"crash_data.csv\")) %>%\n  mutate(crash_date = strptime(crash_date, format=\"%m/%d/%Y %H:%M\")) %>%\n  filter(year(crash_date) == yr)\n\n# Convert crash to sf, extract coordinates\ncrash_sf <- crash %>%\n  filter(!is.na(latitude), !is.na(longitude)) %>%\n  st_as_sf(coords = c('longitude', 'latitude')) %>%\n  st_set_crs(4326) %>%\n  st_transform(crs = st_crs(camera)) %>%\n  mutate(crash_X = st_coordinates(.)[,1], \n         crash_Y = st_coordinates(.)[,2]) %>%\n  select(crash_id, crash_date, crash_X,crash_Y)\n\n# Clip to region\ncamera <- camera[austin,]\ncrash_sf <- crash_sf[austin,]\n\n\nThis is a bit of a mini-blog post based on a workflow that I have used based on some of my own work. A common issue in spatial analysis - and especially in criminology - is the need to analyze points that are merged to another point. In criminology we might say that assaults occurring right outside of a bar are within it’s “spatial influence”. Typically what is done is we define a “buffer” around each of the points \\(j\\) (like bars, or gas stations) of interest and merge all of the crime incidents \\(i\\) that are within each of the \\(j\\) points’ buffer area. This is something I’ve done before looking at the effect of CCTV cameras on crime at businesses in Detroit. This is pretty common across a lot of criminology research (e.g. finding all crime that occurs within a 1-block radius of bars and liquor stores).\nWhile I used to use the “buffer” method, I think there is a more efficient way of doing this via Voronai polygons which accomplishes the same goal, and allows for more flexibility in analysis. Let’s illustrate this using some data from the city of Austin. In this example we are going to look at the incidence of car crashes \\(i\\) around traffic cameras \\(j\\). Our goal will be to merge car crashes to the nearest traffic camera within a defined spatial range.\nHere’s the study area - one of the Austin city council districts, showing the traffic cameras in blue, and the location of crashes in red. In the picture below there are 58 cameras and about 1,700 car accidents. For this example we’re restricting our analysis to only accidents that occurred in 2022 and using cameras that were active (TURNED_ON) at the time. We can see that there are a lot of accidents, many of them quite far from a traffic camera. Let’s say we want to define a study area around each traffic camera of about 300 meters - or about 980 feet.\n\n\nCode\nggplot() +\n  geom_sf(data = austin, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = crash_sf, color = '#BB5566', size = .6, alpha = .7) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nLocation of car crahes (red) and traffic cameras (blue)."
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html#spatial-merging-using-voronoi-polygons",
    "href": "posts/austin-vehicle/spatial_merging.html#spatial-merging-using-voronoi-polygons",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "Spatial Merging using Voronoi Polygons",
    "text": "Spatial Merging using Voronoi Polygons\n\nVoronoi Polygons\nVoronoi polygons(or tessellations) are useful for a number of purposes. Given a set of points \\(j_n\\) we define a set of \\(n\\) regions where all spaces within each region has a single nearest neighbor of the initial point \\(i\\). Practically speaking, this just means we sub-divide a study area into smaller areas corresponding to the proximity to a point. This has many useful properties, such as determining nearest-neighbor distances from points to points. Let’s see how we can do this in R.\nTo start, we’ll first use a helper function to convert the Voronoi tessellation to an sf object that is suitable for merging. We’ll then merge the camera data to the polygon we just created (using st_intersection) and pull a few of the variables we’ll want for this example.\n\n\nCode\n# Helper function to simplify tessellation\n# borrowed from: \n# https://gis.stackexchange.com/questions/362134\nst_voronoi_point <- function(points){\n  ## points must be POINT geometry\n  # check for point geometry and execute if true\n  if(!all(st_geometry_type(points) == \"POINT\")){\n    stop(\"Input not  POINT geometries\")\n  }\n  g = st_combine(st_geometry(points)) # make multipoint\n  v = st_voronoi(g)\n  v = st_collection_extract(v)\n  return(v[unlist(st_intersects(points, v))])\n}\n\n\n# create Voronoi tessellation over cameras\ncamera_poly <- st_voronoi_point(camera) %>%\n  st_intersection(austin) %>%\n  st_as_sf() %>%\n  mutate(camera_id = camera$camera_id,\n         camera_X = camera$camera_X,\n         camera_Y = camera$camera_Y)\n\n\nNow we can plot the result. Below we see we now have a defined set of regions corresponding to the areas nearest to each camera. Therefore, any crashes that occur in one of the Voronoi polygons is also its nearest camera. This saves us the step of determining which point is its nearest neighbor.\n\n\nCode\nggplot() +\n  geom_sf(data = camera_poly, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nAll spaces within each Voronoi polygon are a nearest neighbor to a camera.\n\n\n\n\n\n\nSpatial Merging\nAfter we’ve created the Voronoi regions, all we need to do is merge each point to the region it falls within (which implies the camera there is its nearest neighbor) and then compute the euclidean distance from the crash to the camera. The code below uses a for-loop to get the pairwise distances after spatial joining and then limits the output to only crashes that are within 300 feet of the nearest camera.\n\n\nCode\n# JOIN AND MERGE\n# ----------------------- #\n\n# compute euclidean distance\nedist <- function(a,b){\n  sqrt(sum((a - b) ^ 2))\n}\n\n# get x-y coords for crashes and cameras\n# convert to matrix\ncamera_crash <-  st_join(crash_sf,camera_poly) %>%\n  tibble() %>%\n  select(camera_id, \n         crash_id, \n         camera_X, \n         camera_Y, \n         crash_X, \n         crash_Y)\n\ndmat <- matrix(c(camera_crash$camera_X, \n                 camera_crash$camera_Y, \n                 camera_crash$crash_X, \n                 camera_crash$crash_Y),\n               ncol = 4)\n\n# compute pairwise distances\ndlist <- list()\nfor(i in 1:nrow(dmat)){\n  dlist[[i]] <- edist(c(dmat[i,1], dmat[i,2]), c(dmat[i,3], dmat[i,4]))\n}\n\ncamera_crash$dist <- unlist(dlist)\n\n# get ids of within 300 meters\ndist_ids <- camera_crash$dist <= 300\n\n\nNow we can plot the results. As we see below we now only have crashes that are within 300 feet or less of the nearest camera. One advantage of this approach is that we can make any adjustments to the spatial region we’re interested in by just adjusting the filter above - or we can use the full range of distances in our analysis and look at decay effects (for example, the effect of CCTV cameras on crime clearance).\n\n\nCode\nggplot() +\n  geom_sf(data = austin, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = crash_sf[dist_ids,], color = '#BB5566', size = .6, alpha = .7) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nCar crashes within 300 meters of a traffic camera.\n\n\n\n\nWith this done, we can do any kind of further investigation. For example, which camera observed the greatest number of crashes? Here, the top-ranked camera is at a 4-way intersection leading to the highway. Also, due to its proximity to the highway, it’s very likely that our distance size (300 meters, or about 900 feet) is picking up accidents that are occurring on the highway below. Of course, this is just a demonstration of method of spatial merging, not an investigation into traffic accidents in Austin!\n\n\nCode\ncamera_crash %>%\n  filter(crash_id %in% crash_sf[dist_ids,]$crash_id) %>%\n  count(camera_id) %>%\n  arrange(desc(n)) %>%\n  slice(1:5)\n\n\n# A tibble: 5 × 2\n  camera_id     n\n  <chr>     <int>\n1 919          72\n2 674          59\n3 948          46\n4 155          44\n5 962          35"
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html#summary",
    "href": "posts/austin-vehicle/spatial_merging.html#summary",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "Summary",
    "text": "Summary\nThis little mini-blog highlighted some approaches that can be taken to perform a relatively common spatial procedure. Using Voronoi polygons we looked at how we can use them to easily calculate nearest-neighbor distances. These types of spatial approaches aren’t necessarily the sexiest topics, but I find they often help considerably with modelling pipelines down the road. Sometimes have a good foundation can help with further analysis later.\n\nAn Aside: An even (easier) method?\nOf course, another method is to simply use the a convenient function embedded in the sf library aptly named st_nearest_feature(). This takes two sf objects and returns the indexes of \\(y\\) that are nearest to \\(x\\). While the solution here is equivalent to the one above, it might not necessarily be available in your given software package. Also, while I have no testing to support this, I expect that this would likely be slow in case of many pairwise distances. The presence of the polygons helps avoid the unnecessary computation of distances between points that are not nearest neighbors.\n\n# get index of cameras nearest to each point\nidx <- st_nearest_feature(crash_sf, camera)\nid_dist <- st_distance(camera[idx,], crash_sf, by_element = TRUE)\n\nid_dist[1:5]\n\nUnits: [m]\n         1          2          3          4          5 \n  49.88593 1202.17589  784.61324 1412.67832  282.73844"
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#injuries-and-low-base-counts",
    "href": "posts/hlm-osha/osha_shrinkage.html#injuries-and-low-base-counts",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Injuries and Low Base Counts",
    "text": "Injuries and Low Base Counts\nMy friend, Andy Wheeler, just recently posted on his blog about reported injuries at Amazon warehouses. As he rightly points out, the apparent high number of injuries at these warehouses is primarily a function of the size of the locations. In criminology we often deal with similar issues (namely, why we use crime rates rather than raw counts when comparing geographies of different populations). While I don’t have much to add to Andy’s post, one thing did stand out to me - the issue of low base counts.\n\nBut note that I don’t think Bonded Logistics is a terribly dangerous place. One thing you need to watch out for when evaluating rate data is that places with smaller denominators (here lower total hours worked) tend to be more volatile.(“Injury Rates at Amazon Warehouses” 2022)\n\nThis is also a very common problem across many different disciplines. Andrew Gelman discusses the problem in this paper about issues arising from mapping county-level cancer rates. Similarly, he points out that very variable rates arise from very low sample sizes. For example: imagine a single murder occurs in the city of Union, CT. With a population of 854, that gives us a murder rate per 1,000 of \\(\\frac{1}{854} * 1,000 = 1.17\\). This would potentially make it one of the highest-rate small towns in the state! Logically this doesn’t make sense, because rare events can happen - but it doesn’t imply a single region is especially unusual.\n\n\n\nCounties with low population appear to have very high rates of kidney cancer. However, much of this is an illusion due to higher variance relative to higher population counties."
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#hierarchical-models",
    "href": "posts/hlm-osha/osha_shrinkage.html#hierarchical-models",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\nAll of this discussion made me think about some issues I had addressed when studying crime - namely rare events (homicides or shootings) that are aggregated to small areas (census blocks or street segments). In these previous examples I had applied hierarchical models to help adjust for these issues we commonly observe with rare events. Let’s work with the same data that Andy used in his example. First, we’ll load the OSHA data for 2021 and isolate just the warehouses in North Carolina.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(knitr)\n\n# load osha data\nosha <- read_csv(unzip(\"C:/Users/gioc4/Dropbox/blogs/hlm_osha/ITA-data-cy2021.zip\"))\n\n# isolate NC injuries at warehouses\ninj_wh <- osha %>%\n  filter(naics_code == '493110',\n         state == 'NC') %>%\n  mutate(inj_rate = (total_injuries/total_hours_worked)*2080)\n\n\nIf we plot the distribution of injury rates per-person work hour year we see that the majority of warehouses are quite low, and very few exceed 0.2. However on the far right we see a single extreme example - the outlier that is the Bonded Logistics warehouse.\n\n\nShow code\nggplot(inj_wh) +\n  geom_histogram(aes(x = inj_rate), \n                 fill = \"#004488\", \n                 color = \"white\",\n                 bins = 20,\n                 linewidth =1.5) +\n  labs(x = \"(total_injuries/total_hours_worked)*2080\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nSorting by the top 10 we see that Bonded Logistics has an injury rate nearly 4 times the next highest warehouse. But they also have only a single employee who worked 1,686 hours that year! Is this really a fair comparison? Following what we already know, almost certainly not.\n\n\nShow code\ninj_wh %>%\n  select(company_name, inj_rate, annual_average_employees, total_hours_worked) %>%\n  arrange(desc(inj_rate)) %>%\n  slice(1:10) %>%\n  kable(caption = \"Top 10 Warehouses, by Injury Rate\", \n        digits = 2,\n        col.names = c(\"Company\",\"Injury Rate\",\"Employees\",\"Total Hours Worked\"))\n\n\n\nTop 10 Warehouses, by Injury Rate\n\n\n\n\n\n\n\n\nCompany\nInjury Rate\nEmployees\nTotal Hours Worked\n\n\n\n\nBonded Logistics\n1.23\n1\n1686\n\n\nTechnimark\n0.34\n4\n6154\n\n\nBonded Logistics\n0.30\n4\n6913\n\n\nCAPSTONE LOGISTICS, LLC\n0.29\n36\n43137\n\n\nRH US LLC\n0.27\n7\n7703\n\n\nAldi (N.C.) L.L.C.\n0.22\n385\n569274\n\n\nBrame Specialty Company Inc\n0.17\n6\n12513\n\n\nConn Appliances Inc\n0.16\n15\n26634\n\n\nBlue Line Distribution\n0.16\n31\n53287\n\n\nTosca Services, LLC\n0.16\n41\n66891\n\n\n\n\n\nTo address this issue, we’ll fit a (very) simple Bayesian hierarchical linear model where we give each warehouse its own intercept. We then partially pool estimates from the model toward the group-level means. In short, we’ll model this as the number of injuries \\(y\\) at each warehouse \\(j\\) as a Poisson process, where each warehouse is modeled with its own (varying) intercept. In a minute we will see the advantage of this.\n\\[y_{j} \\sim Poisson(\\lambda_{j})\\] \\[ln(\\lambda{j}) = \\beta_{0j}\\]\nUsing brms we’ll fit a Poisson regression estimating the total number of injuries at any warehouse weighted by the logged number of hours worked. Because the model is extremely simple, we’ll just keep the default priors with this model which are student_t(3,0,3).\n\n\nCode\n# fit the hierarchical model w/ default priors\n\nfit <- brm(total_injuries ~ 1 + (1|id) + \n             offset(log(total_hours_worked)), \n           family = poisson(), \n           data = inj_wh,\n           file = \"C:/Users/gioc4/Dropbox/blogs/hlm_osha/brmfit\",\n           chains = 4, cores = 4, iter = 2000)\n\n\nAfter the model fits, it’s generally a good idea to make sure the predictions from the model correspond with the observed distribution of the data. Our posterior predictive checks show that we have fairly well captured the observed process, with our posterior simulations \\(\\hat{y}\\) largely in line with the observed \\(y\\).\n\n\nShow code\npp_check(fit, \"hist\") + theme_bw()\n\npp_check(fit, \"scatter_avg\") + theme_bw()"
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#shrinkage",
    "href": "posts/hlm-osha/osha_shrinkage.html#shrinkage",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Shrinkage!",
    "text": "Shrinkage!\nHere’s where things get interesting. One of the benefits of a hierarchical model is that estimates from the model are partially pooled (shrunk) toward the group-level means. In a typical no-pooling model, estimates from very sparse clusters can be extreme or even undefined. In our hierarchical example we are applying regularization to the estimates by trading higher bias for lower variance(Gelman et al. 1995). In a Bayesian framework our application of a prior distribution helps set a reasonable boundary for our model estimates.\nTo illustrate this, we can see the difference between the predicted (blue circles) and observed (empty circles) below. For warehouses with very few worked hours we see that the estimates are pulled strongly toward the global mean. For warehouses with more hours, however, there is considerably less shrinkage.\n\n\nShow code\n# predicted vs actual\ninj_wh_pred <- inj_wh %>%\n  select(id, company_name, inj_rate, annual_average_employees, total_hours_worked) %>%\n  mutate(yhat = predict(fit, type = 'response')[,1],\n         inj_rate_pred = (yhat/total_hours_worked) * 2080)\n\n# Plot all values\nggplot(inj_wh_pred, aes(x = total_hours_worked)) +\n  geom_hline(yintercept = mean(inj_wh_pred$inj_rate), linewidth = 1, color = \"#004488\", alpha = .7)+\n  geom_point(aes(y = inj_rate), shape = 21, size = 2)+\n  geom_point(aes(y = inj_rate_pred), shape = 21, fill = \"#004488\", size = 2, alpha = .5) +\n  labs(x = \"Total Hours Worked\", y = \"Total Injuries/Hours Worked\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nIf we constrain ourselves to the left-hand side of the plot we can view this even more clearly. The estimated value for the unusual Bonded Warehouse is 0.1 compared to the observed value of 1.23. While this estimate is farther off from the observed value, it is probably much more reasonable based on the observed values of other warehouses.\n\n\nShow code\ninj_wh_pred %>%\n  filter(total_hours_worked < 1e5) %>%\n  ggplot(aes(x = total_hours_worked)) +\n  geom_hline(yintercept = mean(inj_wh_pred$inj_rate), linewidth = 1, color = \"#004488\", alpha = .7)+\n  geom_point(aes(y = inj_rate), shape = 21, size = 2)+\n  geom_point(aes(y = inj_rate_pred), shape = 21, fill = \"#004488\", size = 2, alpha = .5) +\n  labs(x = \"Total Hours Worked\", y = \"Total Injuries/Hours Worked\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nIf we compare the predicted injury rates to the observed ones, we can see the differences in shrinkage as well. Larger warehouses have estimates quite close to the observed counts (like The Aldi warehouse which has a relatively high rate of injuries for its size).\n\n\nCode\ninj_wh_pred %>%\n  select(company_name, inj_rate, inj_rate_pred, annual_average_employees, total_hours_worked) %>%\n  arrange(desc(inj_rate)) %>%\n  slice(1:10) %>%\n  kable(caption = \"Top 10 Warehouses, by Injury Rate\", \n        digits = 2,\n        col.names = c(\"Company\",\"Injury Rate\",\"(Pred) Injury Rate\", \"Employees\",\"Total Hours Worked\"))\n\n\n\nTop 10 Warehouses, by Injury Rate\n\n\n\n\n\n\n\n\n\nCompany\nInjury Rate\n(Pred) Injury Rate\nEmployees\nTotal Hours Worked\n\n\n\n\nBonded Logistics\n1.23\n0.10\n1\n1686\n\n\nTechnimark\n0.34\n0.08\n4\n6154\n\n\nBonded Logistics\n0.30\n0.08\n4\n6913\n\n\nCAPSTONE LOGISTICS, LLC\n0.29\n0.19\n36\n43137\n\n\nRH US LLC\n0.27\n0.07\n7\n7703\n\n\nAldi (N.C.) L.L.C.\n0.22\n0.22\n385\n569274\n\n\nBrame Specialty Company Inc\n0.17\n0.07\n6\n12513\n\n\nConn Appliances Inc\n0.16\n0.08\n15\n26634\n\n\nBlue Line Distribution\n0.16\n0.11\n31\n53287\n\n\nTosca Services, LLC\n0.16\n0.11\n41\n66891"
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html",
    "title": "The Power of Ensembles",
    "section": "",
    "text": "It’s no secret that ensemble methods are extremely powerful tools in statistical inference, data science, and machine learning. It’s long been known that many “imperfect” models combined together can often perform better than any single model. For example, in the M5 forecasting competition almost all of the top performers used some element of model averaging or ensembling. Indeed, the very foundations of some of the most commonly used tools in machine learning, like random forests and boosting, work by averging across many highly biased models to create a single more powerful model. The success of this method is relies on the fact that averaging across many high-variance models generally results in a single, lower-variance model. This is most evident in the idea of the “wisdom of the crowd”, where large groups of individuals are often better at predicting something compared to a single expert. However, one area which hasn’t received much attention is outlier detection. When we say “outliers” we’re generally referring to observations that are exceptionally unusual compared to the rest of the sample. A rather consise definition by Hawkins (1980) states:\nThis rather broad definition fits well with the general application of outlier detection. It can be used for identifying fraud in insurance or healthcare datasets, intrusion detection for computer networks, or flagging anomalies in time-series data - among many others. The specific challenge I want to address in this mini-blog is an ensembling approach for unsupervised outlier detection. This is doubly interesting because unsupervised learning presents many more issues compared to supervised learning. Below, I’ll contrast some of these differences and then describe an interesting ensemble approach."
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#outlier-detection-with-supervision",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#outlier-detection-with-supervision",
    "title": "The Power of Ensembles",
    "section": "Outlier Detection with Supervision",
    "text": "Outlier Detection with Supervision\nTo start, let’s look at an example using the cardio dataset sourced from the pyod benchmark set. In this case we have 1831 observations with 21 variables, of which about 9.6% of them are considered anomalous. These are conviently labeled for us, where a value of 1 indicates an anomalous reading. If we fit a simple random forest classifier we see that it is trivial to get a very high AUC on the test data (let’s also not get ahead of ourselves here, as this is a toy dataset with a target that is quite easy to predict). Below we see an example of the fairly strong separation between the inliers and outliers. Our random forest works well in this case - giving us a test AUC of .99 and an average precision of .98. While this is an overly simple example, it does expose how easy some models can be (in many cases) when there is a definite target variable.\n\n\nCode\n# fit a random forest classifier\nrf = RandomForestClassifier()\nrf.fit(Xtrain, ytrain)\n\n# extract the predictions, calculate AUC\nrf_preds = rf.predict_proba(Xtest)[:,1]\n\neval_preds(ytest, rf_preds)\n\n\nRoc:0.999\nPrn:0.99\n\n\n\n\nCode\nsns.scatterplot(x = X[:,7], y = X[:,18], hue = y)\n(\n    plt.xlabel(\"Feature 7\"),\n    plt.ylabel(\"Feature 18\"),\n    plt.title(\"Cardio Scatterplot, Inliers and Outliers\")\n)\nplt.show()\n\n\n\n\n\nScatterplot of inliers (0) and outliers (1), displaying strong separation"
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#unsupervised-outlier-detection",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#unsupervised-outlier-detection",
    "title": "The Power of Ensembles",
    "section": "Unsupervised Outlier Detection",
    "text": "Unsupervised Outlier Detection\nLet’s talk about unsupervised outlier detection. Unlike the situation above in an unsupervised setting we don’t have the convenience of a set of labels identifying whether a given observation is anomalous or not. And because we’re lacking this ground truth, it makes things a lot more complicated for choosing both our model(s) and the parameters for those model(s). Let’s talk about why.\n\nHow do we select a best model?\nIn classic supervised learning we can choose a metric to optimize (say, root-mean squared error or log-loss), then fit a model which attempts to minimize that metric. In the simplest case, think about ordinary least squares. In that case we have a simple target of minimizing the sum of squared errors. We can validate the fit of the model by looking at evalution metrics (RMSE, R-Squared, standardized residuals).However, when we lack a way to identify if a given observation is anomalous or not we don’t have any meaningful way to know if one model is doing better than another.\nYou might also be thinking about optimizing a specific parameter in a model (like the number of nearest neighbors \\(K\\) in a K-nearest neighbors model) using some criterion that doesn’t rely on a target variable (like the ‘elbow’ method). However, optimizing this parameter doesn’t guarantee that the model itself is useful. Simply put, we’re often left groping around in the dark trying to decide what the optimal model or set of parameters is.\nLet’s consider this example: Say we’re looking at the same cardio dataset from above and trying to decide what unsupervised outlier detector we want to use. Maybe we’re deciding between a distance-based one like exact K-nearest neighbors (KNN) or a density-based one like local outlier factor (LOF). Let’s also say we’re agnostic to parameter choices, so we stick with the default ones provided by pyod.\n\n# initalize and fit using default params\nknn = KNN()\nlof = LOF()\n\nknn.fit(X)\nlof.fit(X)\n\n# extract the predictions, calculate AUC\nknn_pred = knn.decision_function(X)\n\neval_preds(y, knn_pred)\n\nRoc:0.686\nPrn:0.286\n\n\n\n\nCode\n# extract the predictions, calculate AUC\nlof_pred = lof.decision_function(X)\n\neval_preds(y, lof_pred)\n\n\nRoc:0.546\nPrn:0.154\n\n\nHere we see that the KNN model performs better than the LOF model - however we didn’t adjust any of the \\(K\\) parameters for either model. Because, in practice, we can’t see this, we don’t know a-priori which model or set of parameters will work best in a given case. This is in stark contrast to our first attempt when we could simply focus on decreasing out-of-sample bias."
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#an-unsupervised-ensambling-approach",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#an-unsupervised-ensambling-approach",
    "title": "The Power of Ensembles",
    "section": "An Unsupervised Ensambling Approach",
    "text": "An Unsupervised Ensambling Approach\nSo, because we can’t easily decrease bias (due to the lack of ground truth values) what are our options? Well, as we saw above, there is a large source of variance implicit in these models. This variance can come from multiple sources:\n\nChoice of model. There is considerable variance in how different models perform under different kinds of anomolies. For example, absent some evaluation metric how do you meaningfully choose between KNN, LOF, or one of many other methods. The pyod benchmark page shows that many models perform quite differently under different types of dimensionality and outlier proportion.\nChoice of parameters. Almost all anomaly detection models have added uncertaintly based on the choice of parameters. For example, in K-nearest neighbors we need to specify the parameter \\(K\\). One-class support vector machines (SVM) are notoriously difficult to tune in part due to the number of parameters to choose (choice of kernel, polynomial degree, ect…).\n\nTherefore, in an unsupervised model our best option is to try to reduce the variance implicit in both the sources above. Rather than staking our whole model on a single choice of model, or a single set of parameters, we can ensemble over a wide number of choices to avoid the risk of choosing a catastrophically bad combination. Since we don’t have access to ground truth, this ends up being the safest option (and as we will see, generally produces good results).\n\nALSO: A regression-based approach\nFor this specific post I’m going to focus on an unsupervised ensemble algothrim proposed by Paulheim & Meusel (2015) and further discussed in Aggarwal & Sathe (2017). The authors dub this method “attribute-wise learning for scoring outliers” or ALSO. The approach we are going to use extends the logic of the ALSO model to an ensemble-based approach (hence ALSO-E).\nLet’s talk a bit about the logic here. The general idea of the algothrim is that we iteratively choose a target feature \\(X_j\\) from the full set of features \\(X\\). The chosen \\(j\\) value is used as the target and the remaining \\(X - j\\) features are used to predict \\(j\\). We repeat this for all features in \\(X\\), collecting the standardized model residuals at each step. We then average the residuals across all the models and use them to identify “anomalous” observations. In this case, more anomalous observations will likely be ones whose residuals are substantially larger than the rest of the sample. The beauty of this method is that for the modelling portion we can choose any base model for prediction (e.g. linear regression, random forests, etc…).\nTo avoid models that are very overfit, or have virtually no predictive ability, we define weights for each model using cross-validated RMSE. The goal here is to downweight models that have low predictive ability so they have a proportionally lower effect on the final outlier score. This is defined as\n\\[w_k = 1 - min(1, RMSE(M_k))\\]\nwhich simply means that models that perform worse than predicting the mean (which would give us an RMSE of 1) are weighted to 0, while a theoretically “perfect” model would be weighted 1. This gives us the added benefit of downweighting features that have little or no predictive value, which helps in cases when we might have one or more irrelevant variables.\n\n\nAdding an E to ALSO\nThe base algothrim above fits \\(X\\) models using all \\(n\\) observations in the data. However, we can extend this model to an ensambling method by applying some useful statistical tools - namely variable subsambling. The idea proposed by Aggarwal & Sathe (2017) is to define a number of iterations (say, 100), and for each iteration randomly subsamble the data from between \\(min(1, \\frac{50}{n})\\) and \\(min(1, \\frac{1000}{n})\\). This means that each model is fit on a minimum of 50 observations and up to 1000 (or, \\(n\\) if \\(n\\) is less than 1000). In addition, we randomly choose a feature \\(j\\) to be predicted. Combined, this ensambling approach makes a more efficient use of the available data and results in a more diverse set of models.\nTo my knowledge, there is no “official” implementation of the ALSO-E algothrim, and it is not present in any large libraries (e.g. pyod or sklearn). However the method is generic enough that it is not difficult to code from scrach. Using the notes available I implemented the method myself using a random forest regressor as the base detector. The code below defines a class with a fit() and predict() function. The fit() function handles all the subsampling and fits each sample on a very shallow random forest regressor. The predict() function does the work of getting the residuals and re-weighting them according to their CV-error. While my implementation is certainly not “feature complete”, it’s good enough to try out:\n\n\nCode\n# code to fit an ALSOe anomaly detector\n\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nclass ALSOe():\n    \"\"\" Initializes a regression-based outlier detector\n    \"\"\"\n\n    def __init__(self, N = 100) -> None:\n\n        self.param_list = []\n        self.model_list = []\n        self.anom_list = []\n        self.wt = []\n    \n        self.N = N\n        self.std_scaler = StandardScaler()\n\n    def fit(self, data):\n        \"\"\" Fit an ensemble detector\n        \"\"\"\n\n        # standardize data\n        self.std_scaler = self.std_scaler.fit(X = data)\n        data = self.std_scaler.transform(data)\n\n        # fit N models\n        for i in range(0, self.N):\n\n            # define sample space\n            n = data.shape[0]\n            p = data.shape[1]\n            s = [min([n, 50]), min(n,1000)]\n\n            # draw s random samples from dataframe X\n            s1 = np.random.randint(low = s[0], high = s[1])\n            p1 = np.random.randint(low = 0, high = p)\n            ind = np.random.choice(n, size = s1, replace = False)\n\n            # define random y and X \n            df = data[ind]\n            y = df[:,p1]\n            X = np.delete(df, p1, axis=1)\n\n            # initalize RF regressor\n            rf = RandomForestRegressor(n_estimators=10)\n\n            # fit & predict\n            rf.fit(X, y)\n\n            # add fitted models & y param to list\n            self.model_list.append(rf)\n            self.param_list.append(p1)\n\n    def predict(self, newdata):\n\n        \"\"\" Get anomaly scores from fitted models\n        \"\"\"\n\n        # standardize data\n        newdata = self.std_scaler.transform(newdata)    \n\n        for i,j in zip(self.model_list, self.param_list):\n\n            # define X, y\n            y = newdata[:,j]\n            X = np.delete(newdata, j, axis=1)\n\n            # get predictions on model i, dropping feature j\n            yhat = i.predict(X)\n\n            # rmse\n            resid = np.sqrt(np.square(y - yhat))\n            resid = (resid - np.mean(resid)) / np.std(resid) \n\n            # compute and apply weights\n            cve = cross_val_score(i, X, y, cv=3, scoring='neg_root_mean_squared_error')\n            w = 1 - min(1, np.mean(cve)*-1)\n\n            resid = resid*w\n\n            # add weights and preds to lists\n            self.wt.append(w)\n            self.anom_list.append(resid)\n\n        # export results as min-max scaled\n        anom_score = np.array(self.anom_list).T\n        anom_score = np.mean(anom_score, axis = 1)\n\n        # rescale and export\n        anom_score = StandardScaler().fit_transform(anom_score.reshape(-1,1))\n        anom_score = anom_score.flatten()\n\n        return anom_score\n\n\n\nFitting the model\nWith all this in mind, fitting the actual model is quite simple. As stated above, the ALSO-E approach is largely parameter free, which means there isn’t much for the user to worry about. Here we’ll just initialize a model with 100 iterations, fit all of the random forest regressors, then get the weighted standardized residuals. This whole process can be condensed into basically 3 lines of code:\n\n# Fit an ALSOe regression ensemble\n# using 100 random forests\nad = ALSOe(N = 100)\nad.fit(X)\n\n# extract predictions from the models and combine\n# the standardized outlier scores\nad_preds = ad.predict(X)\n\n\n\nEvaluating the predictions\nNow that we have the predictions, we can look at the distribution of outlier scores. Below we see a histogram of the ensembled scores which, to recall, are rescaled to mean 0 and standard deviation 1. Therefore, the most anomalous observations will have large positive values. Consistent with what we would expect to see, there is a long tail of large residuals which correspond to the outliers, while the bulk of the data corresponds to a mostly “normal” set of values centered around zero.\n\n\nCode\nsns.histplot(x = ad_preds)\n(\n    plt.xlabel(\"Anomaly Score\"),\n    plt.ylabel(\"Observations\"),\n    plt.title(\"ALSO-E Anomaly Scores\")\n)\nplt.show()\n\n\n\n\n\nHistogram of anomaly scores. The characteristic long tail highlights potential anomalous observations\n\n\n\n\nWe can evaluate the performance of our method by bootstrapping the original dataset 10 times, then running our model on each of the boostrap replicates. This is because there is bound to be some degree of randomness based on the chosen samples and variables for each iteration. Averaging over the bootstrap replicates helps give us some idea of how this model might perform “in the wild” so to speak. Below I define a little helper function to resample the dataset, fit the model, and then extract the relevant evaluation metrics. We then loop through the function and put the fit statistics in a set of lists. For evaluation we’ll look at the averages for each set.\n\n# Bootstrap sample from base dataset and evaluate metrics\ndef boot_eval(df):\n    X, y = resample(df['X'], df['y'])\n\n    ad = ALSOe(N = 100)\n    ad.fit(X)\n    ad_preds = ad.predict(X)\n\n    auc = roc_auc_score(y, ad_preds)\n    pre = average_precision_score(y, ad_preds)\n\n    return [auc, pre]\n\n# run models\nroc_list = []\nprn_list = []\n\nfor i in range(10):\n    roc, prn = boot_eval(data)\n\n    roc_list.append(roc)\n    prn_list.append(prn)\n\nShown below, we see we have a decent AUC and average precision score of about 0.71 and 0.26 respectively. While this is substantially lower than the supervised model, it is still better than the base KNN and LOF models above. The ensambling process also makes it easy because we don’t have to specify any parameters other than the number of iterations to run. In testing, the default 100 works well as a starting point, and there aren’t huge performance gains by increasing it substantially.\n\n\nCode\nprint(f'Avg. ROC: {np.round(np.mean(roc_list),3) }\\nAvg. Prn: {np.round(np.mean(prn_list),3)}')\n\n\nAvg. ROC: 0.715\nAvg. Prn: 0.246\n\n\n\n\n\nComparing performance across datasets\nWe can also evaluate its performance on a variety of other datasets. Here I randomly chose another four datasets from the pyod benchmarks page and compared its performance over 10 bootstrap resamplings to the other benchmarked methods in the pyod ecosystem. Looking at the results we see that we get median RocAUC scores of between .7 to .85 and average precision scores between .2 to .85. For an unsupervised model this isn’t too bad, and largely falls within the range of other detectors.\nWe should note that while its performance is never the best, it is also never the worst either. For example: the .707 we achieved on the cardio dataset is lower than some of the best methods (in this case, PCA and Cluster-Based LOF). However, we avoid extremely bad results like with Angle-Based Outlier Detection or LOF. This underscores our goals with the ensemble model: we prefer a more conservative model that tends to perform consistently across many types of anomalies. We also avoid issues related to choosing optimal parameters but simply ensambling over many detectors. In an unsupervised case this decrease in variance is especially desirable.\n\n\nCode\n# load additional datasets\nd1 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/14_glass.npz\")\nd2 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/18_Ionosphere.npz\")\nd3 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/20_letter.npz\")\nd4 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/21_Lymphography.npz\")\n\ndlist = [d1,d2,d3,d4]\nmname = ['Cardio', 'Glass','Ionosphere','Letter','Lympho']\n\nroc_list_m = []\nprn_list_m = []\n\n# run models\nfor j in dlist:\n \n    for i in range(10):\n        roc, prn = boot_eval(j)\n\n        roc_list_m.append(roc)\n        prn_list_m.append(prn)\n\n\n# Plot evaluation metrics across datasets\nevaldf = pd.DataFrame({'RocAUC' : roc_list +roc_list_m, \n                       'Precision' : prn_list + prn_list_m,\n                       'Dataset': sorted([x for x in mname*10])})\\\n            .melt(id_vars = 'Dataset', var_name = 'Metric', value_name = 'Score')\n\n# facet plot across datasets\ng = sns.FacetGrid(evaldf, col = 'Metric', sharey = False, col_wrap=1, aspect = 2)\n(\n    g.map(sns.boxplot, 'Dataset','Score', order = mname),\n    g.fig.subplots_adjust(top=0.9),\n    g.fig.suptitle('ALSO-E Model Evaluation', fontsize=16)\n)\nplt.show()\n\n\n\n\n\nensemble models often are often not as good as the best method, but can achieve consistently decent performance."
  },
  {
    "objectID": "posts/pca-anomaly/pca_anomaly.html",
    "href": "posts/pca-anomaly/pca_anomaly.html",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "I strongly believe in “learning by doing”. One of the things I have been working on quite a bit lately is unsupervised anomaly detection. As with many machine-learning tools, ensembles are incredibly powerful and useful for a variety of circumstances. Anomaly detection ensembles are no exception to that rule. To better understand how each of the individual pieces of a anomaly detection ensemble works, I’ve decided two build one myself “from scratch”. I put that in giant quotes here because I’ll still rely on some existing frameworks in R to help built the underlying tools.\nMy idea is to create an ensemble of several heterogeneous anomaly detection methods in a way that maximizes their individual benefits. Following some of the guidance proposed by Aggarwal & Sathe I will use:\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\nhosp <- read_csv(\"Hospital_Inpatient_Discharges__SPARCS_De-Identified___2021.csv\")\n\n\nThe data for this post comes from the state of New York’s Hospital Inpatient Discharges (SPARCS) data for 2021. This data contains about 2.1 million records on hospital discharges, including some de-identified patient information, procedure types, and costs. From an anomaly detection standpoint, it might make sense to see if we can identify hospitals with anomalous costs relative to other New York Hospitals. Like the New York Times reported, hospitals make up a very substantial portion of what we spent on healthcare.\nWe’ll create a quick feature set based on a few key payment variables. Here I aggregate over each of the hospitals by calculating their (1) average stay length, (2) average charges, (3) average costs, (4) their average cost-per-stay, (5) the ratio between costs to total charges and (6) the average procedure pay difference.\nThis last feature is a bit more complex, but what I am essentially doing is finding the median cost per-procedure by case severity (assuming more severe cases cost more) and then finding out how much each hospital diverges, on average, from the global cost. It’s a indirect way of measuring how much more or less a given procedure costs at each hospital. This is a easy measure to utilize because a 0 indicates that the hospital bills that procedure at near the global median, a 1 means they bill 100% more than the median and -.5 means they bill 50% less than the median.\n\n\nCode\n# compute and aggregate feature set\ndf <-\n  hosp %>%\n  group_by(`CCSR Procedure Code`, `APR Severity of Illness Code`) %>%\n  mutate(\n    proc_charge = median(`Total Costs`),\n    charge_diff = (`Total Costs` - proc_charge)/proc_charge,\n    `Length of Stay` = as.numeric(ifelse(\n      `Length of Stay` == '120+', 120, `Length of Stay`\n    ))\n  ) %>%\n  group_by(id = `Permanent Facility Id`) %>%\n  summarise(\n    stay_len = mean(`Length of Stay`, na.rm = T),\n    charges = mean(`Total Charges`),\n    costs = mean(`Total Costs`),\n    diff = mean(charge_diff),\n    cost_ratio = mean(`Total Costs`/`Total Charges`),\n    cost_per_stay = costs / stay_len\n  )"
  },
  {
    "objectID": "posts/pca-anomaly/pca_anomaly.html#creating-a-pca-anomaly-detector",
    "href": "posts/pca-anomaly/pca_anomaly.html#creating-a-pca-anomaly-detector",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Creating a PCA Anomaly Detector",
    "text": "Creating a PCA Anomaly Detector\nLet’s now get into the nuts and bolts of actually creating a PCA-based anomaly detector. Now, there are a few ways we can go about this, but I’m going to rely on the approach suggested by Charu Aggarwal in his book Outlier Analysis. What he essentially proposes is a “soft” version of principal components where the eigenvectors are weighted by their variance instead of choosing only the eigenvectors that explain the highest proportion of the overall variance. This “soft” approach has some overlap with the mahalanobis distance. This is the same approach taken by the PCA anomaly detector in the Python pyod package if weighting is specified.\n\nPerforming the “soft” PCA\nTranslating this approach from the formula to code is actually pretty straightforward. Agarwal gives us the following:\n\\[Score(\\bar{X}) = \\sum^d_{j=1} \\frac{|(\\bar{X} - \\bar{\\mu}) *\\bar{e_j}|^2}{\\lambda_j}\\] Which we can code into the following below. Before we add our data to the PCA we scale it to have mean 0 and standard deviation 1 so that the input features are scale-invariant. We then work through the process of extracting the eigenvectors, computing the variance for each, and then performing the “soft” PCA approach.\n\n\n\n\n\n\nNote\n\n\n\nA few notes - strictly speaking the the part \\(\\bar{X} - \\bar{\\mu}\\) isn’t totally necessary in most cases because the eigenvectors are already scaled to have a mean of zero (you can also ensure this by specifying cor=TRUE in the princomp() function). This does help in unusual cases where \\(\\bar{\\mu}\\) is not zero.\n\n\n\n# \"Soft\" PCA\n\n# scale input attributes\nX <- df[, 2:7]\nX <- scale(X)\n\n# pca anomaly detection\n# extract eigenvectors & variance\npca <- princomp(X, cor = TRUE)\ne <- pca$scores\nev <- diag(var(e))\nmu <- apply(e, 2, mean)\nn <- ncol(e)\n\n# compute anomaly scores\nalist <- vector(mode = \"list\", length = n)\nfor(i in 1:n){\n  alist[[i]] <- abs( (e[, i] - mu[i])^2 / ev[i])\n}\n\n# extract values & export\nXscore <- as.matrix(do.call(cbind, alist))\nanom <- apply(Xscore, 1, sum)\n\nThis “soft” PCA is in contrast to so-called “hard” PCA where a specific number of principal components are chosen and the remainder are discarded. The “hard” PCA primarily focuses on reconstruction error along the components with the most variance, while the “soft” approach weights outliers on the lower variance components higher. This is useful because the data vary much less on these components, so outliers are often more obvious along these dimensions.\n\n\nCode\npca$scores %>%\n  data.frame() %>%\n  pivot_longer(cols = starts_with(\"Comp\")) %>%\n  ggplot() +\n  geom_histogram(aes(x = value), bins = 30, fill = '#004488') +\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_minimal() +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\nOutliers are often more visable along components with lower variance.\n\n\n\n\nThe code below implements the PCA anomaly detector. Most of the work involves taking the scores, putting them into a matrix, and then re-weighting them by their variance. The final score is just the row-wise sum across the re-weighted columns. The output is just a vector of anomaly scores anom.\n\n\nCode\nggplot(tibble(anom)) +\n  geom_histogram(aes(x = anom), bins = 25, color = 'white', fill = '#004488')+\n  labs(x = \"Anomaly Score\", y = \"Observations\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nDistribution of outlier scores. More anomalous observations have higher scores\n\n\n\n\n\n\nFlagging Outliers\nFor flagging outliers we can rely on the \\(\\chi^2\\) distribution. This is handy because the \\(\\chi^2\\) distribution is formed as the sum of the squares of \\(d\\) independent standard normal random variables. For the purposes of this example we might just choose 1% as our threshold for anomalies. After flagging we can plot the data along different 2d distributions to see where they lie. For example, the 8 anomalous hospitals generally have both longer stay lengths and charge more for comparable procedures relative to other hospitals.\n\n\nCode\n# compute a p-value for anomalies and\n# append anomaly score and p-values to new dataframe\np = sapply(anom, pchisq, df=6, ncp = mean(anom), lower.tail=F)\n\nscored_data <- data.frame(df, anom, p)\n\nflag <- scored_data$p <= 0.01\n\nggplot() +\n  geom_point(data = scored_data, aes(x = stay_len, y = diff), color = '#004488', size = 2, alpha = .25) +\n  geom_point(data = scored_data[flag,], aes(x = stay_len, y = diff), color = '#BB5566', size = 2.5) +\n  labs(x = \"Stay Length\", y = \"Avg. Payment Difference\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nPlotting inliers (blue) and outliers (red) in 2D space.\n\n\n\n\n\n\nPCA Anaomaly Detector: Example Function\nHere’s a minimal working example of the procedure above. As we build our ensemble, we’ll come back to this function later.\n\n\nCode\n# Run a principal components anomaly detector\nadPCA <- function(X){\n  \n  # pca anomaly detection\n  # extract eigenvectors & variance\n  pca <- princomp(X, cor = TRUE)\n  e <- pca$scores\n  ev <- diag(var(e))\n  mu <- apply(e, 2, mean)\n  n <- ncol(e)\n  \n  # compute anomaly scores\n  alist <- vector(mode = \"list\", length = n)\n  for(i in 1:n){\n    alist[[i]] <- abs( (e[, i] - mu[i])^2 / ev[i])\n  }\n  \n  # extract values & export\n  Xscore <- as.matrix(do.call(cbind, alist))\n  anom <- apply(Xscore, 1, sum)\n  \n  return(anom)\n}"
  },
  {
    "objectID": "about.html#consulting",
    "href": "about.html#consulting",
    "title": "About Me",
    "section": "Consulting",
    "text": "Consulting\nOutside of my day job I also work as a freelance statistical analyst. I work with small to medium organizations to solve data related problems. I have worked (or am currently working) with the following organizations:\n\nAustin Police Department\nCity of Milwaukee (with Michigan State University)\nSwedish National Police\nCommunity Services Incorporated (CSI)\n\nIf you are interested in my work, or would like to learn more, please reach out and schedule a free consultation."
  }
]