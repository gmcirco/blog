[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m Gio Circo, a Ph.D. in criminology and former assistant professor. I currently work at Gainwell technologies as a data scientist. My work primarily focuses on causal inference, Bayesian statistics, and anomaly detection.\nI’m big into food, wine, and spirits - especially cocktails."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nMichigan State University | Lansing, MI | PhD in Criminal Justice | August 2013 - May 2018\nIllinois State University| Normal, IL | B.A in Criminal Justice | August 2010 - June 2012"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nGainwell Technologies | Data Scientist | June 2022 - present\nUniversity of New Haven | Assistant Professor | August 2018 - May 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gio Circo, Ph.D.",
    "section": "",
    "text": "Creating ‘RTM’ style map data\n\n\n\n\nR\n\n\nSpatial Statistics\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nHow good are Rhett and Link?\n\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nPart 3: Histogram-based anomaly detector\n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nPart 2: K-nearest neighbors anomaly detector\n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nApplying a PCA anomaly detector\n\n\n\n\nAnomaly Detection\n\n\nPCA\n\n\nTime Series\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nPart 1: Principal components anomaly detector\n\n\n\n\nAnomaly Detection\n\n\nPCA\n\n\nEnsembles\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nCar Crashes in Austin\n\n\n\n\nSpatial Statistics\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nAdventures in outlier detection\n\n\n\n\nAnomaly Detection\n\n\nEnsembles\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian Statistics\n\n\nHLM\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nGio Circo, Ph.D\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html",
    "href": "posts/austin-vehicle/spatial_merging.html",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\n\n\n# DATA SOURCES AND INFO\n# -----------------------------#\n# https://data.austintexas.gov\n#   vehicle crashes: 'Transportation-and-Mobility/Vision-Zero-Crash-Report-Data-Crash-Level-Records'\n#   traffic cameras: 'Transportation-and-Mobility/Traffic-Cameras/'\n#   austin council map: 'dataset/Boundaries-City-of-Austin-Council-Districts'\n\n# Select a specific Austin Council District and year\n# see: https://maps.austintexas.gov/GIS/CouncilDistrictMap/\ncnl <- 3\nyr <- 2022\n\n\n# DATA LOADING\n# -----------------------------#\n\n# Get Austin shapefile, pull only the district we need\naustin <- st_read(\"C:/Users/gioc4/Documents/blog/data/austin_city.shp\", quiet = TRUE) %>%\n  st_transform(crs = 32614) %>%\n  filter(council_di %in% cnl)\n\n# Read traffic camera data & vehicle crash data\n# Limit crashes to a specific year, conver to spatial\ncamera <- st_read(\"C:/Users/gioc4/Documents/blog/data/traffic_camera.shp\", quiet = TRUE) %>%\n  filter(camera_sta == \"TURNED_ON\") %>%\n  distinct(geometry, .keep_all = TRUE) %>%\n  st_transform(crs = 32614) %>%\n  mutate(camera_X = st_coordinates(.)[,1],\n         camera_Y = st_coordinates(.)[,2])\n\ncrash <- read_csv(unz(\"C:/Users/gioc4/Documents/blog/data/austin_crash.zip\",\"crash_data.csv\")) %>%\n  mutate(crash_date = strptime(crash_date, format=\"%m/%d/%Y %H:%M\")) %>%\n  filter(year(crash_date) == yr)\n\n# Convert crash to sf, extract coordinates\ncrash_sf <- crash %>%\n  filter(!is.na(latitude), !is.na(longitude)) %>%\n  st_as_sf(coords = c('longitude', 'latitude')) %>%\n  st_set_crs(4326) %>%\n  st_transform(crs = st_crs(camera)) %>%\n  mutate(crash_X = st_coordinates(.)[,1], \n         crash_Y = st_coordinates(.)[,2]) %>%\n  select(crash_id, crash_date, crash_X,crash_Y)\n\n# Clip to region\ncamera <- camera[austin,]\ncrash_sf <- crash_sf[austin,]\n\n\nThis is a bit of a mini-blog post based on a workflow that I have used based on some of my own work. A common issue in spatial analysis - and especially in criminology - is the need to analyze points that are merged to another point.\nIn criminology we might say that assaults occurring right outside of a bar are within it’s “spatial influence”. Typically what is done is we define a “buffer” around each of the points \\(j\\) (like bars, or gas stations) of interest and merge all of the crime incidents \\(i\\) that are within each of the \\(j\\) points’ buffer area. This is something I’ve done before looking at the effect of CCTV cameras on crime at businesses in Detroit. This is pretty common across a lot of criminology research (e.g. finding all crime that occurs within a 1-block radius of bars and liquor stores).\nWhile I used to use the “buffer” method, I think there is a more efficient way of doing this via Voronai polygons which accomplishes the same goal, and allows for more flexibility in analysis. Let’s illustrate this using some data from the city of Austin. In this example we are going to look at the incidence of car crashes \\(i\\) around traffic cameras \\(j\\). Our goal will be to merge car crashes to the nearest traffic camera within a defined spatial range.\nHere’s the study area - one of the Austin city council districts, showing the traffic cameras in blue, and the location of crashes in red. In the picture below there are 58 cameras and about 1,700 car accidents. For this example we’re restricting our analysis to only accidents that occurred in 2022 and using cameras that were active (TURNED_ON) at the time. We can see that there are a lot of accidents, many of them quite far from a traffic camera. Let’s say we want to define a study area around each traffic camera of about 300 meters - or about 980 feet.\n\n\nCode\nggplot() +\n  geom_sf(data = austin, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = crash_sf, color = '#BB5566', size = .6, alpha = .7) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nLocation of car crahes (red) and traffic cameras (blue)."
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html#spatial-merging-using-voronoi-polygons",
    "href": "posts/austin-vehicle/spatial_merging.html#spatial-merging-using-voronoi-polygons",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "Spatial Merging using Voronoi Polygons",
    "text": "Spatial Merging using Voronoi Polygons\n\nVoronoi Polygons\nVoronoi polygons(or tessellations) are useful for a number of purposes. Given a set of points \\(j_n\\) we define a set of \\(n\\) regions where all spaces within each region has a single nearest neighbor of the initial point \\(i\\). Practically speaking, this just means we sub-divide a study area into smaller areas corresponding to the proximity to a point. This has many useful properties, such as determining nearest-neighbor distances from points to points. Let’s see how we can do this in R.\nTo start, we’ll first use a helper function to convert the Voronoi tessellation to an sf object that is suitable for merging. We’ll then merge the camera data to the polygon we just created (using st_intersection) and pull a few of the variables we’ll want for this example.\n\n\nCode\n# Helper function to simplify tessellation\n# borrowed from: \n# https://gis.stackexchange.com/questions/362134\nst_voronoi_point <- function(points){\n  ## points must be POINT geometry\n  # check for point geometry and execute if true\n  if(!all(st_geometry_type(points) == \"POINT\")){\n    stop(\"Input not  POINT geometries\")\n  }\n  g = st_combine(st_geometry(points)) # make multipoint\n  v = st_voronoi(g)\n  v = st_collection_extract(v)\n  return(v[unlist(st_intersects(points, v))])\n}\n\n\n# create Voronoi tessellation over cameras\ncamera_poly <- st_voronoi_point(camera) %>%\n  st_intersection(austin) %>%\n  st_as_sf() %>%\n  mutate(camera_id = camera$camera_id,\n         camera_X = camera$camera_X,\n         camera_Y = camera$camera_Y)\n\n\nNow we can plot the result. Below we see we now have a defined set of regions corresponding to the areas nearest to each camera. Therefore, any crashes that occur in one of the Voronoi polygons is also its nearest camera. This saves us the step of determining which point is its nearest neighbor.\n\n\nCode\nggplot() +\n  geom_sf(data = camera_poly, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nAll spaces within each Voronoi polygon are a nearest neighbor to a camera.\n\n\n\n\n\n\nSpatial Merging\nAfter we’ve created the Voronoi regions, all we need to do is merge each point to the region it falls within (which implies the camera there is its nearest neighbor) and then compute the euclidean distance from the crash to the camera. The code below uses a for-loop to get the pairwise distances after spatial joining and then limits the output to only crashes that are within 300 feet of the nearest camera.\n\n\nCode\n# JOIN AND MERGE\n# ----------------------- #\n\n# compute euclidean distance\nedist <- function(a,b){\n  sqrt(sum((a - b) ^ 2))\n}\n\n# get x-y coords for crashes and cameras\n# convert to matrix\ncamera_crash <-  st_join(crash_sf,camera_poly) %>%\n  tibble() %>%\n  select(camera_id, \n         crash_id, \n         camera_X, \n         camera_Y, \n         crash_X, \n         crash_Y)\n\ndmat <- matrix(c(camera_crash$camera_X, \n                 camera_crash$camera_Y, \n                 camera_crash$crash_X, \n                 camera_crash$crash_Y),\n               ncol = 4)\n\n# compute pairwise distances\ndlist <- list()\nfor(i in 1:nrow(dmat)){\n  dlist[[i]] <- edist(c(dmat[i,1], dmat[i,2]), c(dmat[i,3], dmat[i,4]))\n}\n\ncamera_crash$dist <- unlist(dlist)\n\n# get ids of within 300 meters\ndist_ids <- camera_crash$dist <= 300\n\n\nNow we can plot the results. As we see below we now only have crashes that are within 300 feet or less of the nearest camera. One advantage of this approach is that we can make any adjustments to the spatial region we’re interested in by just adjusting the filter above - or we can use the full range of distances in our analysis and look at decay effects (for example, the effect of CCTV cameras on crime clearance).\n\n\nCode\nggplot() +\n  geom_sf(data = austin, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = crash_sf[dist_ids,], color = '#BB5566', size = .6, alpha = .7) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nCar crashes within 300 meters of a traffic camera.\n\n\n\n\nWith this done, we can do any kind of further investigation. For example, which camera observed the greatest number of crashes? Here, the top-ranked camera is at a 4-way intersection leading to the highway. Also, due to its proximity to the highway, it’s very likely that our distance size (300 meters, or about 900 feet) is picking up accidents that are occurring on the highway below. Of course, this is just a demonstration of method of spatial merging, not an investigation into traffic accidents in Austin!\n\n\nCode\ncamera_crash %>%\n  filter(crash_id %in% crash_sf[dist_ids,]$crash_id) %>%\n  count(camera_id) %>%\n  arrange(desc(n)) %>%\n  slice(1:5)\n\n\n# A tibble: 5 × 2\n  camera_id     n\n  <chr>     <int>\n1 919          72\n2 674          59\n3 948          46\n4 155          44\n5 962          35"
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html#summary",
    "href": "posts/austin-vehicle/spatial_merging.html#summary",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "Summary",
    "text": "Summary\nThis little mini-blog highlighted some approaches that can be taken to perform a relatively common spatial procedure. Using Voronoi polygons we looked at how we can use them to easily calculate nearest-neighbor distances. These types of spatial approaches aren’t necessarily the sexiest topics, but I find they often help considerably with modelling pipelines down the road. Sometimes have a good foundation can help with further analysis later.\n\nAn Aside: An even (easier) method?\nOf course, another method is to simply use the a convenient function embedded in the sf library aptly named st_nearest_feature(). This takes two sf objects and returns the indexes of \\(y\\) that are nearest to \\(x\\). While the solution here is equivalent to the one above, it might not necessarily be available in your given software package. Also, while I have no testing to support this, I expect that this would likely be slow in case of many pairwise distances. The presence of the polygons helps avoid the unnecessary computation of distances between points that are not nearest neighbors.\n\n# get index of cameras nearest to each point\nidx <- st_nearest_feature(crash_sf, camera)\nid_dist <- st_distance(camera[idx,], crash_sf, by_element = TRUE)\n\nid_dist[1:5]\n\nUnits: [m]\n         1          2          3          4          5 \n  49.88593 1202.17589  784.61324 1412.67832  282.73844"
  },
  {
    "objectID": "posts/gmm/gmm.html",
    "href": "posts/gmm/gmm.html",
    "title": "Good Mythical Morning: Blind Fast Food Taste Tests",
    "section": "",
    "text": "So, its no secret that my wife and I are fans of Rhett and Link’s YouTube series Good Mythical Morning. Specifically, we are fans of the large variety of food- related content like Will it Burger?, Worst Halloween Candy Taste Test, International Burger King Taste Test, and 100 Years of Party Snacks Taste Test.\nWhile normally this would qualify as pretty generic YouTube content, Good Mythical Morning keeps it interesting by “gamifying” many of these segments. These including making the hosts try gross food mash-ups, throw darts at a map to guess the food’s country of origin, or use a shuffleboard to guess the decade the food originated from.\nOne of the games that is common on the channel is the Blind Taste Test where Rhett and Link are presented with food items from different fast food or restaurant chains, and are tasked with guessing which items come from which location. For example, in the Blind Chicken Nugget Taste Test they are given nuggets from 6 locations (McDonalds, Wendys, Chic-Fil-A, Hardees, KFC, and Frozen Tyson Nuggets) and have to try and place them based on taste alone. It’s silly content, but fun."
  },
  {
    "objectID": "posts/gmm/gmm.html#how-good-are-they",
    "href": "posts/gmm/gmm.html#how-good-are-they",
    "title": "Good Mythical Morning: Blind Fast Food Taste Tests",
    "section": "How Good Are They?",
    "text": "How Good Are They?\nOne nagging question I’ve always had is whether their performance on these segments were any better than if they just randomly guessed each time. Blind taste-testing is actually very hard, and distinguishing between 6 different pieces of mass-produced fried meat is probably even harder. Indeed, their performance on a lot of these segments is actually not great. To answer this, I gathered data from all the blind fast food taste tests that I could find, and collected it into a Google Sheet. Most of the games have 6 options, but a few have 5. For simplicity I will focus on the 26 segments where there were six choices available.\n\nChecking Out the Data\n\n\nCode\nlibrary(googlesheets4)\nlibrary(tidyverse)\n\nset.seed(7132322)\n\n# Load Data\n# ---------------\n\ngsheet <-\n  \"https://docs.google.com/spreadsheets/d/1P_LIZnnnaoCZHwmmrBM3O26sPAGCekVj_3qmWyyxTpc/edit?usp=sharing\"\n\ntaste_test <- read_sheet(ss = gsheet) %>%\n  filter(options == 6)\n\n\nThe sheet here has columns corresponding to each game played food, with Rhett and Link’s number of correct guesses stored in their own columns.\n\n\nCode\nhead(taste_test)\n\n\n# A tibble: 6 × 4\n  food          options rhett  link\n  <chr>           <dbl> <dbl> <dbl>\n1 fries               6     4     3\n2 nuggets             6     6     3\n3 fried chicken       6     0     2\n4 bbq pork            6     2     3\n5 donut               6     0     1\n6 taco                6     2     2\n\n\nFirst, let’s look at the distribution of correct guesses for Rhett and Link. We can generate just a simple bar chart here:\n\n\nCode\n# Plot R v L\n# ---------------\n\n# plot observed correct answers for R&L\npar(mfrow = c(1,2))\nbarplot(prop.table(table(taste_test$rhett)), \n        main = \"Rhett\", \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$rhett),3)),\n        col = '#e6862c')\nbarplot(prop.table(table(taste_test$link)), \n        col = '#4ec726', \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$link),3)),\n        main = \"Link\")\n\n\n\n\n\nStrangely enough, based on the 26 games that I collected, Rhett and Link have the exact same number of correct guesses. We can see, however, that their distributions are quite different (Rhett has a single game with all 6 right, and several with 4 correct, but also many more games with none correct). As expected, across all games we see their average is about 1.9 correct answers. Now, recall, our question here is “is Rhett and Link’s performance better than chance alone?”\n\n\nRandom Guessing With Replacement\nSo if we want to compare their performance to “chance” - that is, totally random guessing - we need a probability distribution. Here, the binomial is logical starting point. Let’s start with the simplest assumption of randomly guessing one of the six options with replacement (meaning you can guess the same option twice). While this doesn’t necessarily seem like an ideal strategy, Rhett and Link often do guess the same location two or three times.\nWe can use the probability density function for the binomial distribution to calculate the probability of guessing between 0 and 6 items correctly, where each trial is assumed to be independent. Below, we see that there is about a 67% probability of guessing at least one of the items correctly, and a 40% chance of getting exactly one correct. Getting more than 4 correct is very rare. If we look at the observed results above, we see this mostly matches up. Both Rhett and Link rarely get more that 3 correct.\n\n\nCode\n# totally random (w replacement)\nround(dbinom(0:6, 6, 1/6), 3)\n\n\n[1] 0.335 0.402 0.201 0.054 0.008 0.001 0.000\n\n\n\n\nRandom Guessing Without Replacement\nNow, another strategy might be randomly guess, but not repeat any previous guesses. Instead of the guesses being independent, they are now dependent on prior guesses. That means we need to use a different method to calculate this. The hypergeometric distribution is commonly used to calculate this (e.g. dhyper()). This also falls within the realm of permutation-based math ala derangements. However, I am a simple man with a good computer and a less-good maths background, so I will simply simulate it.\n\n\nCode\n# totally random (w/o replacement)\n# simulate\nn = 1e5\n\nalist <- vector(mode = \"list\", length = n)\nfor(j in 1:n){\n  truth <- sample(1:6, 6)\n  guess <- sample(1:6, 6)\n  \n  alist[j] <- sum(guess == truth)\n  \n}\n\nres <- sapply(alist, sum)\nprop.table(table(res))\n\n\nres\n      0       1       2       3       4       6 \n0.37032 0.36459 0.18739 0.05538 0.02081 0.00151 \n\n\nHere we see that if we limit our guesses to only options not previously guessed, there is a roughly equal probability of getting between 0 and 1 answers correct, and about a 28% chance of getting more than 1 correct. Note, under this strategy it is not possible to get 5 answers correct (because if you incorrectly order at least one item, by definition at least one other item is also incorrectly ordered)"
  },
  {
    "objectID": "posts/gmm/gmm.html#are-rhett-and-link-better-than-random-guessing",
    "href": "posts/gmm/gmm.html#are-rhett-and-link-better-than-random-guessing",
    "title": "Good Mythical Morning: Blind Fast Food Taste Tests",
    "section": "Are Rhett and Link Better than Random Guessing?",
    "text": "Are Rhett and Link Better than Random Guessing?\nLet’s review our results here. Here’s what random guessing looks like under our two scenarios:\n\n\nCode\n# plot two different 'null' distributions\npar(mfrow = c(1, 2))\nbarplot(prop.table(round(dbinom(0:6, 6, 1 / 6), 3)),\n        names.arg = 0:6, \n        main = \"With Replacement\", \n        col = \"#dbd895\")\n\nbarplot(prop.table(table(res)), \n        main = \"Without Replacement\",\n        col = \"#dbd895\")\n\n\n\n\n\nAnd here’s what Rhett and Link’s actual results look like:\n\n\nCode\n# plot observed correct answers for R&L\npar(mfrow = c(1,2))\nbarplot(prop.table(table(taste_test$rhett)), \n        main = \"Rhett\", \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$rhett),3)),\n        col = '#e6862c')\nbarplot(prop.table(table(taste_test$link)), \n        col = '#4ec726', \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$link),3)),\n        main = \"Link\")\n\n\n\n\n\nThe expected value of random guessing in both scenarios is, approximately 1, meaning that Rhett and Link’s scores of 1.8 mean that, on average, they perform marginally better than if you guessed completely at random. This is on the order of about 1 additional item correct compared to just guessing 100% randomly.\n\n\nCode\n# expected value\nmean(rbinom(1e5, 6, 1/6))\n\n\n[1] 1.00159\n\n\nCode\nmean(res)\n\n\n[1] 0.99781\n\n\nSo, congrats Rhett and Link! You are a little bit better at blind tasting fast food than rolling a six-sided dice!"
  },
  {
    "objectID": "posts/hbos-anomaly/hbos_anomaly.html",
    "href": "posts/hbos-anomaly/hbos_anomaly.html",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "This is the third part of a 3-part series. In the first two posts I described how I built a principal components analysis anomaly detector and a k-nearest neighbors anomaly detector as components for a ensemble model. This third post will discuss the last piece, which is a histogram-based anomaly detector.\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector"
  },
  {
    "objectID": "posts/hbos-anomaly/hbos_anomaly.html#building-a-histogram-based-outlier-detector",
    "href": "posts/hbos-anomaly/hbos_anomaly.html#building-a-histogram-based-outlier-detector",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Building a Histogram-Based Outlier Detector",
    "text": "Building a Histogram-Based Outlier Detector\n\nDefining sub-space density\nThe core of the idea behind a histogram-based outlier detector is that it is a method to efficiently explore subspaces of the data by binning observations into discrete groups, then weighting each bin inversely by the number of observations (more on this in a moment). To start, we can provide a quick example showing how we can use histograms to partition the data into bins. Below, I create two histograms for the features representing stay length and average cost per-stay.\n\n\nCode\n# define breaks\nh1 <- hist(df$stay_len, breaks = 5, plot = FALSE)\nh2 <- hist(df$cost_per_stay, breaks = 5, plot = FALSE)\n\n# append to dataframe\nhdf <- df %>%\n  mutate(d1 = findInterval(stay_len, h1$breaks),\n         d2 = findInterval(cost_per_stay, h2$breaks),\n         space = paste0(d1,\"-\",d2))\n\n# Create a data frame with a grid of values for the predictor variables\ngrid <- expand.grid(stay_len = seq(min(hdf$stay_len), max(hdf$stay_len), length.out = 10),\n                    cost_per_stay = seq(min(hdf$cost_per_stay), max(hdf$cost_per_stay), length.out = 10)) %>%\n  mutate(d1 = findInterval(stay_len, h1$breaks),\n         d2 = findInterval(cost_per_stay, h2$breaks),\n         space = paste0(d1,\"-\",d2)) %>%\n    mutate(space = ifelse(space %in% hdf$space, space, NA)) %>%\n    fill(space)\n\n\nIf we plot each of these histograms, we can observe that most values concentrate in a few bins, while a small number of values are in more sparsely-populated bins. Obviously this shows us that the majority of stay lengths are between 0-10 days, and the average cost per-stay is around $4,000.\n\n\nCode\npar(mfrow=c(1,2))\nplot(h1, main = \"Stay Length\", xlab = \"Days\", col = '#004488', border = 'white')\nplot(h2, main = \"Cost Per Stay\", xlab = \"Cost\", col = '#004488', border = 'white')\n\n\n\n\n\nA histogram’s bins are proportional to the number of observations.\n\n\n\n\nWe can plot this in 2 dimensions to see how the feature space distribution is subdivided based on histogram bins. As we would expect, the majority of observations fall into a few regions, while potential outliers exist in much more sparsely populated bins. This is actually fairly similar to a decision tree, where we classify observations based on a set of rules. For example, the lone observation on the far right of the plot is in a region where stay_length >= 44 and cost_per_stay >= 3367 and cost_per_stay <= 4837.\n\n\nCode\nggplot() +\n  geom_tile(data = grid, aes(x = stay_len, y = cost_per_stay, fill = space), alpha = .6) +\n  geom_point(data = df, aes(x = stay_len, y = cost_per_stay), color = '#004488') +\n  labs(x = 'Stay Length', y = 'Cost Per Stay') +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n2D histogram partitioning of observations. Colored regions represent different bin partitions. Note the sparse regions at the top left and lower right quadrants.\n\n\n\n\n\n\nScoring observations\nWith this in mind, we are essentially going to do the above, but in \\(d\\) dimensions (where \\(d\\) is the number of input features). To give each observation an anomaly score, we will follow a very simple scoring mechanism proposed by the original authors of the method where:\n\\[HBOS(p) = \\sum^d_{i=0}log(\\frac{1}{hist_i(p)})\\]\nWhich states that the histogram-based anomaly score is the sum of the log of inverse histogram densities. More simply, for each feature \\(d\\) we compute a histogram density, and each observation is scored based on the inverse of its bin density (Goldstein and Dengel 2012). This means observations in sparsely populated bins receive higher scores, and vice-versa. One of the trade-offs here is that we have to assume feature independence (which is a tenuous assumption in a lot of cases), but even violations of this might not be too bad.\n\n\nA brief aside: choosing the optimal bin size\nOne challenge with this approach is that before we calculate histogram densities we need to define the number of bins for our histograms ahead of time. Now, one simple method might just be to choose a very rough rule-of-thumb (e.g. the “Sturges” rule of \\(1+log2(N)\\)) or to just choose a constant number like 5 or 10. A more principled way, however, would be to derive the optimal number of bins based on some properties of the input data.\nThere are a lot of proposed options out here, but the one that makes a lot of sense to me (and, incidentally, is also used in the pyod implementation of this function) is to iteratively fit histograms, calculate a penalized maximum likelihood estimate for each histogram \\(D\\), and then select the number of bins corresponding to the maximum likelihood estimate (Birgé and Rozenholc 2006). A rough R implementation of this is shown below:\n\n  # internal function: compute optimal bins\nopt_bins <- function(X, upper_bound = 15)\n  {\n    \n    epsilon = 1\n    n <- length(X)\n    \n    # maximum likelihood estimate for bin\n    maximum_likelihood <- array(0, dim = c(upper_bound - 1, 1))\n    \n    # rule of thumb for upper bound\n    if (is.null(upper_bound)) {\n      upper_bound <- as.integer(sqrt(length(X)))\n    }\n    \n    for (i in seq_along(1:(upper_bound - 1))) {\n      b <- i + 1\n      histogram <- hist(X, breaks = b, plot = FALSE)$counts\n      \n      maximum_likelihood[i] <-\n        sum(histogram * log(b * histogram /  n + epsilon) - (b - 1 + log(b) ^ 2.5))\n    }\n    \n    return(which.max(maximum_likelihood))\n  }\n\nSo, running this for the first feature stay_len, we get:\n\nopt_bins(df$stay_len)\n\n[1] 5\n\n\n\n\nBuilding the detector\nWith the issue of histogram bins out of the way, we can procede with the rest of the model. The last bit is really quite simple. For each feature \\(d\\) we compute the optimal number of bins (using the function we just defined above). We then build a histogram for that feature and identify which points fall within each bin. We then score each point according to the formula above, which is the log of the inverse histogram density (making outlying observations have correspondingly higher anomaly scores). The last thing we do after running this algorithm over all \\(d\\) features is to scale their scores (here, I use min-max normalization) and sum them together. The code to do this is below:\n\n# run HBOS\n  for(d in 1:d){\n    \n    h <- hist(X[,d], breaks = opt_bins(X[,d]), plot = FALSE)\n    fi <- findInterval(X[,d], h$breaks)\n    \n    hbos[[d]] <- log(1/h$density[fi])\n  }\n  \n  # minmax scale feature vectors\n  hbos <- lapply(hbos, function(x){(x- min(x)) /(max(x)-min(x))})\n  \n  # return score\n  return(apply(do.call(cbind, hbos), 1, sum))"
  },
  {
    "objectID": "posts/hbos-anomaly/hbos_anomaly.html#running-the-model",
    "href": "posts/hbos-anomaly/hbos_anomaly.html#running-the-model",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Running the Model",
    "text": "Running the Model\nNow we’re ready to run everything. For simplicity, I wrap all this code into a single adHBOS function that contains the optimal histogram binning and the scoring (see: Section 3.1). For flagging anomalies we will just identify the highest 5% (a rough, but arguably acceptable heuristic).\n\nX <- df[,2:7]\n\ndf$anom <-  adHBOS(X)\ndf$flag <- ifelse(df$anom >= quantile(df$anom, .95),1,0)\n\nIf we look at a histogram of we see most scores are low, while the outliers are clearly visible on the right-hand side.\n\n\nCode\nggplot(df) +\n  geom_histogram(aes(x = anom), bins = 10, color = 'white', fill = '#004488')+\n  labs(x = \"Anomaly Score\", y = \"Observations\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nHistogram-based anomaly scores. More anomalous observations have higher scores\n\n\n\n\nComparing this to our earlier plot we see:\n\n\nCode\nggplot() +\n  geom_tile(data = grid, aes(x = stay_len, y = cost_per_stay, fill = space), alpha = .6) +\n  geom_point(data = df, aes(x = stay_len, y = cost_per_stay), color = '#004488') +\n  geom_point(data = df[df$flag == 1,], aes(x = stay_len, y = cost_per_stay, color = '#BB5566'), size = 2) +\n  labs(x = 'Stay Length', y = 'Cost Per Stay') +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\nPlotting inliers (blue) and outliers (red) in 2D space.\n\n\n\n\nAs we expect, most of the observations that are flagged as outliers reside in bins with few other observations. This is pretty consistent with the other two methods we used before (PCA and KNN anomaly detectors). One specific advantage of the HBOS method is that it is very fast for even large datasets. However, with higher levels of dimensionality it is very likely that the assumption of feature independence is tenuous at best. Other methods, like the isolation forest can often perform better in higher dimensions. However, the simplicity of the method makes it easy to explain, which can be a benefit in many cases!\n\nHBOS Anaomaly Detector: Example Function\nHere’s a minimal working example of the procedure above. As we build our ensemble, we’ll come back to this function later.\n\n\nCode\n# Run a principal components anomaly detector\nadHBOS <- function(X, ub = 15){\n  \n  # scale input features, define list to hold scores\n  X <- scale(X)\n  j <- dim(X)[2]\n  hbos <- vector(\"list\",j)\n  \n  # internal function: compute optimal bins\n  opt_bins <- function(X, upper_bound = ub)\n  {\n    \n    epsilon = 1\n    n <- length(X)\n    \n    # maximum likelihood estimate for bin\n    maximum_likelihood <- array(0, dim = c(upper_bound - 1, 1))\n    \n    # rule of thumb for upper bound\n    if (is.null(upper_bound)) {\n      upper_bound <- as.integer(sqrt(length(X)))\n    }\n    \n    for (i in seq_along(1:(upper_bound - 1))) {\n      b <- i + 1\n      histogram <- hist(X, breaks = b, plot = FALSE)$counts\n      \n      maximum_likelihood[i] <-\n        sum(histogram * log(b * histogram /  n + epsilon) - (b - 1 + log(b) ^ 2.5))\n    }\n    \n    return(which.max(maximum_likelihood))\n  }\n  \n  # run HBOS\n  for(j in 1:j){\n    \n    h <- hist(X[,j], breaks = opt_bins(X[,j]), plot = FALSE)\n    fi <- findInterval(X[,j], h$breaks)\n    \n    hbos[[j]] <- log(1/h$density[fi])\n  }\n  \n  # minmax scale feature vectors\n  hbos <- lapply(hbos, function(x){(x- min(x)) /(max(x)-min(x))})\n  \n  # return score\n  return(apply(do.call(cbind, hbos), 1, sum))\n  \n}"
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#injuries-and-low-base-counts",
    "href": "posts/hlm-osha/osha_shrinkage.html#injuries-and-low-base-counts",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Injuries and Low Base Counts",
    "text": "Injuries and Low Base Counts\nMy friend, Andy Wheeler, just recently posted on his blog about reported injuries at Amazon warehouses. As he rightly points out, the apparent high number of injuries at these warehouses is primarily a function of the size of the locations.\nIn criminology we often deal with similar issues (namely, why we use crime rates rather than raw counts when comparing geographies of different populations). While I don’t have much to add to Andy’s post, one thing did stand out to me - the issue of low base counts.\n\nBut note that I don’t think Bonded Logistics is a terribly dangerous place. One thing you need to watch out for when evaluating rate data is that places with smaller denominators (here lower total hours worked) tend to be more volatile.(“Injury Rates at Amazon Warehouses” 2022)\n\nThis is also a very common problem across many different disciplines. Andrew Gelman discusses the problem in this paper about issues arising from mapping county-level cancer rates. Similarly, he points out that very variable rates arise from very low sample sizes. For example: imagine a single murder occurs in the city of Union, CT. With a population of 854, that gives us a murder rate per 1,000 of \\(\\frac{1}{854} * 1,000 = 1.17\\). This would potentially make it one of the highest-rate small towns in the state! Logically this doesn’t make sense, because rare events can happen - but it doesn’t imply a single region is especially unusual.\n\n\n\nCounties with low population appear to have very high rates of kidney cancer. However, much of this is an illusion due to higher variance relative to higher population counties."
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#hierarchical-models",
    "href": "posts/hlm-osha/osha_shrinkage.html#hierarchical-models",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\nAll of this discussion made me think about some issues I had addressed when studying crime - namely rare events (homicides or shootings) that are aggregated to small areas (census blocks or street segments). In these previous examples I had applied hierarchical models to help adjust for these issues we commonly observe with rare events. Let’s work with the same data that Andy used in his example. First, we’ll load the OSHA data for 2021 and isolate just the warehouses in North Carolina.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(knitr)\n\n# load osha data\nosha <- read_csv(unzip(\"C:/Users/gioc4/Dropbox/blogs/hlm_osha/ITA-data-cy2021.zip\"))\n\n# isolate NC injuries at warehouses\ninj_wh <- osha %>%\n  filter(naics_code == '493110',\n         state == 'NC') %>%\n  mutate(inj_rate = (total_injuries/total_hours_worked)*2080)\n\n\nIf we plot the distribution of injury rates per-person work hour year we see that the majority of warehouses are quite low, and very few exceed 0.2. However on the far right we see a single extreme example - the outlier that is the Bonded Logistics warehouse.\n\n\nShow code\nggplot(inj_wh) +\n  geom_histogram(aes(x = inj_rate), \n                 fill = \"#004488\", \n                 color = \"white\",\n                 bins = 20,\n                 linewidth =1.5) +\n  labs(x = \"(total_injuries/total_hours_worked)*2080\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nSorting by the top 10 we see that Bonded Logistics has an injury rate nearly 4 times the next highest warehouse. But they also have only a single employee who worked 1,686 hours that year! Is this really a fair comparison? Following what we already know, almost certainly not.\n\n\nShow code\ninj_wh %>%\n  select(company_name, inj_rate, annual_average_employees, total_hours_worked) %>%\n  arrange(desc(inj_rate)) %>%\n  slice(1:10) %>%\n  kable(caption = \"Top 10 Warehouses, by Injury Rate\", \n        digits = 2,\n        col.names = c(\"Company\",\"Injury Rate\",\"Employees\",\"Total Hours Worked\"))\n\n\n\nTop 10 Warehouses, by Injury Rate\n\n\n\n\n\n\n\n\nCompany\nInjury Rate\nEmployees\nTotal Hours Worked\n\n\n\n\nBonded Logistics\n1.23\n1\n1686\n\n\nTechnimark\n0.34\n4\n6154\n\n\nBonded Logistics\n0.30\n4\n6913\n\n\nCAPSTONE LOGISTICS, LLC\n0.29\n36\n43137\n\n\nRH US LLC\n0.27\n7\n7703\n\n\nAldi (N.C.) L.L.C.\n0.22\n385\n569274\n\n\nBrame Specialty Company Inc\n0.17\n6\n12513\n\n\nConn Appliances Inc\n0.16\n15\n26634\n\n\nBlue Line Distribution\n0.16\n31\n53287\n\n\nTosca Services, LLC\n0.16\n41\n66891\n\n\n\n\n\nTo address this issue, we’ll fit a (very) simple Bayesian hierarchical linear model where we give each warehouse its own intercept. We then partially pool estimates from the model toward the group-level means. In short, we’ll model this as the number of injuries \\(y\\) at each warehouse \\(j\\) as a Poisson process, where each warehouse is modeled with its own (varying) intercept. In a minute we will see the advantage of this.\n\\[y_{j} \\sim Poisson(\\lambda_{j})\\] \\[ln(\\lambda{j}) = \\beta_{0j}\\]\nUsing brms we’ll fit a Poisson regression estimating the total number of injuries at any warehouse weighted by the logged number of hours worked. Because the model is extremely simple, we’ll just keep the default priors with this model which are student_t(3,0,3).\n\n\nCode\n# fit the hierarchical model w/ default priors\n\nfit <- brm(total_injuries ~ 1 + (1|id) + \n             offset(log(total_hours_worked)), \n           family = poisson(), \n           data = inj_wh,\n           file = \"C:/Users/gioc4/Dropbox/blogs/hlm_osha/brmfit\",\n           chains = 4, cores = 4, iter = 2000)\n\n\nAfter the model fits, it’s generally a good idea to make sure the predictions from the model correspond with the observed distribution of the data. Our posterior predictive checks show that we have fairly well captured the observed process, with our posterior simulations \\(\\hat{y}\\) largely in line with the observed \\(y\\).\n\n\nShow code\npp_check(fit, \"hist\") + theme_bw()\n\npp_check(fit, \"scatter_avg\") + theme_bw()"
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#shrinkage",
    "href": "posts/hlm-osha/osha_shrinkage.html#shrinkage",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Shrinkage!",
    "text": "Shrinkage!\nHere’s where things get interesting. One of the benefits of a hierarchical model is that estimates from the model are partially pooled (shrunk) toward the group-level means. In a typical no-pooling model, estimates from very sparse clusters can be extreme or even undefined. In our hierarchical example we are applying regularization to the estimates by trading higher bias for lower variance(Gelman et al. 1995). In a Bayesian framework our application of a prior distribution helps set a reasonable boundary for our model estimates.\nTo illustrate this, we can see the difference between the predicted (blue circles) and observed (empty circles) below. For warehouses with very few worked hours we see that the estimates are pulled strongly toward the global mean. For warehouses with more hours, however, there is considerably less shrinkage.\n\n\nShow code\n# predicted vs actual\ninj_wh_pred <- inj_wh %>%\n  select(id, company_name, inj_rate, annual_average_employees, total_hours_worked) %>%\n  mutate(yhat = predict(fit, type = 'response')[,1],\n         inj_rate_pred = (yhat/total_hours_worked) * 2080)\n\n# Plot all values\nggplot(inj_wh_pred, aes(x = total_hours_worked)) +\n  geom_hline(yintercept = mean(inj_wh_pred$inj_rate), linewidth = 1, color = \"#004488\", alpha = .7)+\n  geom_point(aes(y = inj_rate), shape = 21, size = 2)+\n  geom_point(aes(y = inj_rate_pred), shape = 21, fill = \"#004488\", size = 2, alpha = .5) +\n  labs(x = \"Total Hours Worked\", y = \"Total Injuries/Hours Worked\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nIf we constrain ourselves to the left-hand side of the plot we can view this even more clearly. The estimated value for the unusual Bonded Warehouse is 0.1 compared to the observed value of 1.23. While this estimate is farther off from the observed value, it is probably much more reasonable based on the observed values of other warehouses.\n\n\nShow code\ninj_wh_pred %>%\n  filter(total_hours_worked < 1e5) %>%\n  ggplot(aes(x = total_hours_worked)) +\n  geom_hline(yintercept = mean(inj_wh_pred$inj_rate), linewidth = 1, color = \"#004488\", alpha = .7)+\n  geom_point(aes(y = inj_rate), shape = 21, size = 2)+\n  geom_point(aes(y = inj_rate_pred), shape = 21, fill = \"#004488\", size = 2, alpha = .5) +\n  labs(x = \"Total Hours Worked\", y = \"Total Injuries/Hours Worked\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nIf we compare the predicted injury rates to the observed ones, we can see the differences in shrinkage as well. Larger warehouses have estimates quite close to the observed counts (like The Aldi warehouse which has a relatively high rate of injuries for its size).\n\n\nCode\ninj_wh_pred %>%\n  select(company_name, inj_rate, inj_rate_pred, annual_average_employees, total_hours_worked) %>%\n  arrange(desc(inj_rate)) %>%\n  slice(1:10) %>%\n  kable(caption = \"Top 10 Warehouses, by Injury Rate\", \n        digits = 2,\n        col.names = c(\"Company\",\"Injury Rate\",\"(Pred) Injury Rate\", \"Employees\",\"Total Hours Worked\"))\n\n\n\nTop 10 Warehouses, by Injury Rate\n\n\n\n\n\n\n\n\n\nCompany\nInjury Rate\n(Pred) Injury Rate\nEmployees\nTotal Hours Worked\n\n\n\n\nBonded Logistics\n1.23\n0.10\n1\n1686\n\n\nTechnimark\n0.34\n0.08\n4\n6154\n\n\nBonded Logistics\n0.30\n0.08\n4\n6913\n\n\nCAPSTONE LOGISTICS, LLC\n0.29\n0.19\n36\n43137\n\n\nRH US LLC\n0.27\n0.07\n7\n7703\n\n\nAldi (N.C.) L.L.C.\n0.22\n0.21\n385\n569274\n\n\nBrame Specialty Company Inc\n0.17\n0.07\n6\n12513\n\n\nConn Appliances Inc\n0.16\n0.08\n15\n26634\n\n\nBlue Line Distribution\n0.16\n0.10\n31\n53287\n\n\nTosca Services, LLC\n0.16\n0.11\n41\n66891"
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html",
    "title": "The Power of Ensembles",
    "section": "",
    "text": "It’s no secret that ensemble methods are extremely powerful tools in statistical inference, data science, and machine learning. It’s long been known that many “imperfect” models combined together can often perform better than any single model.\nFor example, in the M5 forecasting competition almost all of the top performers used some element of model averaging or ensembling. Indeed, the very foundations of some of the most commonly used tools in machine learning, like random forests and boosting, work by averging across many highly biased models to create a single more powerful model. The success of this method is relies on the fact that averaging across many high-variance models generally results in a single, lower-variance model. This is most evident in the idea of the “wisdom of the crowd”, where large groups of individuals are often better at predicting something compared to a single expert. However, one area which hasn’t received much attention is outlier detection. When we say “outliers” we’re generally referring to observations that are exceptionally unusual compared to the rest of the sample. A rather consise definition by Hawkins (1980) states:\nThis rather broad definition fits well with the general application of outlier detection. It can be used for identifying fraud in insurance or healthcare datasets, intrusion detection for computer networks, or flagging anomalies in time-series data - among many others. The specific challenge I want to address in this mini-blog is an ensembling approach for unsupervised outlier detection. This is doubly interesting because unsupervised learning presents many more issues compared to supervised learning. Below, I’ll contrast some of these differences and then describe an interesting ensemble approach."
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#outlier-detection-with-supervision",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#outlier-detection-with-supervision",
    "title": "The Power of Ensembles",
    "section": "Outlier Detection with Supervision",
    "text": "Outlier Detection with Supervision\nTo start, let’s look at an example using the cardio dataset sourced from the pyod benchmark set. In this case we have 1831 observations with 21 variables, of which about 9.6% of them are considered anomalous. These are conviently labeled for us, where a value of 1 indicates an anomalous reading. If we fit a simple random forest classifier we see that it is trivial to get a very high AUC on the test data (let’s also not get ahead of ourselves here, as this is a toy dataset with a target that is quite easy to predict). Below we see an example of the fairly strong separation between the inliers and outliers. Our random forest works well in this case - giving us a test AUC of .99 and an average precision of .98. While this is an overly simple example, it does expose how easy some models can be (in many cases) when there is a definite target variable.\n\n\nCode\n# fit a random forest classifier\nrf = RandomForestClassifier()\nrf.fit(Xtrain, ytrain)\n\n# extract the predictions, calculate AUC\nrf_preds = rf.predict_proba(Xtest)[:,1]\n\neval_preds(ytest, rf_preds)\n\n\nRoc:0.997\nPrn:0.977\n\n\n\n\nCode\nsns.scatterplot(x = X[:,7], y = X[:,18], hue = y)\n(\n    plt.xlabel(\"Feature 7\"),\n    plt.ylabel(\"Feature 18\"),\n    plt.title(\"Cardio Scatterplot, Inliers and Outliers\")\n)\nplt.show()\n\n\n\n\n\nScatterplot of inliers (0) and outliers (1), displaying strong separation"
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#unsupervised-outlier-detection",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#unsupervised-outlier-detection",
    "title": "The Power of Ensembles",
    "section": "Unsupervised Outlier Detection",
    "text": "Unsupervised Outlier Detection\nLet’s talk about unsupervised outlier detection. Unlike the situation above in an unsupervised setting we don’t have the convenience of a set of labels identifying whether a given observation is anomalous or not. And because we’re lacking this ground truth, it makes things a lot more complicated for choosing both our model(s) and the parameters for those model(s). Let’s talk about why.\n\nHow do we select a best model?\nIn classic supervised learning we can choose a metric to optimize (say, root-mean squared error or log-loss), then fit a model which attempts to minimize that metric. In the simplest case, think about ordinary least squares. In that case we have a simple target of minimizing the sum of squared errors. We can validate the fit of the model by looking at evalution metrics (RMSE, R-Squared, standardized residuals).However, when we lack a way to identify if a given observation is anomalous or not we don’t have any meaningful way to know if one model is doing better than another.\nYou might also be thinking about optimizing a specific parameter in a model (like the number of nearest neighbors \\(K\\) in a K-nearest neighbors model) using some criterion that doesn’t rely on a target variable (like the ‘elbow’ method). However, optimizing this parameter doesn’t guarantee that the model itself is useful. Simply put, we’re often left groping around in the dark trying to decide what the optimal model or set of parameters is.\nLet’s consider this example: Say we’re looking at the same cardio dataset from above and trying to decide what unsupervised outlier detector we want to use. Maybe we’re deciding between a distance-based one like exact K-nearest neighbors (KNN) or a density-based one like local outlier factor (LOF). Let’s also say we’re agnostic to parameter choices, so we stick with the default ones provided by pyod.\n\n# initalize and fit using default params\nknn = KNN()\nlof = LOF()\n\nknn.fit(X)\nlof.fit(X)\n\n# extract the predictions, calculate AUC\nknn_pred = knn.decision_function(X)\n\neval_preds(y, knn_pred)\n\nRoc:0.686\nPrn:0.286\n\n\n\n\nCode\n# extract the predictions, calculate AUC\nlof_pred = lof.decision_function(X)\n\neval_preds(y, lof_pred)\n\n\nRoc:0.546\nPrn:0.154\n\n\nHere we see that the KNN model performs better than the LOF model - however we didn’t adjust any of the \\(K\\) parameters for either model. Because, in practice, we can’t see this, we don’t know a-priori which model or set of parameters will work best in a given case. This is in stark contrast to our first attempt when we could simply focus on decreasing out-of-sample bias."
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#an-unsupervised-ensambling-approach",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#an-unsupervised-ensambling-approach",
    "title": "The Power of Ensembles",
    "section": "An Unsupervised Ensambling Approach",
    "text": "An Unsupervised Ensambling Approach\nSo, because we can’t easily decrease bias (due to the lack of ground truth values) what are our options? Well, as we saw above, there is a large source of variance implicit in these models. This variance can come from multiple sources:\n\nChoice of model. There is considerable variance in how different models perform under different kinds of anomolies. For example, absent some evaluation metric how do you meaningfully choose between KNN, LOF, or one of many other methods. The pyod benchmark page shows that many models perform quite differently under different types of dimensionality and outlier proportion.\nChoice of parameters. Almost all anomaly detection models have added uncertaintly based on the choice of parameters. For example, in K-nearest neighbors we need to specify the parameter \\(K\\). One-class support vector machines (SVM) are notoriously difficult to tune in part due to the number of parameters to choose (choice of kernel, polynomial degree, ect…).\n\nTherefore, in an unsupervised model our best option is to try to reduce the variance implicit in both the sources above. Rather than staking our whole model on a single choice of model, or a single set of parameters, we can ensemble over a wide number of choices to avoid the risk of choosing a catastrophically bad combination. Since we don’t have access to ground truth, this ends up being the safest option (and as we will see, generally produces good results).\n\nALSO: A regression-based approach\nFor this specific post I’m going to focus on an unsupervised ensemble algothrim proposed by Paulheim & Meusel (2015) and further discussed in Aggarwal & Sathe (2017). The authors dub this method “attribute-wise learning for scoring outliers” or ALSO. The approach we are going to use extends the logic of the ALSO model to an ensemble-based approach (hence ALSO-E).\nLet’s talk a bit about the logic here. The general idea of the algothrim is that we iteratively choose a target feature \\(X_j\\) from the full set of features \\(X\\). The chosen \\(j\\) value is used as the target and the remaining \\(X - j\\) features are used to predict \\(j\\). We repeat this for all features in \\(X\\), collecting the standardized model residuals at each step. We then average the residuals across all the models and use them to identify “anomalous” observations. In this case, more anomalous observations will likely be ones whose residuals are substantially larger than the rest of the sample. The beauty of this method is that for the modelling portion we can choose any base model for prediction (e.g. linear regression, random forests, etc…).\nTo avoid models that are very overfit, or have virtually no predictive ability, we define weights for each model using cross-validated RMSE. The goal here is to downweight models that have low predictive ability so they have a proportionally lower effect on the final outlier score. This is defined as\n\\[w_k = 1 - min(1, RMSE(M_k))\\]\nwhich simply means that models that perform worse than predicting the mean (which would give us an RMSE of 1) are weighted to 0, while a theoretically “perfect” model would be weighted 1. This gives us the added benefit of downweighting features that have little or no predictive value, which helps in cases when we might have one or more irrelevant variables.\n\n\nAdding an E to ALSO\nThe base algothrim above fits \\(X\\) models using all \\(n\\) observations in the data. However, we can extend this model to an ensambling method by applying some useful statistical tools - namely variable subsambling. The idea proposed by Aggarwal & Sathe (2017) is to define a number of iterations (say, 100), and for each iteration randomly subsamble the data from between \\(min(1, \\frac{50}{n})\\) and \\(min(1, \\frac{1000}{n})\\). This means that each model is fit on a minimum of 50 observations and up to 1000 (or, \\(n\\) if \\(n\\) is less than 1000). In addition, we randomly choose a feature \\(j\\) to be predicted. Combined, this ensambling approach makes a more efficient use of the available data and results in a more diverse set of models.\nTo my knowledge, there is no “official” implementation of the ALSO-E algothrim, and it is not present in any large libraries (e.g. pyod or sklearn). However the method is generic enough that it is not difficult to code from scrach. Using the notes available I implemented the method myself using a random forest regressor as the base detector. The code below defines a class with a fit() and predict() function. The fit() function handles all the subsampling and fits each sample on a very shallow random forest regressor. The predict() function does the work of getting the residuals and re-weighting them according to their CV-error. While my implementation is certainly not “feature complete”, it’s good enough to try out:\n\n\nCode\n# code to fit an ALSOe anomaly detector\n\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nclass ALSOe():\n    \"\"\" Initializes a regression-based outlier detector\n    \"\"\"\n\n    def __init__(self, N = 100) -> None:\n\n        self.param_list = []\n        self.model_list = []\n        self.anom_list = []\n        self.wt = []\n    \n        self.N = N\n        self.std_scaler = StandardScaler()\n\n    def fit(self, data):\n        \"\"\" Fit an ensemble detector\n        \"\"\"\n\n        # standardize data\n        self.std_scaler = self.std_scaler.fit(X = data)\n        data = self.std_scaler.transform(data)\n\n        # fit N models\n        for i in range(0, self.N):\n\n            # define sample space\n            n = data.shape[0]\n            p = data.shape[1]\n            s = [min([n, 50]), min(n,1000)]\n\n            # draw s random samples from dataframe X\n            s1 = np.random.randint(low = s[0], high = s[1])\n            p1 = np.random.randint(low = 0, high = p)\n            ind = np.random.choice(n, size = s1, replace = False)\n\n            # define random y and X \n            df = data[ind]\n            y = df[:,p1]\n            X = np.delete(df, p1, axis=1)\n\n            # initalize RF regressor\n            rf = RandomForestRegressor(n_estimators=10)\n\n            # fit & predict\n            rf.fit(X, y)\n\n            # add fitted models & y param to list\n            self.model_list.append(rf)\n            self.param_list.append(p1)\n\n    def predict(self, newdata):\n\n        \"\"\" Get anomaly scores from fitted models\n        \"\"\"\n\n        # standardize data\n        newdata = self.std_scaler.transform(newdata)    \n\n        for i,j in zip(self.model_list, self.param_list):\n\n            # define X, y\n            y = newdata[:,j]\n            X = np.delete(newdata, j, axis=1)\n\n            # get predictions on model i, dropping feature j\n            yhat = i.predict(X)\n\n            # rmse\n            resid = np.sqrt(np.square(y - yhat))\n            resid = (resid - np.mean(resid)) / np.std(resid) \n\n            # compute and apply weights\n            cve = cross_val_score(i, X, y, cv=3, scoring='neg_root_mean_squared_error')\n            w = 1 - min(1, np.mean(cve)*-1)\n\n            resid = resid*w\n\n            # add weights and preds to lists\n            self.wt.append(w)\n            self.anom_list.append(resid)\n\n        # export results as min-max scaled\n        anom_score = np.array(self.anom_list).T\n        anom_score = np.mean(anom_score, axis = 1)\n\n        # rescale and export\n        anom_score = StandardScaler().fit_transform(anom_score.reshape(-1,1))\n        anom_score = anom_score.flatten()\n\n        return anom_score\n\n\n\nFitting the model\nWith all this in mind, fitting the actual model is quite simple. As stated above, the ALSO-E approach is largely parameter free, which means there isn’t much for the user to worry about. Here we’ll just initialize a model with 100 iterations, fit all of the random forest regressors, then get the weighted standardized residuals. This whole process can be condensed into basically 3 lines of code:\n\n# Fit an ALSOe regression ensemble\n# using 100 random forests\nad = ALSOe(N = 100)\nad.fit(X)\n\n# extract predictions from the models and combine\n# the standardized outlier scores\nad_preds = ad.predict(X)\n\n\n\nEvaluating the predictions\nNow that we have the predictions, we can look at the distribution of outlier scores. Below we see a histogram of the ensembled scores which, to recall, are rescaled to mean 0 and standard deviation 1. Therefore, the most anomalous observations will have large positive values. Consistent with what we would expect to see, there is a long tail of large residuals which correspond to the outliers, while the bulk of the data corresponds to a mostly “normal” set of values centered around zero.\n\n\nCode\nsns.histplot(x = ad_preds)\n(\n    plt.xlabel(\"Anomaly Score\"),\n    plt.ylabel(\"Observations\"),\n    plt.title(\"ALSO-E Anomaly Scores\")\n)\nplt.show()\n\n\n\n\n\nHistogram of anomaly scores. The characteristic long tail highlights potential anomalous observations\n\n\n\n\nWe can evaluate the performance of our method by bootstrapping the original dataset 10 times, then running our model on each of the boostrap replicates. This is because there is bound to be some degree of randomness based on the chosen samples and variables for each iteration. Averaging over the bootstrap replicates helps give us some idea of how this model might perform “in the wild” so to speak. Below I define a little helper function to resample the dataset, fit the model, and then extract the relevant evaluation metrics. We then loop through the function and put the fit statistics in a set of lists. For evaluation we’ll look at the averages for each set.\n\n# Bootstrap sample from base dataset and evaluate metrics\ndef boot_eval(df):\n    X, y = resample(df['X'], df['y'])\n\n    ad = ALSOe(N = 100)\n    ad.fit(X)\n    ad_preds = ad.predict(X)\n\n    auc = roc_auc_score(y, ad_preds)\n    pre = average_precision_score(y, ad_preds)\n\n    return [auc, pre]\n\n# run models\nroc_list = []\nprn_list = []\n\nfor i in range(10):\n    roc, prn = boot_eval(data)\n\n    roc_list.append(roc)\n    prn_list.append(prn)\n\nShown below, we see we have a decent AUC and average precision score of about 0.71 and 0.26 respectively. While this is substantially lower than the supervised model, it is still better than the base KNN and LOF models above. The ensambling process also makes it easy because we don’t have to specify any parameters other than the number of iterations to run. In testing, the default 100 works well as a starting point, and there aren’t huge performance gains by increasing it substantially.\n\n\nCode\nprint(f'Avg. ROC: {np.round(np.mean(roc_list),3) }\\nAvg. Prn: {np.round(np.mean(prn_list),3)}')\n\n\nAvg. ROC: 0.693\nAvg. Prn: 0.261\n\n\n\n\n\nComparing performance across datasets\nWe can also evaluate its performance on a variety of other datasets. Here I randomly chose another four datasets from the pyod benchmarks page and compared its performance over 10 bootstrap resamplings to the other benchmarked methods in the pyod ecosystem. Looking at the results we see that we get median RocAUC scores of between .7 to .85 and average precision scores between .2 to .85. For an unsupervised model this isn’t too bad, and largely falls within the range of other detectors.\nWe should note that while its performance is never the best, it is also never the worst either. For example: the .707 we achieved on the cardio dataset is lower than some of the best methods (in this case, PCA and Cluster-Based LOF). However, we avoid extremely bad results like with Angle-Based Outlier Detection or LOF. This underscores our goals with the ensemble model: we prefer a more conservative model that tends to perform consistently across many types of anomalies. We also avoid issues related to choosing optimal parameters but simply ensambling over many detectors. In an unsupervised case this decrease in variance is especially desirable.\n\n\nCode\n# load additional datasets\nd1 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/14_glass.npz\")\nd2 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/18_Ionosphere.npz\")\nd3 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/20_letter.npz\")\nd4 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/21_Lymphography.npz\")\n\ndlist = [d1,d2,d3,d4]\nmname = ['Cardio', 'Glass','Ionosphere','Letter','Lympho']\n\nroc_list_m = []\nprn_list_m = []\n\n# run models\nfor j in dlist:\n \n    for i in range(10):\n        roc, prn = boot_eval(j)\n\n        roc_list_m.append(roc)\n        prn_list_m.append(prn)\n\n\n# Plot evaluation metrics across datasets\nevaldf = pd.DataFrame({'RocAUC' : roc_list +roc_list_m, \n                       'Precision' : prn_list + prn_list_m,\n                       'Dataset': sorted([x for x in mname*10])})\\\n            .melt(id_vars = 'Dataset', var_name = 'Metric', value_name = 'Score')\n\n# facet plot across datasets\ng = sns.FacetGrid(evaldf, col = 'Metric', sharey = False, col_wrap=1, aspect = 2)\n(\n    g.map(sns.boxplot, 'Dataset','Score', order = mname),\n    g.fig.subplots_adjust(top=0.9),\n    g.fig.suptitle('ALSO-E Model Evaluation', fontsize=16)\n)\nplt.show()\n\n\n\n\n\nensemble models often are often not as good as the best method, but can achieve consistently decent performance."
  },
  {
    "objectID": "posts/pca-anomaly/pca_anomaly.html",
    "href": "posts/pca-anomaly/pca_anomaly.html",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "I strongly believe in “learning by doing”. One of the things I have been working on quite a bit lately is unsupervised anomaly detection. As with many machine-learning tools, ensembles are incredibly powerful and useful for a variety of circumstances.\nAnomaly detection ensembles are no exception to that rule. To better understand how each of the individual pieces of a anomaly detection ensemble works, I’ve decided two build one myself “from scratch”. I put that in giant quotes here because I’ll still rely on some existing frameworks in R to help built the underlying tools.\nMy idea is to create an ensemble of several heterogeneous anomaly detection methods in a way that maximizes their individual benefits. Following some of the guidance proposed by Aggarwal & Sathe I will use:\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\nhosp <- read_csv(\"Hospital_Inpatient_Discharges__SPARCS_De-Identified___2021.csv\")\n\n\nThe data for this post comes from the state of New York’s Hospital Inpatient Discharges (SPARCS) data for 2021. This data contains about 2.1 million records on hospital discharges, including some de-identified patient information, procedure types, and costs. From an anomaly detection standpoint, it might make sense to see if we can identify hospitals with anomalous costs relative to other New York Hospitals. Like the New York Times reported, hospitals make up a very substantial portion of what we spent on healthcare.\nWe’ll create a quick feature set based on a few key payment variables. Here I aggregate over each of the hospitals by calculating their (1) average stay length, (2) average charges, (3) average costs, (4) their average cost-per-stay, (5) the ratio between costs to total charges and (6) the average procedure pay difference.\nThis last feature is a bit more complex, but what I am essentially doing is finding the median cost per-procedure by case severity (assuming more severe cases cost more) and then finding out how much each hospital diverges, on average, from the global cost. It’s a indirect way of measuring how much more or less a given procedure costs at each hospital. This is a easy measure to utilize because a 0 indicates that the hospital bills that procedure at near the global median, a 1 means they bill 100% more than the median and -.5 means they bill 50% less than the median.\n\n\nCode\n# compute and aggregate feature set\ndf <-\n  hosp %>%\n  group_by(`CCSR Procedure Code`, `APR Severity of Illness Code`) %>%\n  mutate(\n    proc_charge = median(`Total Costs`),\n    charge_diff = (`Total Costs` - proc_charge)/proc_charge,\n    `Length of Stay` = as.numeric(ifelse(\n      `Length of Stay` == '120+', 120, `Length of Stay`\n    ))\n  ) %>%\n  group_by(id = `Permanent Facility Id`) %>%\n  summarise(\n    stay_len = mean(`Length of Stay`, na.rm = T),\n    charges = mean(`Total Charges`),\n    costs = mean(`Total Costs`),\n    diff = mean(charge_diff),\n    cost_ratio = mean(`Total Costs`/`Total Charges`),\n    cost_per_stay = costs / stay_len\n  )"
  },
  {
    "objectID": "posts/pca-anomaly/pca_anomaly.html#creating-a-pca-anomaly-detector",
    "href": "posts/pca-anomaly/pca_anomaly.html#creating-a-pca-anomaly-detector",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Creating a PCA Anomaly Detector",
    "text": "Creating a PCA Anomaly Detector\nLet’s now get into the nuts and bolts of actually creating a PCA-based anomaly detector. Now, there are a few ways we can go about this, but I’m going to rely on the approach suggested by Charu Aggarwal in his book Outlier Analysis. What he essentially proposes is a “soft” version of principal components where the eigenvectors are weighted by their variance instead of choosing only the eigenvectors that explain the highest proportion of the overall variance. This “soft” approach has some overlap with the mahalanobis distance. This is the same approach taken by the PCA anomaly detector in the Python pyod package if weighting is specified.\n\nPerforming the “soft” PCA\nTranslating this approach from the formula to code is actually pretty straightforward. Agarwal gives us the following:\n\\[Score(\\bar{X}) = \\sum^d_{j=1} \\frac{|(\\bar{X} - \\bar{\\mu}) *\\bar{e_j}|^2}{\\lambda_j}\\] Which we can code into the following below. Before we add our data to the PCA we scale it to have mean 0 and standard deviation 1 so that the input features are scale-invariant. We then work through the process of extracting the eigenvectors, computing the variance for each, and then performing the “soft” PCA approach.\n\n\n\n\n\n\nNote\n\n\n\nA few notes - strictly speaking the the part \\(\\bar{X} - \\bar{\\mu}\\) isn’t totally necessary in most cases because the eigenvectors are already scaled to have a mean of zero (you can also ensure this by specifying cor=TRUE in the princomp() function). This does help in unusual cases where \\(\\bar{\\mu}\\) is not zero.\n\n\n\n# \"Soft\" PCA\n\n# scale input attributes\nX <- df[, 2:7]\nX <- scale(X)\n\n# pca anomaly detection\n# extract eigenvectors & variance\npca <- princomp(X, cor = TRUE)\ne <- pca$scores\nev <- diag(var(e))\nmu <- apply(e, 2, mean)\nn <- ncol(e)\n\n# compute anomaly scores\nalist <- vector(mode = \"list\", length = n)\nfor(i in 1:n){\n  alist[[i]] <- abs( (e[, i] - mu[i])^2 / ev[i])\n}\n\n# extract values & export\nXscore <- as.matrix(do.call(cbind, alist))\nanom <- apply(Xscore, 1, sum)\n\nThis “soft” PCA is in contrast to so-called “hard” PCA where a specific number of principal components are chosen and the remainder are discarded. The “hard” PCA primarily focuses on reconstruction error along the components with the most variance, while the “soft” approach weights outliers on the lower variance components higher. This is useful because the data vary much less on these components, so outliers are often more obvious along these dimensions.\n\n\nCode\npca$scores %>%\n  data.frame() %>%\n  pivot_longer(cols = starts_with(\"Comp\")) %>%\n  ggplot() +\n  geom_histogram(aes(x = value), bins = 30, fill = '#004488') +\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_minimal() +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\nOutliers are often more visable along components with lower variance.\n\n\n\n\nThe code below implements the PCA anomaly detector. Most of the work involves taking the scores, putting them into a matrix, and then re-weighting them by their variance. The final score is just the row-wise sum across the re-weighted columns. The output is just a vector of anomaly scores anom.\n\n\nCode\nggplot(tibble(anom)) +\n  geom_histogram(aes(x = anom), bins = 25, color = 'white', fill = '#004488')+\n  labs(x = \"Anomaly Score\", y = \"Observations\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nDistribution of outlier scores. More anomalous observations have higher scores\n\n\n\n\n\n\nFlagging Outliers\nFor flagging outliers we can rely on the \\(\\chi^2\\) distribution. This is handy because the \\(\\chi^2\\) distribution is formed as the sum of the squares of \\(d\\) independent standard normal random variables. For the purposes of this example we might just choose 1% as our threshold for anomalies. After flagging we can plot the data along different 2d distributions to see where they lie. For example, the 8 anomalous hospitals generally have both longer stay lengths and charge more for comparable procedures relative to other hospitals.\n\n\nCode\n# compute a p-value for anomalies and\n# append anomaly score and p-values to new dataframe\np = sapply(anom, pchisq, df=6, ncp = mean(anom), lower.tail=F)\n\nscored_data <- data.frame(df, anom, p)\n\nflag <- scored_data$p <= 0.01\n\nggplot() +\n  geom_point(data = scored_data, aes(x = stay_len, y = diff), color = '#004488', size = 2, alpha = .25) +\n  geom_point(data = scored_data[flag,], aes(x = stay_len, y = diff), color = '#BB5566', size = 2.5) +\n  labs(x = \"Stay Length\", y = \"Avg. Payment Difference\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nPlotting inliers (blue) and outliers (red) in 2D space.\n\n\n\n\n\n\nPCA Anaomaly Detector: Example Function\nHere’s a minimal working example of the procedure above. As we build our ensemble, we’ll come back to this function later.\n\n\nCode\n# Run a principal components anomaly detector\nadPCA <- function(X){\n  \n  # pca anomaly detection\n  # extract eigenvectors & variance\n  pca <- princomp(X, cor = TRUE)\n  e <- pca$scores\n  ev <- diag(var(e))\n  mu <- apply(e, 2, mean)\n  n <- ncol(e)\n  \n  # compute anomaly scores\n  alist <- vector(mode = \"list\", length = n)\n  for(i in 1:n){\n    alist[[i]] <- abs( (e[, i] - mu[i])^2 / ev[i])\n  }\n  \n  # extract values & export\n  Xscore <- as.matrix(do.call(cbind, alist))\n  anom <- apply(Xscore, 1, sum)\n  \n  return(anom)\n}"
  },
  {
    "objectID": "posts/pca-timeseries/pca_ts.html",
    "href": "posts/pca-timeseries/pca_ts.html",
    "title": "Anomaly Detection for Time Series",
    "section": "",
    "text": "Identifying outliers in time series is one of the more common applications for unsupervised anomaly detection. Some of the most common examples come from network intrusion detection, mechanical processes, and other types of high-volume streaming data.\nOf course, there are just as many proposed ways of identifying outliers from the simple (basic Z-scores) to the complex (convolutional neural networks). There are also some approaches that rely on more conventional tabular approaches. Rob Hyndman proposed a few approaches here and here showing how many high-volume time series can be compressed into a tabular dataset. The general idea is that you can decompose many time series into tabular observations by creating a large variety of features describing each series.\n\n\nThe data we’ll use is on hourly power usage for a large power company (American Electric Power). From this dataset we can perform some basic aggregation (to ensure that all timestamped values are on the same day-hour), then separate each set of 24 hours into their individual days. The goal here is to make it easier to look at hours within each day. The code below does a bit of this processing. Of course, working with dates is still always a pain, despite the improvements in R libraries.\n\n\nCode\n# Read data, convert to zoo\nelec <- read_csv(\"AEP_hourly.csv\") %>%\n  group_by(Datetime) %>%\n  summarise(AEP_MW = sum(AEP_MW)) %>%\n  filter(year(Datetime) %in% 2017)\n\nelec_ts <- zoo(x = elec$AEP_MW, order.by = elec$Datetime, frequency = 24)\n\n# Split the hourly time series into daily time series\ndaily_ts_list <- split(elec_ts, as.Date(index(elec_ts)))\n\n# Extract the first 24 observations of each daily time series\n# dropping days with missing values\ndaily_24_ts_list <- lapply(daily_ts_list, function(x) {\n  if (length(x) >= 24) {\n    return(x[1:24])\n  } else {\n    return(NA)\n  }\n})\n\n# Convert from list to dataframe\ndaily_24_ts_list <- purrr::discard(daily_24_ts_list, ~any(is.na(.)))\n\n\nAfter converting the list of values to a data frame, we can proceed with the featurization. As we said before, we can use the tsfeatures library to decompose each day’s hourly values into a single observation. We can see this creates a data frame with 17 features, which correspond to various measures, including: autocorrelation, seasonality, entropy and other ad-hoc measures of time series behavior.\n\n# Convert from list to dataframe, extract TS features\ndaily_24_ts_list <- purrr::discard(daily_24_ts_list, ~ any(is.na(.)))\n\n# create time series features using `tsfeatures`\ndf <- daily_24_ts_list %>%\n  tsfeatures(\n    features = c(\n      \"acf_features\",\n      \"stl_features\",\n      \"entropy\",\n      \"lumpiness\",\n      \"stability\",\n      \"max_level_shift\"\n    )\n  ) %>%\n  select(-nperiods,-seasonal_period)\n\n\n\nCode\nglimpse(df, width = 65)\n\n\nRows: 364\nColumns: 13\n$ x_acf1      <dbl> 0.8139510, 0.9318718, 0.9123398, 0.9173901,…\n$ x_acf10     <dbl> 1.771656, 2.081091, 1.868137, 1.889912, 1.7…\n$ diff1_acf1  <dbl> 0.6284980, 0.6181949, 0.6534759, 0.6194560,…\n$ diff1_acf10 <dbl> 1.4141518, 0.7771648, 0.6910469, 1.0491669,…\n$ diff2_acf1  <dbl> 0.3426413, 0.2082615, 0.3164917, 0.2786098,…\n$ diff2_acf10 <dbl> 0.5009215, 0.2550913, 0.4214296, 0.2577332,…\n$ trend       <dbl> 0.7256036, 0.9374949, 0.9377550, 0.9478148,…\n$ spike       <dbl> 1.946527e-04, 4.715699e-06, 6.706533e-06, 4…\n$ linearity   <dbl> 1.8002798, 3.6310069, 3.3908752, 4.1128389,…\n$ curvature   <dbl> 0.4988011, -1.2265007, -2.0788821, -0.66100…\n$ e_acf1      <dbl> 0.6719203, 0.6507832, 0.6513875, 0.6636410,…\n$ e_acf10     <dbl> 1.4588480, 1.0862781, 0.9523931, 1.1326925,…\n$ entropy     <dbl> 0.3585884, 0.5225069, 0.4962494, 0.3090901,…\n\n\n\n\n\nAfter doing this, we can proceed as a normal tabular data problem. The PCA anomaly detector that was detailed in an earlier post is an easy plug in here and is a natural fit for the problem. We have a lot of highly correlated measures that likely share a large amount of variance across a few dimensions. We can then weight the lower-variance dimensions higher to identify anomalous series. We’ll use the \\(\\chi^2\\) distribution to derive a p-value, which we can then threshold for flagging outliers.\n\n# Perform anomaly detection\nanom <- adPCA(df)\np = sapply(anom, pchisq, df=ncol(df), ncp = mean(anom), lower.tail=F)\n\n\n\n\nWe can see which series were flagged by the model by highlighting the series which were flagged at the \\(p < .01\\)\n\n\nCode\n# flag observations at p < 0.01\nelec_plot <- elec %>%\n  mutate(date = as.Date(format(Datetime, \"%Y-%m-%d\")),\n         hour = hour(Datetime)) %>%\n  left_join(scored_data) %>%\n  mutate(flag = ifelse(p < 0.01,1,0))\n\nggplot(data = elec_plot, aes(x = Datetime, y = AEP_MW, group = date)) +\n  geom_line(color = '#004488', alpha = .125) +\n  geom_line(data = elec_plot[elec_plot$flag == 1,], color = '#BB5566') +\n  labs(x = \"Date\", y = \"Megawatt Hours\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nAnamolous days are highlighted in red. Note the unusually high spike in late 2017.\n\n\n\n\nWe can also see what these anomalous series look like compared to the other series on a hourly basis. This plot clearly shows one series with a unusual early-morning spike, and several series with flatter trajectories compared to more normally expected seasonality - in particular, they are days with low power consumption in the afternoon when consumption is usually at its highest.\n\n\nCode\nggplot(data = elec_plot, aes(x = hour, y = AEP_MW, group = date)) +\n  geom_line(color = '#004488', alpha = .075) +\n  geom_line(data = elec_plot[elec_plot$flag == 1,], color = '#BB5566', size = .7) +\n  labs(x = \"Hour of Day\", y = \"Megawatt Hours\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nAnomalous days often have flatter curves and dip during high-load hours of the day.\n\n\n\n\nArguably this isn’t an ideal approach because each sub-series is only comprised of 24 observations. That means reliably identifying seasonality via the stl_features is questionable. In addition, this approach loses some information that comes from day-to-day correlations. It would probably be worthwhile testing this approach against something like STL decomposition."
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#spatial-features",
    "href": "posts/rtm-spatial/rtm.html#spatial-features",
    "title": "Generating Spatial Risk Features using R",
    "section": "Spatial Features",
    "text": "Spatial Features\nIn criminology there is a considerable research on the role that fixed spatial features in the environment have on crime. These spatial risk factors have criminogenic qualities that make them “attractors” or “generators”(Brantingham and Brantingham 1995). Absent some change, these places typically contribute a disproportionate share of crime that is largely stable over time(Sherman, Gartin, and Buerger 1989). The classic example is a bar or night club. Alcohol plays a large role in a lot of crime, and locations where many people congregate and become intoxicated also have higher incidences of crime. We can use information about the environment to help solve problems or prioritize patrol areas.\nOne challenge in research is obtaining the point locations for these features. Generally when we perform some kind of spatial analysis we have a study area (e.g. a city or other boundary file) and a set of labeled point features corresponding to the locations of interest. However, reliable places to get this information is often hard to come by. Some cities provide open data portals with commercial information, but these are typically limited to larger cities. In my work I’ve had people ask how to get spatial risk factors for their research, often times for something related to the “Risk Terrain Modeling” approach of spatial analysis. I’ve worked on a few projects now where I’ve had to generate these myself, and have had some success using open data sources like Google to help with it."
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#querying-google-places",
    "href": "posts/rtm-spatial/rtm.html#querying-google-places",
    "title": "Generating Spatial Risk Features using R",
    "section": "Querying Google Places",
    "text": "Querying Google Places\nGoogle has a lot of paid API services which are quite useful for researchers. In most cases there is a free tier, and for smaller one-off projects this makes their API services attractive for research. Let’s walk through an example of how we might do this. For our example we will use the Swedish city of Malmö (which, incidentially is a very lovely city I’ve been lucky enough to visit). We have a shapefile that looks like this:\n\n\n\n\n\nOur goal is to query theGoogle Places API to get the locations of criminogenic spatial risk factors (here, bars and gas stations). One significant limitation with the Google Places API is that there is a limit to the number of locations that will show up for a single query. This means if you ran the query on the entire city, it would only return up to 20 locations. However, we can bypass this by running multiple queries on smaller spatial regions. Other bloggers have provided similar advice as well (see here and here).\nTo do the actual interfacing with the Google Places API we will use the very handy googleway package.\n\nSplitting up into a grid\n\n\nCode\n# GOOGLE PLACES API CODE\n# ================================================= #\n# Giovanni Circo\n# gmcirco42@gmail.com\n#\n# Code to query google place api\n# Divides a boundry shapefile into grid cells\n# of radius r, then queries the api for each cell\n#\n# NOTE:\n# Only requires a free version of the API. Doesn't\n# incur any costs.\n# ================================================= #\n\nlibrary(googleway)\nlibrary(sf)\nlibrary(tidyverse)\n\n# API Key\n# instructions here:\n# https://developers.google.com/maps/documentation/places/web-service/get-api-key\nmykey <- \"[INSERT GOOGLE PLACES API KEY HERE]\"\n\n# Location shapefile\nboundry <- st_read(\"...\\DeSo_Malmö.shp\")\n\n# specify grid size (meters)\nr <- 1200\n\n# Make a grid\nboundry_grid <- st_make_grid(boundry, cellsize = r)\nboundry_grid <- boundry_grid[boundry]\n\n# Transform to lat\\lon\n# Extract coords\narea_coords <- st_transform(boundry_grid, crs = 4326) %>%\n  st_centroid() %>%\n  st_coordinates() %>%\n  data.frame() %>%\n  select(lat = Y, lon = X)\n\n\nWe can divide the city into a series of grids, then iterate through each grid cell and query within it. This way we are more likely to obtain all of the relevant features in that grid cell without hitting the limit. Here, I create 150 1200 square meter grid cells, which gives us something like this:\n\n\n\n\n\nIn addition we extract the X-Y coordinates for the grid centroid, which we will use as our location for the API query. This means we hit the API 150 times, once for each grid cell. This is well within the free number that Google allows.\n\n\nQuerying our features\n\n\nCode\n# EXTRACT FEATURES ON GRID\n#----------------------------#\n\n# Supported place type names:\n# https://developers.google.com/maps/documentation/places/web-service/supported_types\n# Features: gas_station, bar, liquor_store, night_club, pharmacy, restaurant\n\n# Specify feature type\n# number of grid cells\nfeature <- \"bar\"\nn <- nrow(area_coords)\n\n# First, set up function to query google places\n# for each grid centroid\n# add to a list\n\narea_list <- list()\nfor(i in 1:n){\n  \n  area_list[[i]] <-\n    google_places(location = unlist(area_coords[i,]),\n                  place_type = feature,\n                  radius = r,\n                  key = mykey)\n}\n\n# Function to convert results from above\n# to a dataframe suitable for rbinding\n# then conversion to an sf object\nconvert_to_dataframe <-\n  function(x) {\n    \na <- x$results\n\nb <- tibble(\n  lat = a$geometry$location$lat,\n  lon = a$geometry$location$lng,\n  name = a$name,\n  types = a$types,\n  address = a$vicinity,\n  place_id = a$place_id\n)\n\nreturn(b)\n  }\n\n# Rbind the results, and then un-nest on feature type\n# This creates a long-form dataframe that you can then filter\n# based on feature type\narea_dataframe <-\n  do.call(rbind, lapply(area_list, convert_to_dataframe)) %>%\n  distinct(place_id, .keep_all = TRUE) %>%\n  unnest(types)\n\n# Get just feature requested\nfeature_out <- area_dataframe %>%\n  filter(types %in% feature) %>%\n  distinct(address, .keep_all = TRUE)\n\n\nWe can use the code above to iterate through each grid cell, hit the API, and then store the results in a list. I include a few helper functions to assist with pulling out the names and coordinates, binding them into a dataframe, and setting them up to export. The key bit of code is below, which is the part that queries the API for each of the grid cells:\n\narea_list <- list()\nfor(i in 1:n){\n  \n  area_list[[i]] <-\n    google_places(location = unlist(area_coords[i,]),\n                  place_type = feature,\n                  radius = r,\n                  key = mykey)\n}"
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#calculating-grid-cell-distances",
    "href": "posts/rtm-spatial/rtm.html#calculating-grid-cell-distances",
    "title": "Generating Spatial Risk Features using R",
    "section": "Calculating Grid Cell Distances",
    "text": "Calculating Grid Cell Distances\n\n\nCode\ncompute_distance <- function(grid, feature){\n  # get nearest point from grid to feature\n  nearest <- st_nearest_feature(grid,feature)\n  nearest_dist <- st_distance(grid, feature[nearest,], by_element = TRUE)\n  \n  return(nearest_dist)\n}\n\n# specify grid size (meters)\nr <- 250\n\n# Make a city grid\ncity <- st_make_grid(boundry, cellsize = r, square = FALSE)\ncity <- city[boundry]\n\n# get distances\nbar_dist <- compute_distance(city, bar)\ngas_dist <- compute_distance(city, gas)\n  \n# create long-form dataframe\ntbl <- tibble(city) %>%\n  mutate(bar = bar_dist,\n         gas_station = gas_dist) %>%\n  pivot_longer(-city, \n               names_to = \"feature\", \n               values_to = \"dist\") %>%\n  mutate(dist = as.numeric(dist)) %>%\n  st_as_sf()\n\n\nNow that we have our city boundary and our spatial risk factors, all we need to do now is compute the distance from each grid cell to its nearest risk factor. In the end, what we will want is a dataframe with grid cell ids, and columns corresponding to distance to the nearest feature. After merging them, we can create a nice map like this - showing the location sand distances of our risk factors.\n\n\n\n\n\nYou can then use these features in other kinds of spatial risk models (for a great walk through, see (Wheeler and Steenbeek 2021).The big advantage of this approach is that you have the flexibility to implement any kind of model you want at this point - whether it is a conventional RTM model, or a boosted tree model."
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#full-code",
    "href": "posts/rtm-spatial/rtm.html#full-code",
    "title": "Generating Spatial Risk Features using R",
    "section": "Full Code",
    "text": "Full Code\n\n\nCode\n# GOOGLE PLACES API CODE\n# ================================================= #\n# Giovanni Circo\n# gmcirco42@gmail.com\n#\n# Code to query google place api\n# Divides a boundry shapefile into grid cells\n# of radius r, then queries the api for each cell\n#\n# NOTE:\n# Only requires a free version of the API. Doesn't\n# incur any costs.\n# ================================================= #\n\nlibrary(googleway)\nlibrary(sf)\nlibrary(tidyverse)\n\n# API Key\n# instructions here:\n# https://developers.google.com/maps/documentation/places/web-service/get-api-key\nmykey <- \"[INSERT GOOGLE PLACES API KEY HERE]\"\n\n# Location shapefile\nboundry <- st_read(\"DeSo_Malmö.shp\")\n\n# specify grid size (meters)\nr <- 1200\n\n# Make a grid\nboundry_grid <- st_make_grid(boundry, cellsize = r)\nboundry_grid <- boundry_grid[boundry]\n\n# Transform to lat\\lon\n# Extract coords\narea_coords <- st_transform(boundry_grid, crs = 4326) %>%\n  st_centroid() %>%\n  st_coordinates() %>%\n  data.frame() %>%\n  select(lat = Y, lon = X)\n\nplot(boundry_grid)\n  \n# EXTRACT FEATURES ON GRID\n#----------------------------#\n\n# Supported place type names:\n# https://developers.google.com/maps/documentation/places/web-service/supported_types\n# Features: gas_station, bar, liquor_store, night_club, pharmacy, restaurant\n\n# Specify feature type\n# number of grid cells\nfeature <- \"bar\"\nn <- nrow(area_coords)\n\n# First, set up function to query google places\n# for each grid centroid\n# add to a list\n\narea_list <- list()\nfor(i in 1:n){\n  \n  area_list[[i]] <-\n    google_places(location = unlist(area_coords[i,]),\n                  place_type = feature,\n                  radius = r,\n                  key = mykey)\n}\n\n# Function to convert results from above\n# to a dataframe suitable for rbinding\n# then conversion to an sf object\nconvert_to_dataframe <-\n  function(x) {\n    \na <- x$results\n\nb <- tibble(\n  lat = a$geometry$location$lat,\n  lon = a$geometry$location$lng,\n  name = a$name,\n  types = a$types,\n  address = a$vicinity,\n  place_id = a$place_id\n)\n\nreturn(b)\n  }\n\n# Rbind the results, and then un-nest on feature type\n# This creates a long-form dataframe that you can then filter\n# based on feature type\narea_dataframe <-\n  do.call(rbind, lapply(area_list, convert_to_dataframe)) %>%\n  distinct(place_id, .keep_all = TRUE) %>%\n  unnest(types)\n\n# Get just feature requested\nfeature_out <- area_dataframe %>%\n  filter(types %in% feature) %>%\n  distinct(address, .keep_all = TRUE)\n\n# Now export as a shapefile\n# re-assign the crs of the boundry shapefile\nfeature_out %>%\n  st_as_sf(coords = c('lon','lat'), crs = 4326) %>%\n  st_transform(crs = st_crs(boundry)) %>%\n  st_write(paste0(\"Desktop\\\\\",feature,\".shp\"))"
  }
]