[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m Gio Circo, a Ph.D. in criminology and former assistant professor. I currently work at Gainwell technologies as a data scientist. My work primarily focuses on causal inference, Bayesian statistics, and anomaly detection.\nI’m big into food, wine, and spirits - especially cocktails."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nMichigan State University | Lansing, MI | PhD in Criminal Justice | August 2013 - May 2018\nIllinois State University| Normal, IL | B.A in Criminal Justice | August 2010 - June 2012"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nGainwell Technologies | Data Scientist | June 2022 - present\nUniversity of New Haven | Assistant Professor | August 2018 - May 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gio Circo, Ph.D.",
    "section": "",
    "text": "Visualizing journeys between cities\n\n\n\n\nR\n\n\nSpatial Statistics\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nOr: An Old Dog Learning New Tricks\n\n\n\n\nNatural Language Processing\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nA deeper dive with Bayes\n\n\n\n\nR\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nA short discussion on ‘microsynthetic’ controls\n\n\n\n\nR\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nCreating ‘RTM’ style map data\n\n\n\n\nR\n\n\nSpatial Statistics\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nHow good are Rhett and Link?\n\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nPart 3: Histogram-based anomaly detector\n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nPart 2: K-nearest neighbors anomaly detector\n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nApplying a PCA anomaly detector\n\n\n\n\nAnomaly Detection\n\n\nPCA\n\n\nTime Series\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nPart 1: Principal components anomaly detector\n\n\n\n\nAnomaly Detection\n\n\nPCA\n\n\nEnsembles\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nCar Crashes in Austin\n\n\n\n\nSpatial Statistics\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\nAdventures in outlier detection\n\n\n\n\nAnomaly Detection\n\n\nEnsembles\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian Statistics\n\n\nHLM\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nGio Circo, Ph.D\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html",
    "href": "posts/austin-vehicle/spatial_merging.html",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\n\n\n# DATA SOURCES AND INFO\n# -----------------------------#\n# https://data.austintexas.gov\n#   vehicle crashes: 'Transportation-and-Mobility/Vision-Zero-Crash-Report-Data-Crash-Level-Records'\n#   traffic cameras: 'Transportation-and-Mobility/Traffic-Cameras/'\n#   austin council map: 'dataset/Boundaries-City-of-Austin-Council-Districts'\n\n# Select a specific Austin Council District and year\n# see: https://maps.austintexas.gov/GIS/CouncilDistrictMap/\ncnl <- 3\nyr <- 2022\n\n\n# DATA LOADING\n# -----------------------------#\n\n# Get Austin shapefile, pull only the district we need\naustin <- st_read(\"C:/Users/gioc4/Documents/blog/data/austin_city.shp\", quiet = TRUE) %>%\n  st_transform(crs = 32614) %>%\n  filter(council_di %in% cnl)\n\n# Read traffic camera data & vehicle crash data\n# Limit crashes to a specific year, conver to spatial\ncamera <- st_read(\"C:/Users/gioc4/Documents/blog/data/traffic_camera.shp\", quiet = TRUE) %>%\n  filter(camera_sta == \"TURNED_ON\") %>%\n  distinct(geometry, .keep_all = TRUE) %>%\n  st_transform(crs = 32614) %>%\n  mutate(camera_X = st_coordinates(.)[,1],\n         camera_Y = st_coordinates(.)[,2])\n\ncrash <- read_csv(unz(\"C:/Users/gioc4/Documents/blog/data/austin_crash.zip\",\"crash_data.csv\")) %>%\n  mutate(crash_date = strptime(crash_date, format=\"%m/%d/%Y %H:%M\")) %>%\n  filter(year(crash_date) == yr)\n\n# Convert crash to sf, extract coordinates\ncrash_sf <- crash %>%\n  filter(!is.na(latitude), !is.na(longitude)) %>%\n  st_as_sf(coords = c('longitude', 'latitude')) %>%\n  st_set_crs(4326) %>%\n  st_transform(crs = st_crs(camera)) %>%\n  mutate(crash_X = st_coordinates(.)[,1], \n         crash_Y = st_coordinates(.)[,2]) %>%\n  select(crash_id, crash_date, crash_X,crash_Y)\n\n# Clip to region\ncamera <- camera[austin,]\ncrash_sf <- crash_sf[austin,]\n\n\nThis is a bit of a mini-blog post based on a workflow that I have used based on some of my own work. A common issue in spatial analysis - and especially in criminology - is the need to analyze points that are merged to another point.\nIn criminology we might say that assaults occurring right outside of a bar are within it’s “spatial influence”. Typically what is done is we define a “buffer” around each of the points \\(j\\) (like bars, or gas stations) of interest and merge all of the crime incidents \\(i\\) that are within each of the \\(j\\) points’ buffer area. This is something I’ve done before looking at the effect of CCTV cameras on crime at businesses in Detroit. This is pretty common across a lot of criminology research (e.g. finding all crime that occurs within a 1-block radius of bars and liquor stores).\nWhile I used to use the “buffer” method, I think there is a more efficient way of doing this via Voronai polygons which accomplishes the same goal, and allows for more flexibility in analysis. Let’s illustrate this using some data from the city of Austin. In this example we are going to look at the incidence of car crashes \\(i\\) around traffic cameras \\(j\\). Our goal will be to merge car crashes to the nearest traffic camera within a defined spatial range.\nHere’s the study area - one of the Austin city council districts, showing the traffic cameras in blue, and the location of crashes in red. In the picture below there are 58 cameras and about 1,700 car accidents. For this example we’re restricting our analysis to only accidents that occurred in 2022 and using cameras that were active (TURNED_ON) at the time. We can see that there are a lot of accidents, many of them quite far from a traffic camera. Let’s say we want to define a study area around each traffic camera of about 300 meters - or about 980 feet.\n\n\nCode\nggplot() +\n  geom_sf(data = austin, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = crash_sf, color = '#BB5566', size = .6, alpha = .7) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nLocation of car crahes (red) and traffic cameras (blue)."
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html#spatial-merging-using-voronoi-polygons",
    "href": "posts/austin-vehicle/spatial_merging.html#spatial-merging-using-voronoi-polygons",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "Spatial Merging using Voronoi Polygons",
    "text": "Spatial Merging using Voronoi Polygons\n\nVoronoi Polygons\nVoronoi polygons(or tessellations) are useful for a number of purposes. Given a set of points \\(j_n\\) we define a set of \\(n\\) regions where all spaces within each region has a single nearest neighbor of the initial point \\(i\\). Practically speaking, this just means we sub-divide a study area into smaller areas corresponding to the proximity to a point. This has many useful properties, such as determining nearest-neighbor distances from points to points. Let’s see how we can do this in R.\nTo start, we’ll first use a helper function to convert the Voronoi tessellation to an sf object that is suitable for merging. We’ll then merge the camera data to the polygon we just created (using st_intersection) and pull a few of the variables we’ll want for this example.\n\n\nCode\n# Helper function to simplify tessellation\n# borrowed from: \n# https://gis.stackexchange.com/questions/362134\nst_voronoi_point <- function(points){\n  ## points must be POINT geometry\n  # check for point geometry and execute if true\n  if(!all(st_geometry_type(points) == \"POINT\")){\n    stop(\"Input not  POINT geometries\")\n  }\n  g = st_combine(st_geometry(points)) # make multipoint\n  v = st_voronoi(g)\n  v = st_collection_extract(v)\n  return(v[unlist(st_intersects(points, v))])\n}\n\n\n# create Voronoi tessellation over cameras\ncamera_poly <- st_voronoi_point(camera) %>%\n  st_intersection(austin) %>%\n  st_as_sf() %>%\n  mutate(camera_id = camera$camera_id,\n         camera_X = camera$camera_X,\n         camera_Y = camera$camera_Y)\n\n\nNow we can plot the result. Below we see we now have a defined set of regions corresponding to the areas nearest to each camera. Therefore, any crashes that occur in one of the Voronoi polygons is also its nearest camera. This saves us the step of determining which point is its nearest neighbor.\n\n\nCode\nggplot() +\n  geom_sf(data = camera_poly, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nAll spaces within each Voronoi polygon are a nearest neighbor to a camera.\n\n\n\n\n\n\nSpatial Merging\nAfter we’ve created the Voronoi regions, all we need to do is merge each point to the region it falls within (which implies the camera there is its nearest neighbor) and then compute the euclidean distance from the crash to the camera. The code below uses a for-loop to get the pairwise distances after spatial joining and then limits the output to only crashes that are within 300 feet of the nearest camera.\n\n\nCode\n# JOIN AND MERGE\n# ----------------------- #\n\n# compute euclidean distance\nedist <- function(a,b){\n  sqrt(sum((a - b) ^ 2))\n}\n\n# get x-y coords for crashes and cameras\n# convert to matrix\ncamera_crash <-  st_join(crash_sf,camera_poly) %>%\n  tibble() %>%\n  select(camera_id, \n         crash_id, \n         camera_X, \n         camera_Y, \n         crash_X, \n         crash_Y)\n\ndmat <- matrix(c(camera_crash$camera_X, \n                 camera_crash$camera_Y, \n                 camera_crash$crash_X, \n                 camera_crash$crash_Y),\n               ncol = 4)\n\n# compute pairwise distances\ndlist <- list()\nfor(i in 1:nrow(dmat)){\n  dlist[[i]] <- edist(c(dmat[i,1], dmat[i,2]), c(dmat[i,3], dmat[i,4]))\n}\n\ncamera_crash$dist <- unlist(dlist)\n\n# get ids of within 300 meters\ndist_ids <- camera_crash$dist <= 300\n\n\nNow we can plot the results. As we see below we now only have crashes that are within 300 feet or less of the nearest camera. One advantage of this approach is that we can make any adjustments to the spatial region we’re interested in by just adjusting the filter above - or we can use the full range of distances in our analysis and look at decay effects (for example, the effect of CCTV cameras on crime clearance).\n\n\nCode\nggplot() +\n  geom_sf(data = austin, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = crash_sf[dist_ids,], color = '#BB5566', size = .6, alpha = .7) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nCar crashes within 300 meters of a traffic camera.\n\n\n\n\nWith this done, we can do any kind of further investigation. For example, which camera observed the greatest number of crashes? Here, the top-ranked camera is at a 4-way intersection leading to the highway. Also, due to its proximity to the highway, it’s very likely that our distance size (300 meters, or about 900 feet) is picking up accidents that are occurring on the highway below. Of course, this is just a demonstration of method of spatial merging, not an investigation into traffic accidents in Austin!\n\n\nCode\ncamera_crash %>%\n  filter(crash_id %in% crash_sf[dist_ids,]$crash_id) %>%\n  count(camera_id) %>%\n  arrange(desc(n)) %>%\n  slice(1:5)\n\n\n# A tibble: 5 × 2\n  camera_id     n\n  <chr>     <int>\n1 919          72\n2 674          59\n3 948          46\n4 155          44\n5 962          35"
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html#summary",
    "href": "posts/austin-vehicle/spatial_merging.html#summary",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "Summary",
    "text": "Summary\nThis little mini-blog highlighted some approaches that can be taken to perform a relatively common spatial procedure. Using Voronoi polygons we looked at how we can use them to easily calculate nearest-neighbor distances. These types of spatial approaches aren’t necessarily the sexiest topics, but I find they often help considerably with modelling pipelines down the road. Sometimes have a good foundation can help with further analysis later.\n\nAn Aside: An even (easier) method?\nOf course, another method is to simply use the a convenient function embedded in the sf library aptly named st_nearest_feature(). This takes two sf objects and returns the indexes of \\(y\\) that are nearest to \\(x\\). While the solution here is equivalent to the one above, it might not necessarily be available in your given software package. Also, while I have no testing to support this, I expect that this would likely be slow in case of many pairwise distances. The presence of the polygons helps avoid the unnecessary computation of distances between points that are not nearest neighbors.\n\n# get index of cameras nearest to each point\nidx <- st_nearest_feature(crash_sf, camera)\nid_dist <- st_distance(camera[idx,], crash_sf, by_element = TRUE)\n\nid_dist[1:5]\n\nUnits: [m]\n         1          2          3          4          5 \n  49.88593 1202.17589  784.61324 1412.67832  282.73844"
  },
  {
    "objectID": "posts/coffee/coffee.html#the-great-american-coffee-taste-test",
    "href": "posts/coffee/coffee.html#the-great-american-coffee-taste-test",
    "title": "The Great American Coffee Taste Test",
    "section": "The Great American Coffee Taste Test",
    "text": "The Great American Coffee Taste Test\nIn October I was lucky enough to participate in popular coffee YouTuber James Hoffman’s Great American Coffee Taste Test. In short, participants got 4 samples of coffee and were able to brew, taste, and rate them live. One the interesting parts of this was that the data was freely shared after the tasting was completed. As both a data and coffee nerd, I couldn’t resit a dive into this dataset.\nThe one we’re going to focus on is the unusual Coffee ‘D’. In contrast to the other coffees in the taste test, Coffee ‘D’ was a natural process. The difference between a washed coffee and natural coffee is:\n\nWashed coffee and natural process coffee differ primarily in their processing methods. Washed coffee involves removing the outer fruit from the coffee bean before drying, using water to ferment and wash away the pulp, resulting in a clean, bright flavor profile with pronounced acidity. In contrast, natural process coffee involves drying the whole coffee cherry intact, allowing the bean to ferment inside the fruit, imparting a fruitier, sometimes wine-like, sweetness with a heavier body due to prolonged contact with the fruit, often exhibiting complex, earthy, or fermented flavor notes. These distinct processes significantly influence the taste and characteristics of the final brew, offering a spectrum of flavors to coffee enthusiasts.\n\nThe tl;dr is that natural process coffees tend to have more fermented, fruity flavors that are prized by some consumers, but often disliked by others. This is the one we’re going to focus our attention on here.\n\nThe survey\nWhile there were numerous questions on the survey, my focus was primarily on the following:\n\nAge\nGender\nSelf-rated coffee expertise\n\nI categorized ages into groups (18-24, 25-34, 35-44, 45-54, and 55+), and gender into (Male, Female). The self-rated coffee expertise was on a scale from 1 to 10, with 1 representing “I’m a novice” and 10 representing “I’m a coffee expert.”\nInitially, we need to convert the survey data from a wide format to a long one. In the current data view, each person’s response is repeated four times (once for each coffee type), while their age, gender, and self-reported coffee expertise remain constant. This approach allows us to model responses more efficiently and retain information across different coffee types.\n\n\n\n\n \n  \n    age \n    gender \n    expertise \n    coffee \n    ranking \n  \n \n\n  \n    18-24 \n    Other (please specify) \n    10 \n    pref_a \n    1 \n  \n  \n    18-24 \n    Other (please specify) \n    10 \n    pref_b \n    1 \n  \n  \n    18-24 \n    Other (please specify) \n    10 \n    pref_c \n    1 \n  \n  \n    18-24 \n    Other (please specify) \n    10 \n    pref_d \n    1 \n  \n  \n    55+ \n    Not Provided \n    7 \n    pref_a \n    3 \n  \n  \n    55+ \n    Not Provided \n    7 \n    pref_b \n    3 \n  \n  \n    55+ \n    Not Provided \n    7 \n    pref_c \n    3 \n  \n  \n    55+ \n    Not Provided \n    7 \n    pref_d \n    3 \n  \n  \n    25-34 \n    Female \n    6 \n    pref_a \n    3 \n  \n  \n    25-34 \n    Female \n    6 \n    pref_b \n    3"
  },
  {
    "objectID": "posts/coffee/coffee.html#fitting-a-bayesian-model",
    "href": "posts/coffee/coffee.html#fitting-a-bayesian-model",
    "title": "The Great American Coffee Taste Test",
    "section": "Fitting A Bayesian Model",
    "text": "Fitting A Bayesian Model\nWe’re employing an ordinal regression model with a cumulative link function, which is a typical method for analyzing Likert-style data. Gender remains constant across all coffee categories, while we permit the effects of age and expertise to differ for each type of coffee. Essentially, this suggests that we assume broader gender differences for all coffees, while acknowledging that the effects of age and expertise may differ across various coffee types.\n\n# set reasonable priors\nprior <- c(prior(normal(0,2), class = Intercept),\n           prior(normal(0,2), class = b),\n           prior(normal(0,2), class = sd))\n\n# hlm with varying slopes for expertise\n# and varying intercepts for age\nfit2 <-\n  brm(\n    ranking ~ 1 + \n      gender +\n      (age + expertise | coffee),\n    data = coffee_ranking,\n    prior = prior,\n    family = cumulative(\"probit\"),\n    chains = 4,\n    cores = 4,\n    iter = 2000,\n    control = list(adapt_delta = 0.9)\n  )\n\nThere are a few divergent transitions after fitting, but that can be fixed by upping the adapt_delta parameter. In general I’m satisfied with the fit, as all of our Rhat values are equal to 1.00, and the Bulk_ESS and Tail_ESS look fine too.\nAfter fitting the model, we can get some initial insights by plotting out some of the model coefficients. Specifically, we probably want to focus our attention on the varying effects for age and expertise on coffee preferences. We can do this by extracting the random effects from the model and plotting them. Here, we can see the estimated effect of age on preferences for each coffee (relative to the 18-24 group).\n\n\n\n\n\nEffect of age on coffee preferences. Older individuals tend to rate coffee D lower.\n\n\n\n\n…and the effect of self-reported expertise:\n\n\n\n\n\nEffect of expertise on coffee preferences. Individuals with higher expertise tend to rate coffee ‘A’ and ‘D’ higher.\n\n\n\n\nFrom this we see results that largely fit with what was reported in the video. Older people tend to dislike Coffee ‘A’ and ‘D’ more, and people with higher expertise tend to like them more.\n\nDigging a little deeper: Males vs. Females\nLet’s start by looking at all the predicted rankings for coffee ‘D’. In this case we have 500 different predictions, corresponding to all possible age x gender x expertise x ranking combinations. Each individual line represents the estimated ranking for a particular consumer.\n\n\n\n\n\nPredictions for all consumer groups. Each line represents a single age, gender, and expertise combination.\n\n\n\n\nUsing this data we can do some simple comparisons. For example: what is the estimated difference between males and females ranking coffee ‘D’? In his summary, James Hoffman highlighted that females were much more likely to strongly dislike coffee ‘D’ relative to males. However, if you look at the data, females in the survey also generally reported much less expertise in coffee.\nSo what if we control for expertise, and see what the estimated difference in gender is assuming they are otherwise comparable coffee consumers. Below, we set the expertise for predictions to the median, which is 6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Coffee rankings for males and females, across all age groups\n\n\nAnd here are the estimates for males and females, holding age constant at 25-34 and self-reported expertise to the median (6). As we can see both males and females with similar expertise within the same age groups largely rate coffee D similarly, with the largest difference being for ranking it a 5.\n\n\n\n\n\nEffect of gender on preferences for coffee ‘D’ holding age constant to 25-34.\n\n\n\n\nSo really, there aren’t very large difference in regards to gender after we control for expertise. The large gender difference that James sees in the survey is likely mostly an artifact of differences in experience with coffee between males and females1.\n\n\n\n\n\nFemale respondents were less likely to report high expertise with coffee, relative to males.\n\n\n\n\nIn fact, if we look at the differences in self-reported expertise we have estimated rankings for a male, aged 25-34 for self-reported expertise at 1, 5, and 10. As is fairly clear, the distribution of estimated rankings is substantially different across levels of expertise. For example, we estimate about an 80% probability that a person with a self-assessed expertise of 10 to rate coffee ‘D’ a 4 or 5. In contrast that drops to about 45% for a person with an expertise of 5, and 20% for a person with an expertise of 1.\n\n\n\n\n\nEffect of expertise on preferences for coffee ‘D’ holding age and gender constant to 25-34 and male. Persons with higher coffee expertise are more likely to rate coffee ‘D’ higher.\n\n\n\n\nIf it’s not already clear, the biggest differences in rating the unusual natural-process coffee is not really related to gender, but rather it is mostly based on an individuals expertise or ‘expertise’ with coffee."
  },
  {
    "objectID": "posts/coffee/coffee.html#comparing-two-hypothetical-consumers",
    "href": "posts/coffee/coffee.html#comparing-two-hypothetical-consumers",
    "title": "The Great American Coffee Taste Test",
    "section": "Comparing two hypothetical consumers",
    "text": "Comparing two hypothetical consumers\nSo now that we have our model, we can use it to pose any number of comparisons by relying on posterior draws.\nLet’s first look at the most common age-gender-expertise combinations. Below we see that males largely make up the most common individuals who completed the survey. From a hypothetical perspective, let’s compare how the most common male respondent (between 25-34 years old with an expertise of 7) would rate a given coffee compared to the most common female respondent (25-34 with an expertise of 5).\n\n\nCode\ncoffee_data %>%\n  count(age,gender,expertise) %>%\n  arrange(desc(n)) %>%\n  slice(1:10) %>%\n  kable()\n\n\n\n\n \n  \n    age \n    gender \n    expertise \n    n \n  \n \n\n  \n    25-34 years old \n    Male \n    7 \n    383 \n  \n  \n    25-34 years old \n    Male \n    6 \n    305 \n  \n  \n    25-34 years old \n    Male \n    8 \n    199 \n  \n  \n    35-44 years old \n    Male \n    7 \n    168 \n  \n  \n    25-34 years old \n    Male \n    5 \n    159 \n  \n  \n    35-44 years old \n    Male \n    6 \n    159 \n  \n  \n    35-44 years old \n    Male \n    8 \n    114 \n  \n  \n    25-34 years old \n    Female \n    5 \n    84 \n  \n  \n    25-34 years old \n    Male \n    4 \n    81 \n  \n  \n    25-34 years old \n    Female \n    6 \n    80 \n  \n\n\n\n\n\nFirst we need to compute 4000 posterior draws for each hypothetical user. These will be probabilities for our hypothetical person scoring a given coffee a 1 though a 5. We can get this by calling the posterior_epred function.\n\n# get 4000 posterior draws for two hypothetical individuals\n# for scoring a given coffee\n\n# helper function for generating predictions\nmake_comparisons <-\n  function(fit,\n           pred_coffee,\n           pred_exp,\n           pred_age,\n           pred_gender) {\n    newdata <-\n      data.frame(\n        expertise = pred_exp,\n        age = pred_age,\n        gender = pred_gender,\n        coffee = pred_coffee\n      )\n    \n    return(posterior_epred(fit2, newdata = newdata))\n  }\n\n# get posterior draws\nZ <-\n  make_comparisons(\n    fit2,\n    pred_coffee = c(\"pref_d\"),\n    pred_exp = c(7, 5),\n    pred_age = \"25-34\",\n    pred_gender = c(\"Male\", \"Female\")\n  )\n\n# matrix of estimated differences\n# p1 - p2\np_diff <- Z[, 1, 1:5] - Z[, 2, 1:5]\n\nThen we can put them into a dataframe and plot their distributions. As we can see, they are quite far apart. A 25-34 year old male with an expertise of 7 has a probability of about 35% of scoring coffee ‘D’ a 5, compared to 19% for female with an expertise of 5.\n\n\n\n\n\nOne nice thing about a Bayesian approach is that we have access to the full posterior, so we can compute any kind of comparisons. For example, what is the predicted median difference between these two individuals rating coffee ‘D’ a 5, with an 89% credible interval?\n\nquantile(p_diff[,5], probs = c(.06, .5, .94))\n\n       6%       50%       94% \n0.1386954 0.1519386 0.1651294 \n\n\nOr we can plot the estimated difference of males and females for each of the response categories:\n\n\n\n\n\nOr what if we had two consumers with similarly rated expertise, but one was much older?\n\nZ2 <-\n  make_comparisons(\n    fit2,\n    pred_coffee = \"pref_d\",\n    pred_exp = 6,\n    pred_age = c(\"25-34\", \"55+\"),\n    pred_gender = \"Male\"\n  )\n\np_diff_2 <- Z2[, 1, 1:5] - Z2[, 2, 1:5]\n\nquantile(p_diff_2[,5], probs = c(.06, .5, .94))\n\n       6%       50%       94% \n0.1952684 0.2140120 0.2323127 \n\n\nWhich suggests that the probability of a 25-34 year old rating coffee ‘D’ a 5 is about 21 percentage points higher than a 55+ year old individual. That’s a huge age difference!"
  },
  {
    "objectID": "posts/coffee/coffee.html#full-data",
    "href": "posts/coffee/coffee.html#full-data",
    "title": "The Great American Coffee Taste Test",
    "section": "Full Data",
    "text": "Full Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(broom.mixed)\n\n# load survey data here\ncoffee <- read_csv(\"/gatt.csv\")\n\ncol_pal <- c( '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377')\n\nset.seed(123)\n\n## Function to extract random effects from brms model\npull_ranef <- function(x, idx){\n  return(\n    data.frame(x[,,idx]) %>%\n  mutate(coffee = rownames(.), variable = idx) \n  )\n}\n\n\n# Setup data\ncoffee_data <-\n  coffee %>%\n  select(\n    age = `What is your age?`,\n    gender = `Gender`,\n    expertise = `Lastly, how would you rate your own coffee expertise?`,\n    pref_a = `Coffee A - Personal Preference`,\n    pref_b = `Coffee B - Personal Preference`,\n    pref_c = `Coffee C - Personal Preference`,\n    pref_d = `Coffee D - Personal Preference`\n  ) %>%\n  replace_na(\n    list(\n      age = 'Not Provided',\n      gender = 'Not Provided',\n      race = 'Not Provided',\n      education = 'Not Provided'\n    )\n  )\n\ncoffee_ranking <-\n  coffee_data %>%\n  na.omit() %>%\n  select(age, gender, expertise, pref_a:pref_d) %>%\n  pivot_longer(cols = starts_with(\"pref\"),\n               names_to = \"coffee\",\n               values_to = \"ranking\") %>%\n  mutate(age = case_when(\n    age %in% c(\"<18 years old\",\"18-24 years old\") ~ \"18-24\",\n    age == \"25-34 years old\" ~ \"25-34\",\n    age == \"35-44 years old\" ~ \"35-44\",\n    age == \"45-54 years old\" ~ \"45-54\",\n    age %in% c(\"55-64 years old\", \">65 years old\") ~ \"55+\"\n  ))\n\n# fit model\nfit2 <-\n  brm(\n    ranking ~ 1 + \n      gender +\n      (age + expertise | coffee),\n    data = coffee_ranking,\n    prior = prior,\n    family = cumulative(\"probit\"),\n    chains = 4,\n    cores = 4,\n    iter = 2000,\n    control = list(adapt_delta = 0.99)\n  )\n\nstrata <- coffee_ranking %>%\n  filter(gender %in% c(\"Male\",\"Female\")) %>%\n  distinct(expertise, age, gender, coffee) %>%\n  complete(expertise,age,gender,coffee)\n\nfit1_preds <-\n  predict(fit2, newdata = strata) %>%\n  data.frame()\n\npred_data <-\n  tibble(strata, fit1_preds) %>%\n  set_names(c(\n    'expertise',\n    'age',\n    'gender',\n    'coffee',\n    'p1',\n    'p2',\n    'p3',\n    'p4',\n    'p5'\n  )) %>%\n  mutate(\n    gender = fct_relevel(gender, \"Male\"),\n    age = fct_relevel(\n      age,\n      \"18-24\",\n      \"25-34\",\n      \"35-44\",\n      \"45-54\",\n      '55+'\n    )\n  ) %>%\n  pivot_longer(cols = starts_with(\"p\"),\n               names_to = 'ranking',\n               values_to = 'prob')\n\n# random effects\nfit2_summary <- ranef(fit2)[[1]]\nvals <- c(\"Intercept\",\"age25M34\",\"age35M44\",\"age45M54\",\"age55P\",\"expertise\")\n\n# pull into nice dataframe\nres <-\n  sapply(vals, function(x) {\n    pull_ranef(fit2_summary, x)\n  },\n  simplify = FALSE) %>%\n  do.call(rbind, .)"
  },
  {
    "objectID": "posts/distance-plotting/distance-plotting.html",
    "href": "posts/distance-plotting/distance-plotting.html",
    "title": "How to Draw Lines Between Pairs of Points in R",
    "section": "",
    "text": "Here’s a quick one. I was recently asked how you might plot the travel of individuals over time on a map. For example, if you had longitudinal data recording the residences of respondents over a course of many years, it might be interesting to see to where and how far they traveled. Doing this in R isn’t too difficult, but it isn’t quite straightforward either. Below I’ll show off my approach using the sf package.\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\nset.seed(111424)\n\n# load data\n# cities data: https://simplemaps.com/data/us-cities\n# usa states: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip\n\ncities <- read_csv(\"./data/uscities.csv\")\nusa <- st_read(\"./data/usa_states/cb_2018_us_state_500k.shp\")\n\n# create a base layer map\nusa_map <-\n  usa %>%\n  filter(NAME %in% state.name,!NAME %in% c(\"Alaska\", \"Hawaii\")) %>%\n  st_transform(crs = 4326)\n\n# subset 50 largest us cities\ncities_sub <- cities %>%\n  filter(state_name %in% state.name,!state_name %in% c(\"Alaska\", \"Hawaii\")) %>%\n  slice(1:50)\n\n\nLet’s say we have some data which lists the name of a person, the cities they’ve been to, and the dates they moved. We want to create a plot that draws a line (in order) of their travel between cities. A sample dataset might look something like this below. We have a person identifier and a sequence of dates that display the dates they lived in a location, along with the associated latitude and longitude.\n\n\nCode\nN <- sample(1:length(cities_sub), 3)\n\n# sample of data\nsample_d <-\n  cities_sub %>%\n  slice(N) %>%\n  mutate(person_id = 'a12345',\n         from_date = as.Date(c('2016-12-31',\n                               '2018-04-07',\n                               '2024-03-03'))) %>%\n  select(person_id, city, from_date, lat, lng)\n\nkable(sample_d)\n\n\n\n\n \n  \n    person_id \n    city \n    from_date \n    lat \n    lng \n  \n \n\n  \n    a12345 \n    Seattle \n    2016-12-31 \n    47.6211 \n    -122.3244 \n  \n  \n    a12345 \n    Houston \n    2018-04-07 \n    29.7860 \n    -95.3885 \n  \n  \n    a12345 \n    Phoenix \n    2024-03-03 \n    33.5722 \n    -112.0892 \n  \n\n\n\n\n\nNow what we want to do is find a way to plot these as a linestring on a map. To do this we can create a simple function that will take this dataframe as input, and assume that for each sequence of points they are ordered from oldest to newest. The function will then extract the points and create an st_linestring object that links them together. Because sf objects interface well with ggplot you can easily make a direct call to plot ontop of a base map.\n\n\nCode\n# function to iterate through n number of points\n\n# given some input distance data 'dd'\n# function expects to see a lng, lat\n# and rows sorted by sequence\n\ndistance_linestring <- function(dd){\n  points_list <- list()\n  idx = 1\n  for(i in 1:nrow(dd)){\n    points_list[[idx]] <- st_point(c(dd$lng[idx],dd$lat[idx]))\n    idx = idx+1\n  }\n  ls = st_linestring(do.call(rbind, points_list)) %>% st_sfc(crs = 4326)\n  \n  return(ls)\n}\n\n# let's draw a line between three random cities\nd1 = distance_linestring(sample_d)\n\n\nAfter calling our distance_linestring function we take the output d1 and plot it on our basemap.\n\n\nCode\n# set up base map\nbase_map <- ggplot() + \n  geom_sf(data = usa_map, fill = '#FFFFFF', color = '#BBBBBB')\n\nbase_map +\n  geom_sf(data = d1, color = '#BB5566', linewidth = 1) +\n  theme_void()\n\n\n\n\n\nAnd there we go! A single journey.\n\n\nA more common example might ask us to visualize patterns that many people take - for instance, all participants of a longitudinal survey. We can easily extend the function defined above and wrap it in a for-loop. To illustrate what this looks like I simulate some data for 100 theoretical trips between 2 and 5 cities:\n\n\nCode\n# OK, let's simulate 100 people travelling up to 5 cities\n# then we store the results in a list and plot them on a base map\nlinestring_list <- list()\niter = 100\nmax_N = 5\n\nfor(i in 1:iter){\n k <- sample(2:max_N,1)\n N <- sample(1:length(cities_sub),k)\n \n linestring_list[[i]] <- distance_linestring(cities_sub[N,])\n}\n\n\n# reset basemap\nbase_map <- ggplot() + \n  geom_sf(data = usa_map, fill = '#FFFFFF', color = '#BBBBBB')\n\n# iterate through the list of locations and add each to the plot\nfor(p in 1:length(linestring_list)){\n  base_map = base_map + geom_sf(data = linestring_list[[p]], color = '#BB5566', linewidth = 1, alpha = .2)\n}\n\n\nSo we just simulate a lot of journeys that go between 2 and 5 states, store them in a list, then run our linestring_list function over it. The for-loop to add lines is a bit hack-y, but it works. We can then just plot them out:\n\n\nCode\nbase_map +\n  theme_void()\n\n\n\n\n\nAnd if we want to know how far, on average, each person traveled, we can just compute the sum of distances across our list. Simple!\n\n\nCode\n# distance in meters\ndists_m <- sapply(linestring_list, st_length)\n\nhist(dists_m/1609, xlab = \"Distance in Miles\", main = \"Miles Travelled\")"
  },
  {
    "objectID": "posts/distance-plotting/distance-plotting.html#full-data",
    "href": "posts/distance-plotting/distance-plotting.html#full-data",
    "title": "How to Draw Lines Between Pairs of Points in R",
    "section": "Full Data",
    "text": "Full Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\nset.seed(111424)\n\n# load data\n# cities data: https://simplemaps.com/data/us-cities\n# usa states: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip\n\ncities <- read_csv(\"./data/uscities.csv\")\nusa <- st_read(\"./data/usa_states/cb_2018_us_state_500k.shp\")\n\n# create a base layer map\nusa_map <-\n  usa %>%\n  filter(NAME %in% state.name,!NAME %in% c(\"Alaska\", \"Hawaii\")) %>%\n  st_transform(crs = 4326)\n\n# subset 50 largest us cities\ncities_sub <- cities %>%\n  filter(state_name %in% state.name,!state_name %in% c(\"Alaska\", \"Hawaii\")) %>%\n  slice(1:50)\n\n# function to iterate through n number of points\n\n# given some input distance data 'dd'\n# function expects to see a lng, lat\n# and rows sorted by sequence\n\ndistance_linestring <- function(dd){\n  points_list <- list()\n  idx = 1\n  for(i in 1:nrow(dd)){\n    points_list[[idx]] <- st_point(c(dd$lng[idx],dd$lat[idx]))\n    idx = idx+1\n  }\n  ls = st_linestring(do.call(rbind, points_list)) %>% st_sfc(crs = 4326)\n  \n  return(ls)\n}\n\n# let's draw a line between three random cities\nd1 = distance_linestring(sample_d)\n\n# set up base map\nbase_map <- ggplot() + \n  geom_sf(data = usa_map, fill = '#FFFFFF', color = '#BBBBBB')\n\nbase_map +\n  geom_sf(data = d1, color = '#BB5566', linewidth = 1) +\n  theme_void()\n\n# OK, let's simulate 100 people travelling up to 5 cities\n# then we store the results in a list and plot them on a base map\nlinestring_list <- list()\niter = 100\nmax_N = 5\n\nfor(i in 1:iter){\n k <- sample(2:max_N,1)\n N <- sample(1:length(cities_sub),k)\n \n linestring_list[[i]] <- distance_linestring(cities_sub[N,])\n}\n\n\n# reset basemap\nbase_map <- ggplot() + \n  geom_sf(data = usa_map, fill = '#FFFFFF', color = '#BBBBBB')\n\n# iterate through the list of locations and add each to the plot\nfor(p in 1:length(linestring_list)){\n  base_map = base_map + geom_sf(data = linestring_list[[p]], color = '#BB5566', linewidth = 1, alpha = .2)\n}\n\nbase_map +\n  theme_void()\n\n# distance in meters\ndists_m <- sapply(linestring_list, st_length)\n\nhist(dists_m/1609, xlab = \"Distance in Miles\", main = \"Miles Travelled\")"
  },
  {
    "objectID": "posts/gmm/gmm.html",
    "href": "posts/gmm/gmm.html",
    "title": "Good Mythical Morning: Blind Fast Food Taste Tests",
    "section": "",
    "text": "So, its no secret that my wife and I are fans of Rhett and Link’s YouTube series Good Mythical Morning. Specifically, we are fans of the large variety of food- related content like Will it Burger?, Worst Halloween Candy Taste Test, International Burger King Taste Test, and 100 Years of Party Snacks Taste Test.\nWhile normally this would qualify as pretty generic YouTube content, Good Mythical Morning keeps it interesting by “gamifying” many of these segments. These including making the hosts try gross food mash-ups, throw darts at a map to guess the food’s country of origin, or use a shuffleboard to guess the decade the food originated from.\nOne of the games that is common on the channel is the Blind Taste Test where Rhett and Link are presented with food items from different fast food or restaurant chains, and are tasked with guessing which items come from which location. For example, in the Blind Chicken Nugget Taste Test they are given nuggets from 6 locations (McDonalds, Wendys, Chic-Fil-A, Hardees, KFC, and Frozen Tyson Nuggets) and have to try and place them based on taste alone. It’s silly content, but fun."
  },
  {
    "objectID": "posts/gmm/gmm.html#how-good-are-they",
    "href": "posts/gmm/gmm.html#how-good-are-they",
    "title": "Good Mythical Morning: Blind Fast Food Taste Tests",
    "section": "How Good Are They?",
    "text": "How Good Are They?\nOne nagging question I’ve always had is whether their performance on these segments were any better than if they just randomly guessed each time. Blind taste-testing is actually very hard, and distinguishing between 6 different pieces of mass-produced fried meat is probably even harder. Indeed, their performance on a lot of these segments is actually not great. To answer this, I gathered data from all the blind fast food taste tests that I could find, and collected it into a Google Sheet. Most of the games have 6 options, but a few have 5. For simplicity I will focus on the 26 segments where there were six choices available.\n\nChecking Out the Data\n\n\nCode\nlibrary(googlesheets4)\nlibrary(tidyverse)\n\nset.seed(7132322)\n\n# Load Data\n# ---------------\n\ngsheet <-\n  \"https://docs.google.com/spreadsheets/d/1P_LIZnnnaoCZHwmmrBM3O26sPAGCekVj_3qmWyyxTpc/edit?usp=sharing\"\n\ntaste_test <- read_sheet(ss = gsheet) %>%\n  filter(options == 6)\n\n\nThe sheet here has columns corresponding to each game played food, with Rhett and Link’s number of correct guesses stored in their own columns.\n\n\nCode\nhead(taste_test)\n\n\n# A tibble: 6 × 4\n  food          options rhett  link\n  <chr>           <dbl> <dbl> <dbl>\n1 fries               6     4     3\n2 nuggets             6     6     3\n3 fried chicken       6     0     2\n4 bbq pork            6     2     3\n5 donut               6     0     1\n6 taco                6     2     2\n\n\nFirst, let’s look at the distribution of correct guesses for Rhett and Link. We can generate just a simple bar chart here:\n\n\nCode\n# Plot R v L\n# ---------------\n\n# plot observed correct answers for R&L\npar(mfrow = c(1,2))\nbarplot(prop.table(table(taste_test$rhett)), \n        main = \"Rhett\", \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$rhett),3)),\n        col = '#e6862c')\nbarplot(prop.table(table(taste_test$link)), \n        col = '#4ec726', \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$link),3)),\n        main = \"Link\")\n\n\n\n\n\nStrangely enough, based on the 26 games that I collected, Rhett and Link have the exact same number of correct guesses. We can see, however, that their distributions are quite different (Rhett has a single game with all 6 right, and several with 4 correct, but also many more games with none correct). As expected, across all games we see their average is about 1.9 correct answers. Now, recall, our question here is “is Rhett and Link’s performance better than chance alone?”\n\n\nRandom Guessing With Replacement\nSo if we want to compare their performance to “chance” - that is, totally random guessing - we need a probability distribution. Here, the binomial is logical starting point. Let’s start with the simplest assumption of randomly guessing one of the six options with replacement (meaning you can guess the same option twice). While this doesn’t necessarily seem like an ideal strategy, Rhett and Link often do guess the same location two or three times.\nWe can use the probability density function for the binomial distribution to calculate the probability of guessing between 0 and 6 items correctly, where each trial is assumed to be independent. Below, we see that there is about a 67% probability of guessing at least one of the items correctly, and a 40% chance of getting exactly one correct. Getting more than 4 correct is very rare. If we look at the observed results above, we see this mostly matches up. Both Rhett and Link rarely get more that 3 correct.\n\n\nCode\n# totally random (w replacement)\nround(dbinom(0:6, 6, 1/6), 3)\n\n\n[1] 0.335 0.402 0.201 0.054 0.008 0.001 0.000\n\n\n\n\nRandom Guessing Without Replacement\nNow, another strategy might be randomly guess, but not repeat any previous guesses. Instead of the guesses being independent, they are now dependent on prior guesses. That means we need to use a different method to calculate this. The hypergeometric distribution is commonly used to calculate this (e.g. dhyper()). This also falls within the realm of permutation-based math ala derangements. However, I am a simple man with a good computer and a less-good maths background, so I will simply simulate it.\n\n\nCode\n# totally random (w/o replacement)\n# simulate\nn = 1e5\n\nalist <- vector(mode = \"list\", length = n)\nfor(j in 1:n){\n  truth <- sample(1:6, 6)\n  guess <- sample(1:6, 6)\n  \n  alist[j] <- sum(guess == truth)\n  \n}\n\nres <- sapply(alist, sum)\nprop.table(table(res))\n\n\nres\n      0       1       2       3       4       6 \n0.37032 0.36459 0.18739 0.05538 0.02081 0.00151 \n\n\nHere we see that if we limit our guesses to only options not previously guessed, there is a roughly equal probability of getting between 0 and 1 answers correct, and about a 28% chance of getting more than 1 correct. Note, under this strategy it is not possible to get 5 answers correct (because if you incorrectly order at least one item, by definition at least one other item is also incorrectly ordered)"
  },
  {
    "objectID": "posts/gmm/gmm.html#are-rhett-and-link-better-than-random-guessing",
    "href": "posts/gmm/gmm.html#are-rhett-and-link-better-than-random-guessing",
    "title": "Good Mythical Morning: Blind Fast Food Taste Tests",
    "section": "Are Rhett and Link Better than Random Guessing?",
    "text": "Are Rhett and Link Better than Random Guessing?\nLet’s review our results here. Here’s what random guessing looks like under our two scenarios:\n\n\nCode\n# plot two different 'null' distributions\npar(mfrow = c(1, 2))\nbarplot(prop.table(round(dbinom(0:6, 6, 1 / 6), 3)),\n        names.arg = 0:6, \n        main = \"With Replacement\", \n        col = \"#dbd895\")\n\nbarplot(prop.table(table(res)), \n        main = \"Without Replacement\",\n        col = \"#dbd895\")\n\n\n\n\n\nAnd here’s what Rhett and Link’s actual results look like:\n\n\nCode\n# plot observed correct answers for R&L\npar(mfrow = c(1,2))\nbarplot(prop.table(table(taste_test$rhett)), \n        main = \"Rhett\", \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$rhett),3)),\n        col = '#e6862c')\nbarplot(prop.table(table(taste_test$link)), \n        col = '#4ec726', \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$link),3)),\n        main = \"Link\")\n\n\n\n\n\nThe expected value of random guessing in both scenarios is, approximately 1, meaning that Rhett and Link’s scores of 1.8 mean that, on average, they perform marginally better than if you guessed completely at random. This is on the order of about 1 additional item correct compared to just guessing 100% randomly.\n\n\nCode\n# expected value\nmean(rbinom(1e5, 6, 1/6))\n\n\n[1] 1.00159\n\n\nCode\nmean(res)\n\n\n[1] 0.99781\n\n\nSo, congrats Rhett and Link! You are a little bit better at blind tasting fast food than rolling a six-sided dice!"
  },
  {
    "objectID": "posts/hbos-anomaly/hbos_anomaly.html",
    "href": "posts/hbos-anomaly/hbos_anomaly.html",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "This is the third part of a 3-part series. In the first two posts I described how I built a principal components analysis anomaly detector and a k-nearest neighbors anomaly detector as components for a ensemble model. This third post will discuss the last piece, which is a histogram-based anomaly detector.\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector"
  },
  {
    "objectID": "posts/hbos-anomaly/hbos_anomaly.html#building-a-histogram-based-outlier-detector",
    "href": "posts/hbos-anomaly/hbos_anomaly.html#building-a-histogram-based-outlier-detector",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Building a Histogram-Based Outlier Detector",
    "text": "Building a Histogram-Based Outlier Detector\n\nDefining sub-space density\nThe core of the idea behind a histogram-based outlier detector is that it is a method to efficiently explore subspaces of the data by binning observations into discrete groups, then weighting each bin inversely by the number of observations (more on this in a moment). To start, we can provide a quick example showing how we can use histograms to partition the data into bins. Below, I create two histograms for the features representing stay length and average cost per-stay.\n\n\nCode\n# define breaks\nh1 <- hist(df$stay_len, breaks = 5, plot = FALSE)\nh2 <- hist(df$cost_per_stay, breaks = 5, plot = FALSE)\n\n# append to dataframe\nhdf <- df %>%\n  mutate(d1 = findInterval(stay_len, h1$breaks),\n         d2 = findInterval(cost_per_stay, h2$breaks),\n         space = paste0(d1,\"-\",d2))\n\n# Create a data frame with a grid of values for the predictor variables\ngrid <- expand.grid(stay_len = seq(min(hdf$stay_len), max(hdf$stay_len), length.out = 10),\n                    cost_per_stay = seq(min(hdf$cost_per_stay), max(hdf$cost_per_stay), length.out = 10)) %>%\n  mutate(d1 = findInterval(stay_len, h1$breaks),\n         d2 = findInterval(cost_per_stay, h2$breaks),\n         space = paste0(d1,\"-\",d2)) %>%\n    mutate(space = ifelse(space %in% hdf$space, space, NA)) %>%\n    fill(space)\n\n\nIf we plot each of these histograms, we can observe that most values concentrate in a few bins, while a small number of values are in more sparsely-populated bins. Obviously this shows us that the majority of stay lengths are between 0-10 days, and the average cost per-stay is around $4,000.\n\n\nCode\npar(mfrow=c(1,2))\nplot(h1, main = \"Stay Length\", xlab = \"Days\", col = '#004488', border = 'white')\nplot(h2, main = \"Cost Per Stay\", xlab = \"Cost\", col = '#004488', border = 'white')\n\n\n\n\n\nA histogram’s bins are proportional to the number of observations.\n\n\n\n\nWe can plot this in 2 dimensions to see how the feature space distribution is subdivided based on histogram bins. As we would expect, the majority of observations fall into a few regions, while potential outliers exist in much more sparsely populated bins. This is actually fairly similar to a decision tree, where we classify observations based on a set of rules. For example, the lone observation on the far right of the plot is in a region where stay_length >= 44 and cost_per_stay >= 3367 and cost_per_stay <= 4837.\n\n\nCode\nggplot() +\n  geom_tile(data = grid, aes(x = stay_len, y = cost_per_stay, fill = space), alpha = .6) +\n  geom_point(data = df, aes(x = stay_len, y = cost_per_stay), color = '#004488') +\n  labs(x = 'Stay Length', y = 'Cost Per Stay') +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n2D histogram partitioning of observations. Colored regions represent different bin partitions. Note the sparse regions at the top left and lower right quadrants.\n\n\n\n\n\n\nScoring observations\nWith this in mind, we are essentially going to do the above, but in \\(d\\) dimensions (where \\(d\\) is the number of input features). To give each observation an anomaly score, we will follow a very simple scoring mechanism proposed by the original authors of the method where:\n\\[HBOS(p) = \\sum^d_{i=0}log(\\frac{1}{hist_i(p)})\\]\nWhich states that the histogram-based anomaly score is the sum of the log of inverse histogram densities. More simply, for each feature \\(d\\) we compute a histogram density, and each observation is scored based on the inverse of its bin density (Goldstein and Dengel 2012). This means observations in sparsely populated bins receive higher scores, and vice-versa. One of the trade-offs here is that we have to assume feature independence (which is a tenuous assumption in a lot of cases), but even violations of this might not be too bad.\n\n\nA brief aside: choosing the optimal bin size\nOne challenge with this approach is that before we calculate histogram densities we need to define the number of bins for our histograms ahead of time. Now, one simple method might just be to choose a very rough rule-of-thumb (e.g. the “Sturges” rule of \\(1+log2(N)\\)) or to just choose a constant number like 5 or 10. A more principled way, however, would be to derive the optimal number of bins based on some properties of the input data.\nThere are a lot of proposed options out here, but the one that makes a lot of sense to me (and, incidentally, is also used in the pyod implementation of this function) is to iteratively fit histograms, calculate a penalized maximum likelihood estimate for each histogram \\(D\\), and then select the number of bins corresponding to the maximum likelihood estimate (Birgé and Rozenholc 2006). A rough R implementation of this is shown below:\n\n  # internal function: compute optimal bins\nopt_bins <- function(X, upper_bound = 15)\n  {\n    \n    epsilon = 1\n    n <- length(X)\n    \n    # maximum likelihood estimate for bin\n    maximum_likelihood <- array(0, dim = c(upper_bound - 1, 1))\n    \n    # rule of thumb for upper bound\n    if (is.null(upper_bound)) {\n      upper_bound <- as.integer(sqrt(length(X)))\n    }\n    \n    for (i in seq_along(1:(upper_bound - 1))) {\n      b <- i + 1\n      histogram <- hist(X, breaks = b, plot = FALSE)$counts\n      \n      maximum_likelihood[i] <-\n        sum(histogram * log(b * histogram /  n + epsilon) - (b - 1 + log(b) ^ 2.5))\n    }\n    \n    return(which.max(maximum_likelihood))\n  }\n\nSo, running this for the first feature stay_len, we get:\n\nopt_bins(df$stay_len)\n\n[1] 5\n\n\n\n\nBuilding the detector\nWith the issue of histogram bins out of the way, we can procede with the rest of the model. The last bit is really quite simple. For each feature \\(d\\) we compute the optimal number of bins (using the function we just defined above). We then build a histogram for that feature and identify which points fall within each bin. We then score each point according to the formula above, which is the log of the inverse histogram density (making outlying observations have correspondingly higher anomaly scores). The last thing we do after running this algorithm over all \\(d\\) features is to scale their scores (here, I use min-max normalization) and sum them together. The code to do this is below:\n\n# run HBOS\n  for(d in 1:d){\n    \n    h <- hist(X[,d], breaks = opt_bins(X[,d]), plot = FALSE)\n    fi <- findInterval(X[,d], h$breaks)\n    \n    hbos[[d]] <- log(1/h$density[fi])\n  }\n  \n  # minmax scale feature vectors\n  hbos <- lapply(hbos, function(x){(x- min(x)) /(max(x)-min(x))})\n  \n  # return score\n  return(apply(do.call(cbind, hbos), 1, sum))"
  },
  {
    "objectID": "posts/hbos-anomaly/hbos_anomaly.html#running-the-model",
    "href": "posts/hbos-anomaly/hbos_anomaly.html#running-the-model",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Running the Model",
    "text": "Running the Model\nNow we’re ready to run everything. For simplicity, I wrap all this code into a single adHBOS function that contains the optimal histogram binning and the scoring (see: Section 3.1). For flagging anomalies we will just identify the highest 5% (a rough, but arguably acceptable heuristic).\n\nX <- df[,2:7]\n\ndf$anom <-  adHBOS(X)\ndf$flag <- ifelse(df$anom >= quantile(df$anom, .95),1,0)\n\nIf we look at a histogram of we see most scores are low, while the outliers are clearly visible on the right-hand side.\n\n\nCode\nggplot(df) +\n  geom_histogram(aes(x = anom), bins = 10, color = 'white', fill = '#004488')+\n  labs(x = \"Anomaly Score\", y = \"Observations\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nHistogram-based anomaly scores. More anomalous observations have higher scores\n\n\n\n\nComparing this to our earlier plot we see:\n\n\nCode\nggplot() +\n  geom_tile(data = grid, aes(x = stay_len, y = cost_per_stay, fill = space), alpha = .6) +\n  geom_point(data = df, aes(x = stay_len, y = cost_per_stay), color = '#004488') +\n  geom_point(data = df[df$flag == 1,], aes(x = stay_len, y = cost_per_stay, color = '#BB5566'), size = 2) +\n  labs(x = 'Stay Length', y = 'Cost Per Stay') +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\nPlotting inliers (blue) and outliers (red) in 2D space.\n\n\n\n\nAs we expect, most of the observations that are flagged as outliers reside in bins with few other observations. This is pretty consistent with the other two methods we used before (PCA and KNN anomaly detectors). One specific advantage of the HBOS method is that it is very fast for even large datasets. However, with higher levels of dimensionality it is very likely that the assumption of feature independence is tenuous at best. Other methods, like the isolation forest can often perform better in higher dimensions. However, the simplicity of the method makes it easy to explain, which can be a benefit in many cases!\n\nHBOS Anaomaly Detector: Example Function\nHere’s a minimal working example of the procedure above. As we build our ensemble, we’ll come back to this function later.\n\n\nCode\n# Run a principal components anomaly detector\nadHBOS <- function(X, ub = 15){\n  \n  # scale input features, define list to hold scores\n  X <- scale(X)\n  j <- dim(X)[2]\n  hbos <- vector(\"list\",j)\n  \n  # internal function: compute optimal bins\n  opt_bins <- function(X, upper_bound = ub)\n  {\n    \n    epsilon = 1\n    n <- length(X)\n    \n    # maximum likelihood estimate for bin\n    maximum_likelihood <- array(0, dim = c(upper_bound - 1, 1))\n    \n    # rule of thumb for upper bound\n    if (is.null(upper_bound)) {\n      upper_bound <- as.integer(sqrt(length(X)))\n    }\n    \n    for (i in seq_along(1:(upper_bound - 1))) {\n      b <- i + 1\n      histogram <- hist(X, breaks = b, plot = FALSE)$counts\n      \n      maximum_likelihood[i] <-\n        sum(histogram * log(b * histogram /  n + epsilon) - (b - 1 + log(b) ^ 2.5))\n    }\n    \n    return(which.max(maximum_likelihood))\n  }\n  \n  # run HBOS\n  for(j in 1:j){\n    \n    h <- hist(X[,j], breaks = opt_bins(X[,j]), plot = FALSE)\n    fi <- findInterval(X[,j], h$breaks)\n    \n    hbos[[j]] <- log(1/h$density[fi])\n  }\n  \n  # minmax scale feature vectors\n  hbos <- lapply(hbos, function(x){(x- min(x)) /(max(x)-min(x))})\n  \n  # return score\n  return(apply(do.call(cbind, hbos), 1, sum))\n  \n}"
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#injuries-and-low-base-counts",
    "href": "posts/hlm-osha/osha_shrinkage.html#injuries-and-low-base-counts",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Injuries and Low Base Counts",
    "text": "Injuries and Low Base Counts\nMy friend, Andy Wheeler, just recently posted on his blog about reported injuries at Amazon warehouses. As he rightly points out, the apparent high number of injuries at these warehouses is primarily a function of the size of the locations.\nIn criminology we often deal with similar issues (namely, why we use crime rates rather than raw counts when comparing geographies of different populations). While I don’t have much to add to Andy’s post, one thing did stand out to me - the issue of low base counts.\n\nBut note that I don’t think Bonded Logistics is a terribly dangerous place. One thing you need to watch out for when evaluating rate data is that places with smaller denominators (here lower total hours worked) tend to be more volatile.(“Injury Rates at Amazon Warehouses” 2022)\n\nThis is also a very common problem across many different disciplines. Andrew Gelman discusses the problem in this paper about issues arising from mapping county-level cancer rates. Similarly, he points out that very variable rates arise from very low sample sizes. For example: imagine a single murder occurs in the city of Union, CT. With a population of 854, that gives us a murder rate per 1,000 of \\(\\frac{1}{854} * 1,000 = 1.17\\). This would potentially make it one of the highest-rate small towns in the state! Logically this doesn’t make sense, because rare events can happen - but it doesn’t imply a single region is especially unusual.\n\n\n\nCounties with low population appear to have very high rates of kidney cancer. However, much of this is an illusion due to higher variance relative to higher population counties."
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#hierarchical-models",
    "href": "posts/hlm-osha/osha_shrinkage.html#hierarchical-models",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\nAll of this discussion made me think about some issues I had addressed when studying crime - namely rare events (homicides or shootings) that are aggregated to small areas (census blocks or street segments). In these previous examples I had applied hierarchical models to help adjust for these issues we commonly observe with rare events. Let’s work with the same data that Andy used in his example. First, we’ll load the OSHA data for 2021 and isolate just the warehouses in North Carolina.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(knitr)\n\n# load osha data\nosha <- read_csv(unzip(\"C:/Users/gioc4/Dropbox/blogs/hlm_osha/ITA-data-cy2021.zip\"))\n\n# isolate NC injuries at warehouses\ninj_wh <- osha %>%\n  filter(naics_code == '493110',\n         state == 'NC') %>%\n  mutate(inj_rate = (total_injuries/total_hours_worked)*2080)\n\n\nIf we plot the distribution of injury rates per-person work hour year we see that the majority of warehouses are quite low, and very few exceed 0.2. However on the far right we see a single extreme example - the outlier that is the Bonded Logistics warehouse.\n\n\nShow code\nggplot(inj_wh) +\n  geom_histogram(aes(x = inj_rate), \n                 fill = \"#004488\", \n                 color = \"white\",\n                 bins = 20,\n                 linewidth =1.5) +\n  labs(x = \"(total_injuries/total_hours_worked)*2080\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nSorting by the top 10 we see that Bonded Logistics has an injury rate nearly 4 times the next highest warehouse. But they also have only a single employee who worked 1,686 hours that year! Is this really a fair comparison? Following what we already know, almost certainly not.\n\n\nShow code\ninj_wh %>%\n  select(company_name, inj_rate, annual_average_employees, total_hours_worked) %>%\n  arrange(desc(inj_rate)) %>%\n  slice(1:10) %>%\n  kable(caption = \"Top 10 Warehouses, by Injury Rate\", \n        digits = 2,\n        col.names = c(\"Company\",\"Injury Rate\",\"Employees\",\"Total Hours Worked\"))\n\n\n\nTop 10 Warehouses, by Injury Rate\n\n\n\n\n\n\n\n\nCompany\nInjury Rate\nEmployees\nTotal Hours Worked\n\n\n\n\nBonded Logistics\n1.23\n1\n1686\n\n\nTechnimark\n0.34\n4\n6154\n\n\nBonded Logistics\n0.30\n4\n6913\n\n\nCAPSTONE LOGISTICS, LLC\n0.29\n36\n43137\n\n\nRH US LLC\n0.27\n7\n7703\n\n\nAldi (N.C.) L.L.C.\n0.22\n385\n569274\n\n\nBrame Specialty Company Inc\n0.17\n6\n12513\n\n\nConn Appliances Inc\n0.16\n15\n26634\n\n\nBlue Line Distribution\n0.16\n31\n53287\n\n\nTosca Services, LLC\n0.16\n41\n66891\n\n\n\n\n\nTo address this issue, we’ll fit a (very) simple Bayesian hierarchical linear model where we give each warehouse its own intercept. We then partially pool estimates from the model toward the group-level means. In short, we’ll model this as the number of injuries \\(y\\) at each warehouse \\(j\\) as a Poisson process, where each warehouse is modeled with its own (varying) intercept. In a minute we will see the advantage of this.\n\\[y_{j} \\sim Poisson(\\lambda_{j})\\] \\[ln(\\lambda{j}) = \\beta_{0j}\\]\nUsing brms we’ll fit a Poisson regression estimating the total number of injuries at any warehouse weighted by the logged number of hours worked. Because the model is extremely simple, we’ll just keep the default priors with this model which are student_t(3,0,3).\n\n\nCode\n# fit the hierarchical model w/ default priors\n\nfit <- brm(total_injuries ~ 1 + (1|id) + \n             offset(log(total_hours_worked)), \n           family = poisson(), \n           data = inj_wh,\n           file = \"C:/Users/gioc4/Dropbox/blogs/hlm_osha/brmfit\",\n           chains = 4, cores = 4, iter = 2000)\n\n\nAfter the model fits, it’s generally a good idea to make sure the predictions from the model correspond with the observed distribution of the data. Our posterior predictive checks show that we have fairly well captured the observed process, with our posterior simulations \\(\\hat{y}\\) largely in line with the observed \\(y\\).\n\n\nShow code\npp_check(fit, \"hist\") + theme_bw()\n\npp_check(fit, \"scatter_avg\") + theme_bw()"
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#shrinkage",
    "href": "posts/hlm-osha/osha_shrinkage.html#shrinkage",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Shrinkage!",
    "text": "Shrinkage!\nHere’s where things get interesting. One of the benefits of a hierarchical model is that estimates from the model are partially pooled (shrunk) toward the group-level means. In a typical no-pooling model, estimates from very sparse clusters can be extreme or even undefined. In our hierarchical example we are applying regularization to the estimates by trading higher bias for lower variance(Gelman et al. 1995). In a Bayesian framework our application of a prior distribution helps set a reasonable boundary for our model estimates.\nTo illustrate this, we can see the difference between the predicted (blue circles) and observed (empty circles) below. For warehouses with very few worked hours we see that the estimates are pulled strongly toward the global mean. For warehouses with more hours, however, there is considerably less shrinkage.\n\n\nShow code\n# predicted vs actual\ninj_wh_pred <- inj_wh %>%\n  select(id, company_name, inj_rate, annual_average_employees, total_hours_worked) %>%\n  mutate(yhat = predict(fit, type = 'response')[,1],\n         inj_rate_pred = (yhat/total_hours_worked) * 2080)\n\n# Plot all values\nggplot(inj_wh_pred, aes(x = total_hours_worked)) +\n  geom_hline(yintercept = mean(inj_wh_pred$inj_rate), linewidth = 1, color = \"#004488\", alpha = .7)+\n  geom_point(aes(y = inj_rate), shape = 21, size = 2)+\n  geom_point(aes(y = inj_rate_pred), shape = 21, fill = \"#004488\", size = 2, alpha = .5) +\n  labs(x = \"Total Hours Worked\", y = \"Total Injuries/Hours Worked\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nIf we constrain ourselves to the left-hand side of the plot we can view this even more clearly. The estimated value for the unusual Bonded Warehouse is 0.1 compared to the observed value of 1.23. While this estimate is farther off from the observed value, it is probably much more reasonable based on the observed values of other warehouses.\n\n\nShow code\ninj_wh_pred %>%\n  filter(total_hours_worked < 1e5) %>%\n  ggplot(aes(x = total_hours_worked)) +\n  geom_hline(yintercept = mean(inj_wh_pred$inj_rate), linewidth = 1, color = \"#004488\", alpha = .7)+\n  geom_point(aes(y = inj_rate), shape = 21, size = 2)+\n  geom_point(aes(y = inj_rate_pred), shape = 21, fill = \"#004488\", size = 2, alpha = .5) +\n  labs(x = \"Total Hours Worked\", y = \"Total Injuries/Hours Worked\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nIf we compare the predicted injury rates to the observed ones, we can see the differences in shrinkage as well. Larger warehouses have estimates quite close to the observed counts (like The Aldi warehouse which has a relatively high rate of injuries for its size).\n\n\nCode\ninj_wh_pred %>%\n  select(company_name, inj_rate, inj_rate_pred, annual_average_employees, total_hours_worked) %>%\n  arrange(desc(inj_rate)) %>%\n  slice(1:10) %>%\n  kable(caption = \"Top 10 Warehouses, by Injury Rate\", \n        digits = 2,\n        col.names = c(\"Company\",\"Injury Rate\",\"(Pred) Injury Rate\", \"Employees\",\"Total Hours Worked\"))\n\n\n\nTop 10 Warehouses, by Injury Rate\n\n\n\n\n\n\n\n\n\nCompany\nInjury Rate\n(Pred) Injury Rate\nEmployees\nTotal Hours Worked\n\n\n\n\nBonded Logistics\n1.23\n0.10\n1\n1686\n\n\nTechnimark\n0.34\n0.08\n4\n6154\n\n\nBonded Logistics\n0.30\n0.08\n4\n6913\n\n\nCAPSTONE LOGISTICS, LLC\n0.29\n0.19\n36\n43137\n\n\nRH US LLC\n0.27\n0.07\n7\n7703\n\n\nAldi (N.C.) L.L.C.\n0.22\n0.21\n385\n569274\n\n\nBrame Specialty Company Inc\n0.17\n0.07\n6\n12513\n\n\nConn Appliances Inc\n0.16\n0.08\n15\n26634\n\n\nBlue Line Distribution\n0.16\n0.10\n31\n53287\n\n\nTosca Services, LLC\n0.16\n0.11\n41\n66891"
  },
  {
    "objectID": "posts/nlp-falls/nlp_falls.html",
    "href": "posts/nlp-falls/nlp_falls.html",
    "title": "Using NLP To Classify Medical Falls",
    "section": "",
    "text": "There’s no question that natural language processing (NLP) facilitated by deep learning has exploded in popularity (much of which is popularized by the ChatGPT family of models). This is an exciting time to be involved in AI and machine learning. However, for the kinds of tasks I typically work on in my day job, a lot of the deep learning models don’t provide much benefit. In fact, for most tabular data problems, random forests + boosting tend to work incredibly well. Areas where deep learning excels, like unstructured text or image input, are not things I find myself working on. That being said, I am always sharpening my skills and dipping my toes into areas where I am least familiar.\nA huge advantage today, compared to even ten years ago, is the ecosystem of open data and pre-trained models. HuggingFace in particular has a lot of easily obtainable pre-trained models. Stuff like the Transformers library make it easy for a neophyte like me to hop in and start doing work without too much overhead."
  },
  {
    "objectID": "posts/nlp-falls/nlp_falls.html#predicting-elderly-falls-from-medical-narratives",
    "href": "posts/nlp-falls/nlp_falls.html#predicting-elderly-falls-from-medical-narratives",
    "title": "Using NLP To Classify Medical Falls",
    "section": "Predicting Elderly Falls from Medical Narratives",
    "text": "Predicting Elderly Falls from Medical Narratives\nFor this example I am going to rely on some data from DrivenData - an organization that hosts data competitions. The data here are verified fall events for adults aged 65+. This sample comes more broadly from the National Electronic Injury Survellience System(NEISS). This is useful because the sample of cases here are human-verified falls cases, in which case we have a source of truth. While you could probably get pretty far just doing some regex like str.match(\"FALL|FELL|SLIPPED\") but it would likely miss more subtle cases. This is where having something like a BERT model is useful.\nLet’s say we have a set of verified falls narratives (which we do) and we have a large set of miscellanous narratives that contain falls cases, as well as other injuries that are not falls. Our goal is to find narratives that are likely to be related to elderly fall cases. To do this, we will use the verified falls cases narratives from DataDriven as our “training data” so to speak, and we will use an NLP model to find cases that are semantically similar to these verified falls cases.\n\nData Setup\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport re\nfrom sentence_transformers import SentenceTransformer, util\n\nnp.random.seed(1)\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# load raw data\nfalls = pd.read_csv(\"../../../data/falls/falls.csv\")\nneis = pd.read_csv(\"../../../data/falls/neis.csv\")\n\n# process datetime\nfalls['treatment_date'] = pd.to_datetime(falls['treatment_date'])\n\nTo get set up we read in the verified falls narratives, as well as the full sample of NEIS cases from 2022. After reading in our data we can perform some minor data cleaning to the narratives. Specifically, because we want to isolate narrative characterics associated with falls we should exclude the leading information about the patient’s age and sex, as well as some other medical terminology. We can also remap some abbreviations to English and properly extract the actual age of the patient from the narrative.\n\n# define remappings of abbreviations\n# and strings to remove from narratives\n\nremap = {\n    \"FX\": \"FRACTURE\",\n    \"INJ\": \"INJURY\",\n    \"LAC\": \"LACERATION\",\n    \"CONT\": \"CONTUSION\",\n    \"CHI\" : \"CLOSED HEAD INJURY\",\n    \"ETOH\": \"ALCOHOL\",\n    \"SDH\": \"SUBDURAL HEMATOMA\",\n    \"NH\": \"NURSING HOME\",\n    \"PT\": \"PATIENT\",\n    \"LT\": \"LEFT\",\n    \"RT\": \"RIGHT\",\n    \"&\" : \" AND \"\n}\nstr_remove = \"YOM|YOF|MOM|MOF|C/O|S/P|H/O|DX\"\n\n\ndef process_text(txt):\n    words = txt.split()\n    new_words = [remap.get(word, word) for word in words]\n    txt = \" \".join(new_words)\n\n    txt = re.sub(\"[^a-zA-Z ]\", \"\", txt)\n    txt = re.sub(str_remove, \"\", txt)\n\n    return re.sub(r\"^\\s+\", \"\", txt)\n\ndef narrative_age(string):\n    age = re.match(\"^\\d+\",string)\n\n    if not age:\n        age = 0\n    else:\n        age = age[0]\n        \n    return age\n\nWe then apply these to our verified falls data and our raw NEIS data from 2022:\n\n# process narrative text and extract patient age from narrative\nfalls['processed_narrative'] = falls['narrative'].apply(process_text)\nneis['processed_narrative'] = neis['Narrative_1'].apply(process_text)\n\nfalls['narrative_age'] = falls['narrative'].apply(narrative_age).astype(int)\nneis['narrative_age'] = neis['Narrative_1'].apply(narrative_age).astype(int)\n\n# neis cases are from 2022, remove from verified falls\nfalls = falls[falls['treatment_date'] < \"2022-01-01\"]\n\n# filter narrative ages to 65+\nfalls = falls[falls['narrative_age'] >= 65]\nneis = neis[neis['narrative_age'] >= 65]\n\nWe can see that our coding changes the narratives subtly. For example this string:\n\nfalls['narrative'][15]\n\n'87YOF HAD A FALL TO THE FLOOR AT THE NH STRUCK BACK OF HEAD HEMATOMA TO SCALP'\n\n\nIs changed to this:\n\nfalls['processed_narrative'][15]\n\n'HAD A FALL TO THE FLOOR AT THE NURSING HOME STRUCK BACK OF HEAD HEMATOMA TO SCALP'\n\n\nThis minimal amount of pre-processing should help the model identify similar cases without being affected by too much extranenous information. In addition, because the typical model has about 30,000 words encoded we need to make sure we avoid abbreviations which will be absent from the model dictionary.\n\n\nImplementing the Transformer model\nWe can grab all of our verified fall narratives as well as a random sample of narratives from the 2022 NEIS data. Below we’ll take a sample of 250 cases and run them through our model.\n\nN = 250\nidx = np.random.choice(neis.shape[0], N, replace=False)\n\nfall_narrative = np.array(falls['processed_narrative'])\nneis_narrative = np.array(neis['processed_narrative'])[idx]\n\nWe take the processed narratives and convert them to tokens using the pre-trained sentence transformer:\n\nembed_train = model.encode(fall_narrative)\nembed_test = model.encode(neis_narrative)\n\nWe then compute the cosine similarity between the two tensors. What we will end up with is the distance from our NEIS narratives and the verified fall cases. Cases with larger distances should be less likely to contain information about elderly fall cases.\n\ncos_sim = util.cos_sim(embed_test, embed_train)\n\nFor simplicity we scale the distances between 0 and 1, so that 1 is most similar and 0 is least similar. We can then just compare the rank-ordered narratives.\n\ndists = cos_sim.mean(1)\nd_min, d_max = dists.min(), dists.max()\n\ndists = (dists - d_min)/(d_max - d_min)\ndists = np.array(dists)\n\nout = dict(zip(neis_narrative, dists)) \n\nPlotting a histogram of the minmax scaled cosine similarity scores shows a lot of narratives that are very similar and a long tail of those that are not so similar. Of course, there isn’t a single cut point of what we would consider acceptable for classification purposes, but we could certainly use these scores in a regression to determine a suitible cut point if we were so interested.\n\n\nCode\ncparams = {\n    \"axes.spines.left\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.top\": False,\n    \"axes.spines.bottom\": False,\n    \"grid.linestyle\": \"--\"\n}\n\nsns.set_theme(style=\"ticks\", rc = cparams)\n\n(\n    sns.histplot(dists, color=\"#004488\"),\n    plt.xlabel(\"Cosine Similarity (minmax scaled)\")\n)\nplt.show()\n\n\n\n\n\n\n\nResults\nTime to actually see the results. Our results are stored in a dictionary which allows us to just pull narratives by similarity score. Let’s test it out by looking at the top 10 most similar NEIS narratives:\n\nsorted(out, key=out.get, reverse=True)[:10]\n\n['FELL TO THE FLOOR AT THE NURSING HOME  CLOSED HEAD INJURY',\n 'FELL ON THE FLOOR  CLOSED HEAD INJURY',\n 'WAS AT THE NURSING HOME AND SLIPPED AND FELL TO THE FLOOR STRIKING HIS HEAD  SCALP LACERATION',\n 'PRESENTS AFTER A FALL WHILE WALKING ACROSS THE LIVING ROOM AND HE TRIPPED AND FELL TO THE FLOOR REPORTS HE HIT HIS HEAD AND LEFT SHOULDER ON A RUG  FALL CLOSED FRACTURE OF CERVICAL VERTEBRA',\n 'FELL TO THE FLOOR STRUCK HEAD  CLOSED HEAD INJURY ABRASION TO KNEES',\n 'WAS GETTING INTO BED AND FELL TO THE FLOOR ONTO HEAD  CLOSED HEAD INJURY',\n 'FELL ON FLOOR AT NH INJURY  AND  BODY PATIENT NS  FALL',\n 'FELL BACKWARDS FROM  STEPS CLOSED HEAD INJURY ABRASION HAND',\n 'PRESENTS WITH HEAD INJURY AFTER A FALL REPORTS HE WAS FOUND ON THE FLOOR IN A RESTAURANT AFTER HE SLIPPED AND FELL HITTING HIS HEAD  INJURY OF HEAD',\n 'WENT TO SIT DOWN AND SOMEONE MOVED HER CHAIR AND SHE FELL BACKWARDS HITTING HER HEAD ON THE FLOOR  FALL BLUNT HEAD TRAUMA TAIL BONE PAIN']\n\n\nAnd the 10 least similar narratives:\n\nsorted(out, key=out.get, reverse=False)[:10]\n\n['SYNCOPAL EPISODE WHILE FOLDING NS CLOTHINGSYNCOPE',\n 'WAS COOKING SOME SALMON AND THEN SPRAYED A  AEROSOL DEODORANT DUE TO THE SMELL WHICH CAUSED HER TO FEEL THAT SOMETHING WAS STUCK IN HER THROAT FOREIGN BODY SENSATION IN THROAT',\n 'WAS PLAYING GOLF AND DEVELOPED AMS AND PASSED OUT  SYNCOPE',\n 'CO STABBING RIGHT CHEST PAIN RADIATES TO HER BACK SHORTNESS OF BREATH AFTER HER HHA WAS MOPPING THE FLOOR LAST NIGHT W STRONGPOTENT CLEANING AGENT THAT TRIGGERED HER ASTHMA  CHEST PAIN ASTHMA',\n 'WITH FISH HOOK IN HIS RIGHT INDEX FNIGER HAPPENED AT A LAKE FB RIGHT INDEX FINGER',\n 'CUT THUMB WITH BROKEN BOTTLE NO OTHER DETAILS LWOT NO ',\n 'EXERCISING FELT PAIN IN RIGHT LOWER LEG  LOWER LEG PAIN',\n 'CO LEFT SIDED CHEST PAIN FOR THE PAST THREE DAYS AFTER WORKING OUT AT THE GYM  LEFT PECTORALIS MUSCLE STRAIN',\n 'PRESENTS AFTER BEING IN A ROOM FILLED WITH SMOKE FOR  HOURS AFTER THERE WAS A FIRE IN HER NEIGHBORS APARTMENT UNKNOWN IF FIRE DEPARTMENT INVOLVED  SMOKE INHALATION PAIN IN THROAT ELEVATED TROPONIN',\n 'ON  FOR AF WAS WASHING DISHES AND SLASHED ARM ON A KNIFE  LACERATION OF RIGHT FOREARM']\n\n\nSo in general, it did a pretty good job. The most similar cases are all clearly related to falls, while the least similar ones are all a mix of other injuries. While I don’t have any tests here (coming soon!) I suspect this does better than very simple regex queries. If only because it has the ability to find similarities without needing to match on specific strings."
  },
  {
    "objectID": "posts/nlp-falls/nlp_falls.html#singular-queries",
    "href": "posts/nlp-falls/nlp_falls.html#singular-queries",
    "title": "Using NLP To Classify Medical Falls",
    "section": "Singular Queries",
    "text": "Singular Queries\nWe can extend this model a bit and create a small class object that will take a single query in, and return the \\(K\\) most similar narratives. Below, we bundle our functions into a NarrativeQuery class object. After encoding the narrative we can provide query strings to find sementically similar narratives.\n\nclass NarrativeQuery:\n    def __init__(self, narrative):\n        self.narrative = narrative\n        self.narrative_embedding = None\n        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n    def encode(self):\n        self.narrative_embedding = self.model.encode(self.narrative)\n\n    def search_narrative(self, query, K = 5):\n        embed_query = self.model.encode(query)\n\n        query_out = self.cos_sim(self.narrative_embedding, embed_query)\n\n        return sorted(query_out, key=query_out.get, reverse=True)[:K]\n\n    def cos_sim(self, embed, embed_query):\n        cs = util.cos_sim(embed, embed_query)\n\n        dists = cs.mean(1)\n        d_min, d_max = dists.min(), dists.max()\n\n        dists = (dists - d_min)/(d_max - d_min)\n        dists = np.array(dists)\n\n        return dict(zip(self.narrative, dists))\n\nThis sets it up:\n\nFallsQuery = NarrativeQuery(neis_narrative)\nFallsQuery.encode()\n\n…and this performs the search. Here we’re just looking for narratives where a person slipped in a bathtub.\n\nFallsQuery.search_narrative(query=\"SLIPPED IN BATHTUB\", K = 10)\n\n['SLIPPED AND FELL IN THE SHOWER LANDING ONTO BUTTOCKS  CONTUSION TO BUTTOCKS',\n 'SLIPPED ON FLOOR AND FELL AT HOME  FALL',\n 'PRESENTS AFTER A SLIP AND FALL IN THE TUB STRIKING HER HEAD ON THE WALL  SYNCOPE FALL HEAD STRIKE',\n 'FELL IN THE SHOWER  FRACTURED UPPER BACK',\n 'PATIENT FELL IN THE SHOWER AND HIT HER HEAD AND HER LEFT ELBOW  LACERATION OF SCALP WITHOUT FOREIGN BODY STRUCK BATH TUB WITH FALL ABRASION OF LEFT ELBOW',\n 'WAS WALKING FROM THE BATHROOM TO THE BEDROOM AND PASSED OUT FALLING TO THE FLOOR CAUSING A SKIN TEAR TO HIS LEFT ELBOW  SKIN TEAR OF LEFT ELBOW',\n 'SLIPPED AND FELL IN FLOOR AT HOME  R HIP FRACTURE',\n 'FELL IN THE SHOWER AT HOME TWISTING RIGHT KNEE  RUPTURE RIGHT PATELAR TENDON',\n 'WEARING SOCKS SLIPPED AND FELLHEAD INJURYFX FEMUR',\n 'SLIPEPD AND FELL IN THE SHOWER STRUCK HEAD  CLOSED HEAD INJURY CONTUSION TO LEFT HIP']\n\n\nNow this is cool. Using the sentence transformer we are able to get passages that are similar in style to what we searched, without sharing the exact same language. For example, the search query is \"SLIPPED IN BATHTUB\" but we get results like \"FELL IN THE SHOWER\" and \"SLIP AND FALL IN THE TUB\". If we were looking specifically for passages related to falls in the bathtub these obviously make sense (many bathtubs are also just showers as well)."
  },
  {
    "objectID": "posts/nlp-falls/nlp_falls.html#finally",
    "href": "posts/nlp-falls/nlp_falls.html#finally",
    "title": "Using NLP To Classify Medical Falls",
    "section": "Finally",
    "text": "Finally\nNow, this isn’t probably news to most people that actually regularly work with language models. However, it is quite impressive that with a pre-trained model and very minimal pre-processing, you can obtain reasonable results off-the-shelf. I’ll definitely be keeping my eyes on these models in the future and looking for ways where they can improve my workflow."
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html",
    "title": "The Power of Ensembles",
    "section": "",
    "text": "It’s no secret that ensemble methods are extremely powerful tools in statistical inference, data science, and machine learning. It’s long been known that many “imperfect” models combined together can often perform better than any single model.\nFor example, in the M5 forecasting competition almost all of the top performers used some element of model averaging or ensembling. Indeed, the very foundations of some of the most commonly used tools in machine learning, like random forests and boosting, work by averging across many highly biased models to create a single more powerful model. The success of this method is relies on the fact that averaging across many high-variance models generally results in a single, lower-variance model. This is most evident in the idea of the “wisdom of the crowd”, where large groups of individuals are often better at predicting something compared to a single expert. However, one area which hasn’t received much attention is outlier detection. When we say “outliers” we’re generally referring to observations that are exceptionally unusual compared to the rest of the sample. A rather consise definition by Hawkins (1980) states:\nThis rather broad definition fits well with the general application of outlier detection. It can be used for identifying fraud in insurance or healthcare datasets, intrusion detection for computer networks, or flagging anomalies in time-series data - among many others. The specific challenge I want to address in this mini-blog is an ensembling approach for unsupervised outlier detection. This is doubly interesting because unsupervised learning presents many more issues compared to supervised learning. Below, I’ll contrast some of these differences and then describe an interesting ensemble approach."
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#outlier-detection-with-supervision",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#outlier-detection-with-supervision",
    "title": "The Power of Ensembles",
    "section": "Outlier Detection with Supervision",
    "text": "Outlier Detection with Supervision\nTo start, let’s look at an example using the cardio dataset sourced from the pyod benchmark set. In this case we have 1831 observations with 21 variables, of which about 9.6% of them are considered anomalous. These are conviently labeled for us, where a value of 1 indicates an anomalous reading. If we fit a simple random forest classifier we see that it is trivial to get a very high AUC on the test data (let’s also not get ahead of ourselves here, as this is a toy dataset with a target that is quite easy to predict). Below we see an example of the fairly strong separation between the inliers and outliers. Our random forest works well in this case - giving us a test AUC of .99 and an average precision of .98. While this is an overly simple example, it does expose how easy some models can be (in many cases) when there is a definite target variable.\n\n\nCode\n# fit a random forest classifier\nrf = RandomForestClassifier()\nrf.fit(Xtrain, ytrain)\n\n# extract the predictions, calculate AUC\nrf_preds = rf.predict_proba(Xtest)[:,1]\n\neval_preds(ytest, rf_preds)\n\n\nRoc:0.997\nPrn:0.977\n\n\n\n\nCode\nsns.scatterplot(x = X[:,7], y = X[:,18], hue = y)\n(\n    plt.xlabel(\"Feature 7\"),\n    plt.ylabel(\"Feature 18\"),\n    plt.title(\"Cardio Scatterplot, Inliers and Outliers\")\n)\nplt.show()\n\n\n\n\n\nScatterplot of inliers (0) and outliers (1), displaying strong separation"
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#unsupervised-outlier-detection",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#unsupervised-outlier-detection",
    "title": "The Power of Ensembles",
    "section": "Unsupervised Outlier Detection",
    "text": "Unsupervised Outlier Detection\nLet’s talk about unsupervised outlier detection. Unlike the situation above in an unsupervised setting we don’t have the convenience of a set of labels identifying whether a given observation is anomalous or not. And because we’re lacking this ground truth, it makes things a lot more complicated for choosing both our model(s) and the parameters for those model(s). Let’s talk about why.\n\nHow do we select a best model?\nIn classic supervised learning we can choose a metric to optimize (say, root-mean squared error or log-loss), then fit a model which attempts to minimize that metric. In the simplest case, think about ordinary least squares. In that case we have a simple target of minimizing the sum of squared errors. We can validate the fit of the model by looking at evalution metrics (RMSE, R-Squared, standardized residuals).However, when we lack a way to identify if a given observation is anomalous or not we don’t have any meaningful way to know if one model is doing better than another.\nYou might also be thinking about optimizing a specific parameter in a model (like the number of nearest neighbors \\(K\\) in a K-nearest neighbors model) using some criterion that doesn’t rely on a target variable (like the ‘elbow’ method). However, optimizing this parameter doesn’t guarantee that the model itself is useful. Simply put, we’re often left groping around in the dark trying to decide what the optimal model or set of parameters is.\nLet’s consider this example: Say we’re looking at the same cardio dataset from above and trying to decide what unsupervised outlier detector we want to use. Maybe we’re deciding between a distance-based one like exact K-nearest neighbors (KNN) or a density-based one like local outlier factor (LOF). Let’s also say we’re agnostic to parameter choices, so we stick with the default ones provided by pyod.\n\n# initalize and fit using default params\nknn = KNN()\nlof = LOF()\n\nknn.fit(X)\nlof.fit(X)\n\n# extract the predictions, calculate AUC\nknn_pred = knn.decision_function(X)\n\neval_preds(y, knn_pred)\n\nRoc:0.686\nPrn:0.286\n\n\n\n\nCode\n# extract the predictions, calculate AUC\nlof_pred = lof.decision_function(X)\n\neval_preds(y, lof_pred)\n\n\nRoc:0.546\nPrn:0.154\n\n\nHere we see that the KNN model performs better than the LOF model - however we didn’t adjust any of the \\(K\\) parameters for either model. Because, in practice, we can’t see this, we don’t know a-priori which model or set of parameters will work best in a given case. This is in stark contrast to our first attempt when we could simply focus on decreasing out-of-sample bias."
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#an-unsupervised-ensambling-approach",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#an-unsupervised-ensambling-approach",
    "title": "The Power of Ensembles",
    "section": "An Unsupervised Ensambling Approach",
    "text": "An Unsupervised Ensambling Approach\nSo, because we can’t easily decrease bias (due to the lack of ground truth values) what are our options? Well, as we saw above, there is a large source of variance implicit in these models. This variance can come from multiple sources:\n\nChoice of model. There is considerable variance in how different models perform under different kinds of anomolies. For example, absent some evaluation metric how do you meaningfully choose between KNN, LOF, or one of many other methods. The pyod benchmark page shows that many models perform quite differently under different types of dimensionality and outlier proportion.\nChoice of parameters. Almost all anomaly detection models have added uncertaintly based on the choice of parameters. For example, in K-nearest neighbors we need to specify the parameter \\(K\\). One-class support vector machines (SVM) are notoriously difficult to tune in part due to the number of parameters to choose (choice of kernel, polynomial degree, ect…).\n\nTherefore, in an unsupervised model our best option is to try to reduce the variance implicit in both the sources above. Rather than staking our whole model on a single choice of model, or a single set of parameters, we can ensemble over a wide number of choices to avoid the risk of choosing a catastrophically bad combination. Since we don’t have access to ground truth, this ends up being the safest option (and as we will see, generally produces good results).\n\nALSO: A regression-based approach\nFor this specific post I’m going to focus on an unsupervised ensemble algothrim proposed by Paulheim & Meusel (2015) and further discussed in Aggarwal & Sathe (2017). The authors dub this method “attribute-wise learning for scoring outliers” or ALSO. The approach we are going to use extends the logic of the ALSO model to an ensemble-based approach (hence ALSO-E).\nLet’s talk a bit about the logic here. The general idea of the algothrim is that we iteratively choose a target feature \\(X_j\\) from the full set of features \\(X\\). The chosen \\(j\\) value is used as the target and the remaining \\(X - j\\) features are used to predict \\(j\\). We repeat this for all features in \\(X\\), collecting the standardized model residuals at each step. We then average the residuals across all the models and use them to identify “anomalous” observations. In this case, more anomalous observations will likely be ones whose residuals are substantially larger than the rest of the sample. The beauty of this method is that for the modelling portion we can choose any base model for prediction (e.g. linear regression, random forests, etc…).\nTo avoid models that are very overfit, or have virtually no predictive ability, we define weights for each model using cross-validated RMSE. The goal here is to downweight models that have low predictive ability so they have a proportionally lower effect on the final outlier score. This is defined as\n\\[w_k = 1 - min(1, RMSE(M_k))\\]\nwhich simply means that models that perform worse than predicting the mean (which would give us an RMSE of 1) are weighted to 0, while a theoretically “perfect” model would be weighted 1. This gives us the added benefit of downweighting features that have little or no predictive value, which helps in cases when we might have one or more irrelevant variables.\n\n\nAdding an E to ALSO\nThe base algothrim above fits \\(X\\) models using all \\(n\\) observations in the data. However, we can extend this model to an ensambling method by applying some useful statistical tools - namely variable subsambling. The idea proposed by Aggarwal & Sathe (2017) is to define a number of iterations (say, 100), and for each iteration randomly subsamble the data from between \\(min(1, \\frac{50}{n})\\) and \\(min(1, \\frac{1000}{n})\\). This means that each model is fit on a minimum of 50 observations and up to 1000 (or, \\(n\\) if \\(n\\) is less than 1000). In addition, we randomly choose a feature \\(j\\) to be predicted. Combined, this ensambling approach makes a more efficient use of the available data and results in a more diverse set of models.\nTo my knowledge, there is no “official” implementation of the ALSO-E algothrim, and it is not present in any large libraries (e.g. pyod or sklearn). However the method is generic enough that it is not difficult to code from scrach. Using the notes available I implemented the method myself using a random forest regressor as the base detector. The code below defines a class with a fit() and predict() function. The fit() function handles all the subsampling and fits each sample on a very shallow random forest regressor. The predict() function does the work of getting the residuals and re-weighting them according to their CV-error. While my implementation is certainly not “feature complete”, it’s good enough to try out:\n\n\nCode\n# code to fit an ALSOe anomaly detector\n\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nclass ALSOe():\n    \"\"\" Initializes a regression-based outlier detector\n    \"\"\"\n\n    def __init__(self, N = 100) -> None:\n\n        self.param_list = []\n        self.model_list = []\n        self.anom_list = []\n        self.wt = []\n    \n        self.N = N\n        self.std_scaler = StandardScaler()\n\n    def fit(self, data):\n        \"\"\" Fit an ensemble detector\n        \"\"\"\n\n        # standardize data\n        self.std_scaler = self.std_scaler.fit(X = data)\n        data = self.std_scaler.transform(data)\n\n        # fit N models\n        for i in range(0, self.N):\n\n            # define sample space\n            n = data.shape[0]\n            p = data.shape[1]\n            s = [min([n, 50]), min(n,1000)]\n\n            # draw s random samples from dataframe X\n            s1 = np.random.randint(low = s[0], high = s[1])\n            p1 = np.random.randint(low = 0, high = p)\n            ind = np.random.choice(n, size = s1, replace = False)\n\n            # define random y and X \n            df = data[ind]\n            y = df[:,p1]\n            X = np.delete(df, p1, axis=1)\n\n            # initalize RF regressor\n            rf = RandomForestRegressor(n_estimators=10)\n\n            # fit & predict\n            rf.fit(X, y)\n\n            # add fitted models & y param to list\n            self.model_list.append(rf)\n            self.param_list.append(p1)\n\n    def predict(self, newdata):\n\n        \"\"\" Get anomaly scores from fitted models\n        \"\"\"\n\n        # standardize data\n        newdata = self.std_scaler.transform(newdata)    \n\n        for i,j in zip(self.model_list, self.param_list):\n\n            # define X, y\n            y = newdata[:,j]\n            X = np.delete(newdata, j, axis=1)\n\n            # get predictions on model i, dropping feature j\n            yhat = i.predict(X)\n\n            # rmse\n            resid = np.sqrt(np.square(y - yhat))\n            resid = (resid - np.mean(resid)) / np.std(resid) \n\n            # compute and apply weights\n            cve = cross_val_score(i, X, y, cv=3, scoring='neg_root_mean_squared_error')\n            w = 1 - min(1, np.mean(cve)*-1)\n\n            resid = resid*w\n\n            # add weights and preds to lists\n            self.wt.append(w)\n            self.anom_list.append(resid)\n\n        # export results as min-max scaled\n        anom_score = np.array(self.anom_list).T\n        anom_score = np.mean(anom_score, axis = 1)\n\n        # rescale and export\n        anom_score = StandardScaler().fit_transform(anom_score.reshape(-1,1))\n        anom_score = anom_score.flatten()\n\n        return anom_score\n\n\n\nFitting the model\nWith all this in mind, fitting the actual model is quite simple. As stated above, the ALSO-E approach is largely parameter free, which means there isn’t much for the user to worry about. Here we’ll just initialize a model with 100 iterations, fit all of the random forest regressors, then get the weighted standardized residuals. This whole process can be condensed into basically 3 lines of code:\n\n# Fit an ALSOe regression ensemble\n# using 100 random forests\nad = ALSOe(N = 100)\nad.fit(X)\n\n# extract predictions from the models and combine\n# the standardized outlier scores\nad_preds = ad.predict(X)\n\n\n\nEvaluating the predictions\nNow that we have the predictions, we can look at the distribution of outlier scores. Below we see a histogram of the ensembled scores which, to recall, are rescaled to mean 0 and standard deviation 1. Therefore, the most anomalous observations will have large positive values. Consistent with what we would expect to see, there is a long tail of large residuals which correspond to the outliers, while the bulk of the data corresponds to a mostly “normal” set of values centered around zero.\n\n\nCode\nsns.histplot(x = ad_preds)\n(\n    plt.xlabel(\"Anomaly Score\"),\n    plt.ylabel(\"Observations\"),\n    plt.title(\"ALSO-E Anomaly Scores\")\n)\nplt.show()\n\n\n\n\n\nHistogram of anomaly scores. The characteristic long tail highlights potential anomalous observations\n\n\n\n\nWe can evaluate the performance of our method by bootstrapping the original dataset 10 times, then running our model on each of the boostrap replicates. This is because there is bound to be some degree of randomness based on the chosen samples and variables for each iteration. Averaging over the bootstrap replicates helps give us some idea of how this model might perform “in the wild” so to speak. Below I define a little helper function to resample the dataset, fit the model, and then extract the relevant evaluation metrics. We then loop through the function and put the fit statistics in a set of lists. For evaluation we’ll look at the averages for each set.\n\n# Bootstrap sample from base dataset and evaluate metrics\ndef boot_eval(df):\n    X, y = resample(df['X'], df['y'])\n\n    ad = ALSOe(N = 100)\n    ad.fit(X)\n    ad_preds = ad.predict(X)\n\n    auc = roc_auc_score(y, ad_preds)\n    pre = average_precision_score(y, ad_preds)\n\n    return [auc, pre]\n\n# run models\nroc_list = []\nprn_list = []\n\nfor i in range(10):\n    roc, prn = boot_eval(data)\n\n    roc_list.append(roc)\n    prn_list.append(prn)\n\nShown below, we see we have a decent AUC and average precision score of about 0.71 and 0.26 respectively. While this is substantially lower than the supervised model, it is still better than the base KNN and LOF models above. The ensambling process also makes it easy because we don’t have to specify any parameters other than the number of iterations to run. In testing, the default 100 works well as a starting point, and there aren’t huge performance gains by increasing it substantially.\n\n\nCode\nprint(f'Avg. ROC: {np.round(np.mean(roc_list),3) }\\nAvg. Prn: {np.round(np.mean(prn_list),3)}')\n\n\nAvg. ROC: 0.693\nAvg. Prn: 0.261\n\n\n\n\n\nComparing performance across datasets\nWe can also evaluate its performance on a variety of other datasets. Here I randomly chose another four datasets from the pyod benchmarks page and compared its performance over 10 bootstrap resamplings to the other benchmarked methods in the pyod ecosystem. Looking at the results we see that we get median RocAUC scores of between .7 to .85 and average precision scores between .2 to .85. For an unsupervised model this isn’t too bad, and largely falls within the range of other detectors.\nWe should note that while its performance is never the best, it is also never the worst either. For example: the .707 we achieved on the cardio dataset is lower than some of the best methods (in this case, PCA and Cluster-Based LOF). However, we avoid extremely bad results like with Angle-Based Outlier Detection or LOF. This underscores our goals with the ensemble model: we prefer a more conservative model that tends to perform consistently across many types of anomalies. We also avoid issues related to choosing optimal parameters but simply ensambling over many detectors. In an unsupervised case this decrease in variance is especially desirable.\n\n\nCode\n# load additional datasets\nd1 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/14_glass.npz\")\nd2 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/18_Ionosphere.npz\")\nd3 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/20_letter.npz\")\nd4 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/21_Lymphography.npz\")\n\ndlist = [d1,d2,d3,d4]\nmname = ['Cardio', 'Glass','Ionosphere','Letter','Lympho']\n\nroc_list_m = []\nprn_list_m = []\n\n# run models\nfor j in dlist:\n \n    for i in range(10):\n        roc, prn = boot_eval(j)\n\n        roc_list_m.append(roc)\n        prn_list_m.append(prn)\n\n\n# Plot evaluation metrics across datasets\nevaldf = pd.DataFrame({'RocAUC' : roc_list +roc_list_m, \n                       'Precision' : prn_list + prn_list_m,\n                       'Dataset': sorted([x for x in mname*10])})\\\n            .melt(id_vars = 'Dataset', var_name = 'Metric', value_name = 'Score')\n\n# facet plot across datasets\ng = sns.FacetGrid(evaldf, col = 'Metric', sharey = False, col_wrap=1, aspect = 2)\n(\n    g.map(sns.boxplot, 'Dataset','Score', order = mname),\n    g.fig.subplots_adjust(top=0.9),\n    g.fig.suptitle('ALSO-E Model Evaluation', fontsize=16)\n)\nplt.show()\n\n\n\n\n\nensemble models often are often not as good as the best method, but can achieve consistently decent performance."
  },
  {
    "objectID": "posts/pca-anomaly/pca_anomaly.html",
    "href": "posts/pca-anomaly/pca_anomaly.html",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "I strongly believe in “learning by doing”. One of the things I have been working on quite a bit lately is unsupervised anomaly detection. As with many machine-learning tools, ensembles are incredibly powerful and useful for a variety of circumstances.\nAnomaly detection ensembles are no exception to that rule. To better understand how each of the individual pieces of a anomaly detection ensemble works, I’ve decided two build one myself “from scratch”. I put that in giant quotes here because I’ll still rely on some existing frameworks in R to help built the underlying tools.\nMy idea is to create an ensemble of several heterogeneous anomaly detection methods in a way that maximizes their individual benefits. Following some of the guidance proposed by Aggarwal & Sathe I will use:\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\nhosp <- read_csv(\"Hospital_Inpatient_Discharges__SPARCS_De-Identified___2021.csv\")\n\n\nThe data for this post comes from the state of New York’s Hospital Inpatient Discharges (SPARCS) data for 2021. This data contains about 2.1 million records on hospital discharges, including some de-identified patient information, procedure types, and costs. From an anomaly detection standpoint, it might make sense to see if we can identify hospitals with anomalous costs relative to other New York Hospitals. Like the New York Times reported, hospitals make up a very substantial portion of what we spent on healthcare.\nWe’ll create a quick feature set based on a few key payment variables. Here I aggregate over each of the hospitals by calculating their (1) average stay length, (2) average charges, (3) average costs, (4) their average cost-per-stay, (5) the ratio between costs to total charges and (6) the average procedure pay difference.\nThis last feature is a bit more complex, but what I am essentially doing is finding the median cost per-procedure by case severity (assuming more severe cases cost more) and then finding out how much each hospital diverges, on average, from the global cost. It’s a indirect way of measuring how much more or less a given procedure costs at each hospital. This is a easy measure to utilize because a 0 indicates that the hospital bills that procedure at near the global median, a 1 means they bill 100% more than the median and -.5 means they bill 50% less than the median.\n\n\nCode\n# compute and aggregate feature set\ndf <-\n  hosp %>%\n  group_by(`CCSR Procedure Code`, `APR Severity of Illness Code`) %>%\n  mutate(\n    proc_charge = median(`Total Costs`),\n    charge_diff = (`Total Costs` - proc_charge)/proc_charge,\n    `Length of Stay` = as.numeric(ifelse(\n      `Length of Stay` == '120+', 120, `Length of Stay`\n    ))\n  ) %>%\n  group_by(id = `Permanent Facility Id`) %>%\n  summarise(\n    stay_len = mean(`Length of Stay`, na.rm = T),\n    charges = mean(`Total Charges`),\n    costs = mean(`Total Costs`),\n    diff = mean(charge_diff),\n    cost_ratio = mean(`Total Costs`/`Total Charges`),\n    cost_per_stay = costs / stay_len\n  )"
  },
  {
    "objectID": "posts/pca-anomaly/pca_anomaly.html#creating-a-pca-anomaly-detector",
    "href": "posts/pca-anomaly/pca_anomaly.html#creating-a-pca-anomaly-detector",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Creating a PCA Anomaly Detector",
    "text": "Creating a PCA Anomaly Detector\nLet’s now get into the nuts and bolts of actually creating a PCA-based anomaly detector. Now, there are a few ways we can go about this, but I’m going to rely on the approach suggested by Charu Aggarwal in his book Outlier Analysis. What he essentially proposes is a “soft” version of principal components where the eigenvectors are weighted by their variance instead of choosing only the eigenvectors that explain the highest proportion of the overall variance. This “soft” approach has some overlap with the mahalanobis distance. This is the same approach taken by the PCA anomaly detector in the Python pyod package if weighting is specified.\n\nPerforming the “soft” PCA\nTranslating this approach from the formula to code is actually pretty straightforward. Agarwal gives us the following:\n\\[Score(\\bar{X}) = \\sum^d_{j=1} \\frac{|(\\bar{X} - \\bar{\\mu}) *\\bar{e_j}|^2}{\\lambda_j}\\] Which we can code into the following below. Before we add our data to the PCA we scale it to have mean 0 and standard deviation 1 so that the input features are scale-invariant. We then work through the process of extracting the eigenvectors, computing the variance for each, and then performing the “soft” PCA approach.\n\n\n\n\n\n\nNote\n\n\n\nA few notes - strictly speaking the the part \\(\\bar{X} - \\bar{\\mu}\\) isn’t totally necessary in most cases because the eigenvectors are already scaled to have a mean of zero (you can also ensure this by specifying cor=TRUE in the princomp() function). This does help in unusual cases where \\(\\bar{\\mu}\\) is not zero.\n\n\n\n# \"Soft\" PCA\n\n# scale input attributes\nX <- df[, 2:7]\nX <- scale(X)\n\n# pca anomaly detection\n# extract eigenvectors & variance\npca <- princomp(X, cor = TRUE)\ne <- pca$scores\nev <- diag(var(e))\nmu <- apply(e, 2, mean)\nn <- ncol(e)\n\n# compute anomaly scores\nalist <- vector(mode = \"list\", length = n)\nfor(i in 1:n){\n  alist[[i]] <- abs( (e[, i] - mu[i])^2 / ev[i])\n}\n\n# extract values & export\nXscore <- as.matrix(do.call(cbind, alist))\nanom <- apply(Xscore, 1, sum)\n\nThis “soft” PCA is in contrast to so-called “hard” PCA where a specific number of principal components are chosen and the remainder are discarded. The “hard” PCA primarily focuses on reconstruction error along the components with the most variance, while the “soft” approach weights outliers on the lower variance components higher. This is useful because the data vary much less on these components, so outliers are often more obvious along these dimensions.\n\n\nCode\npca$scores %>%\n  data.frame() %>%\n  pivot_longer(cols = starts_with(\"Comp\")) %>%\n  ggplot() +\n  geom_histogram(aes(x = value), bins = 30, fill = '#004488') +\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_minimal() +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\nOutliers are often more visable along components with lower variance.\n\n\n\n\nThe code below implements the PCA anomaly detector. Most of the work involves taking the scores, putting them into a matrix, and then re-weighting them by their variance. The final score is just the row-wise sum across the re-weighted columns. The output is just a vector of anomaly scores anom.\n\n\nCode\nggplot(tibble(anom)) +\n  geom_histogram(aes(x = anom), bins = 25, color = 'white', fill = '#004488')+\n  labs(x = \"Anomaly Score\", y = \"Observations\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nDistribution of outlier scores. More anomalous observations have higher scores\n\n\n\n\n\n\nFlagging Outliers\nFor flagging outliers we can rely on the \\(\\chi^2\\) distribution. This is handy because the \\(\\chi^2\\) distribution is formed as the sum of the squares of \\(d\\) independent standard normal random variables. For the purposes of this example we might just choose 1% as our threshold for anomalies. After flagging we can plot the data along different 2d distributions to see where they lie. For example, the 8 anomalous hospitals generally have both longer stay lengths and charge more for comparable procedures relative to other hospitals.\n\n\nCode\n# compute a p-value for anomalies and\n# append anomaly score and p-values to new dataframe\np = sapply(anom, pchisq, df=6, ncp = mean(anom), lower.tail=F)\n\nscored_data <- data.frame(df, anom, p)\n\nflag <- scored_data$p <= 0.01\n\nggplot() +\n  geom_point(data = scored_data, aes(x = stay_len, y = diff), color = '#004488', size = 2, alpha = .25) +\n  geom_point(data = scored_data[flag,], aes(x = stay_len, y = diff), color = '#BB5566', size = 2.5) +\n  labs(x = \"Stay Length\", y = \"Avg. Payment Difference\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nPlotting inliers (blue) and outliers (red) in 2D space.\n\n\n\n\n\n\nPCA Anaomaly Detector: Example Function\nHere’s a minimal working example of the procedure above. As we build our ensemble, we’ll come back to this function later.\n\n\nCode\n# Run a principal components anomaly detector\nadPCA <- function(X){\n  \n  # pca anomaly detection\n  # extract eigenvectors & variance\n  pca <- princomp(X, cor = TRUE)\n  e <- pca$scores\n  ev <- diag(var(e))\n  mu <- apply(e, 2, mean)\n  n <- ncol(e)\n  \n  # compute anomaly scores\n  alist <- vector(mode = \"list\", length = n)\n  for(i in 1:n){\n    alist[[i]] <- abs( (e[, i] - mu[i])^2 / ev[i])\n  }\n  \n  # extract values & export\n  Xscore <- as.matrix(do.call(cbind, alist))\n  anom <- apply(Xscore, 1, sum)\n  \n  return(anom)\n}"
  },
  {
    "objectID": "posts/pca-timeseries/pca_ts.html",
    "href": "posts/pca-timeseries/pca_ts.html",
    "title": "Anomaly Detection for Time Series",
    "section": "",
    "text": "Identifying outliers in time series is one of the more common applications for unsupervised anomaly detection. Some of the most common examples come from network intrusion detection, mechanical processes, and other types of high-volume streaming data.\nOf course, there are just as many proposed ways of identifying outliers from the simple (basic Z-scores) to the complex (convolutional neural networks). There are also some approaches that rely on more conventional tabular approaches. Rob Hyndman proposed a few approaches here and here showing how many high-volume time series can be compressed into a tabular dataset. The general idea is that you can decompose many time series into tabular observations by creating a large variety of features describing each series.\n\n\nThe data we’ll use is on hourly power usage for a large power company (American Electric Power). From this dataset we can perform some basic aggregation (to ensure that all timestamped values are on the same day-hour), then separate each set of 24 hours into their individual days. The goal here is to make it easier to look at hours within each day. The code below does a bit of this processing. Of course, working with dates is still always a pain, despite the improvements in R libraries.\n\n\nCode\n# Read data, convert to zoo\nelec <- read_csv(\"AEP_hourly.csv\") %>%\n  group_by(Datetime) %>%\n  summarise(AEP_MW = sum(AEP_MW)) %>%\n  filter(year(Datetime) %in% 2017)\n\nelec_ts <- zoo(x = elec$AEP_MW, order.by = elec$Datetime, frequency = 24)\n\n# Split the hourly time series into daily time series\ndaily_ts_list <- split(elec_ts, as.Date(index(elec_ts)))\n\n# Extract the first 24 observations of each daily time series\n# dropping days with missing values\ndaily_24_ts_list <- lapply(daily_ts_list, function(x) {\n  if (length(x) >= 24) {\n    return(x[1:24])\n  } else {\n    return(NA)\n  }\n})\n\n# Convert from list to dataframe\ndaily_24_ts_list <- purrr::discard(daily_24_ts_list, ~any(is.na(.)))\n\n\nAfter converting the list of values to a data frame, we can proceed with the featurization. As we said before, we can use the tsfeatures library to decompose each day’s hourly values into a single observation. We can see this creates a data frame with 17 features, which correspond to various measures, including: autocorrelation, seasonality, entropy and other ad-hoc measures of time series behavior.\n\n# Convert from list to dataframe, extract TS features\ndaily_24_ts_list <- purrr::discard(daily_24_ts_list, ~ any(is.na(.)))\n\n# create time series features using `tsfeatures`\ndf <- daily_24_ts_list %>%\n  tsfeatures(\n    features = c(\n      \"acf_features\",\n      \"stl_features\",\n      \"entropy\",\n      \"lumpiness\",\n      \"stability\",\n      \"max_level_shift\"\n    )\n  ) %>%\n  select(-nperiods,-seasonal_period)\n\n\n\nCode\nglimpse(df, width = 65)\n\n\nRows: 364\nColumns: 13\n$ x_acf1      <dbl> 0.8139510, 0.9318718, 0.9123398, 0.9173901,…\n$ x_acf10     <dbl> 1.771656, 2.081091, 1.868137, 1.889912, 1.7…\n$ diff1_acf1  <dbl> 0.6284980, 0.6181949, 0.6534759, 0.6194560,…\n$ diff1_acf10 <dbl> 1.4141518, 0.7771648, 0.6910469, 1.0491669,…\n$ diff2_acf1  <dbl> 0.3426413, 0.2082615, 0.3164917, 0.2786098,…\n$ diff2_acf10 <dbl> 0.5009215, 0.2550913, 0.4214296, 0.2577332,…\n$ trend       <dbl> 0.7256036, 0.9374949, 0.9377550, 0.9478148,…\n$ spike       <dbl> 1.946527e-04, 4.715699e-06, 6.706533e-06, 4…\n$ linearity   <dbl> 1.8002798, 3.6310069, 3.3908752, 4.1128389,…\n$ curvature   <dbl> 0.4988011, -1.2265007, -2.0788821, -0.66100…\n$ e_acf1      <dbl> 0.6719203, 0.6507832, 0.6513875, 0.6636410,…\n$ e_acf10     <dbl> 1.4588480, 1.0862781, 0.9523931, 1.1326925,…\n$ entropy     <dbl> 0.3585884, 0.5225069, 0.4962494, 0.3090901,…\n\n\n\n\n\nAfter doing this, we can proceed as a normal tabular data problem. The PCA anomaly detector that was detailed in an earlier post is an easy plug in here and is a natural fit for the problem. We have a lot of highly correlated measures that likely share a large amount of variance across a few dimensions. We can then weight the lower-variance dimensions higher to identify anomalous series. We’ll use the \\(\\chi^2\\) distribution to derive a p-value, which we can then threshold for flagging outliers.\n\n# Perform anomaly detection\nanom <- adPCA(df)\np = sapply(anom, pchisq, df=ncol(df), ncp = mean(anom), lower.tail=F)\n\n\n\n\nWe can see which series were flagged by the model by highlighting the series which were flagged at the \\(p < .01\\)\n\n\nCode\n# flag observations at p < 0.01\nelec_plot <- elec %>%\n  mutate(date = as.Date(format(Datetime, \"%Y-%m-%d\")),\n         hour = hour(Datetime)) %>%\n  left_join(scored_data) %>%\n  mutate(flag = ifelse(p < 0.01,1,0))\n\nggplot(data = elec_plot, aes(x = Datetime, y = AEP_MW, group = date)) +\n  geom_line(color = '#004488', alpha = .125) +\n  geom_line(data = elec_plot[elec_plot$flag == 1,], color = '#BB5566') +\n  labs(x = \"Date\", y = \"Megawatt Hours\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nAnamolous days are highlighted in red. Note the unusually high spike in late 2017.\n\n\n\n\nWe can also see what these anomalous series look like compared to the other series on a hourly basis. This plot clearly shows one series with a unusual early-morning spike, and several series with flatter trajectories compared to more normally expected seasonality - in particular, they are days with low power consumption in the afternoon when consumption is usually at its highest.\n\n\nCode\nggplot(data = elec_plot, aes(x = hour, y = AEP_MW, group = date)) +\n  geom_line(color = '#004488', alpha = .075) +\n  geom_line(data = elec_plot[elec_plot$flag == 1,], color = '#BB5566', size = .7) +\n  labs(x = \"Hour of Day\", y = \"Megawatt Hours\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nAnomalous days often have flatter curves and dip during high-load hours of the day.\n\n\n\n\nArguably this isn’t an ideal approach because each sub-series is only comprised of 24 observations. That means reliably identifying seasonality via the stl_features is questionable. In addition, this approach loses some information that comes from day-to-day correlations. It would probably be worthwhile testing this approach against something like STL decomposition."
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#spatial-features",
    "href": "posts/rtm-spatial/rtm.html#spatial-features",
    "title": "Generating Spatial Risk Features using R",
    "section": "Spatial Features",
    "text": "Spatial Features\nIn criminology there is a considerable research on the role that fixed spatial features in the environment have on crime. These spatial risk factors have criminogenic qualities that make them “attractors” or “generators”(Brantingham and Brantingham 1995). Absent some change, these places typically contribute a disproportionate share of crime that is largely stable over time(Sherman, Gartin, and Buerger 1989). The classic example is a bar or night club. Alcohol plays a large role in a lot of crime, and locations where many people congregate and become intoxicated also have higher incidences of crime. We can use information about the environment to help solve problems or prioritize patrol areas.\nOne challenge in research is obtaining the point locations for these features. Generally when we perform some kind of spatial analysis we have a study area (e.g. a city or other boundary file) and a set of labeled point features corresponding to the locations of interest. However, reliable places to get this information is often hard to come by. Some cities provide open data portals with commercial information, but these are typically limited to larger cities. In my work I’ve had people ask how to get spatial risk factors for their research, often times for something related to the “Risk Terrain Modeling” approach of spatial analysis. I’ve worked on a few projects now where I’ve had to generate these myself, and have had some success using open data sources like Google to help with it."
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#querying-google-places",
    "href": "posts/rtm-spatial/rtm.html#querying-google-places",
    "title": "Generating Spatial Risk Features using R",
    "section": "Querying Google Places",
    "text": "Querying Google Places\nGoogle has a lot of paid API services which are quite useful for researchers. In most cases there is a free tier, and for smaller one-off projects this makes their API services attractive for research. Let’s walk through an example of how we might do this. For our example we will use the Swedish city of Malmö (which, incidentially is a very lovely city I’ve been lucky enough to visit). We have a shapefile that looks like this:\n\n\n\n\n\nOur goal is to query theGoogle Places API to get the locations of criminogenic spatial risk factors (here, bars and gas stations). One significant limitation with the Google Places API is that there is a limit to the number of locations that will show up for a single query. This means if you ran the query on the entire city, it would only return up to 20 locations. However, we can bypass this by running multiple queries on smaller spatial regions. Other bloggers have provided similar advice as well (see here and here).\nTo do the actual interfacing with the Google Places API we will use the very handy googleway package.\n\nSplitting up into a grid\n\n\nCode\n# GOOGLE PLACES API CODE\n# ================================================= #\n# Giovanni Circo\n# gmcirco42@gmail.com\n#\n# Code to query google place api\n# Divides a boundry shapefile into grid cells\n# of radius r, then queries the api for each cell\n#\n# NOTE:\n# Only requires a free version of the API. Doesn't\n# incur any costs.\n# ================================================= #\n\nlibrary(googleway)\nlibrary(sf)\nlibrary(tidyverse)\n\n# API Key\n# instructions here:\n# https://developers.google.com/maps/documentation/places/web-service/get-api-key\nmykey <- \"[INSERT GOOGLE PLACES API KEY HERE]\"\n\n# Location shapefile\nboundry <- st_read(\"...\\DeSo_Malmö.shp\")\n\n# specify grid size (meters)\nr <- 1200\n\n# Make a grid\nboundry_grid <- st_make_grid(boundry, cellsize = r)\nboundry_grid <- boundry_grid[boundry]\n\n# Transform to lat\\lon\n# Extract coords\narea_coords <- st_transform(boundry_grid, crs = 4326) %>%\n  st_centroid() %>%\n  st_coordinates() %>%\n  data.frame() %>%\n  select(lat = Y, lon = X)\n\n\nWe can divide the city into a series of grids, then iterate through each grid cell and query within it. This way we are more likely to obtain all of the relevant features in that grid cell without hitting the limit. Here, I create 150 1200 square meter grid cells, which gives us something like this:\n\n\n\n\n\nIn addition we extract the X-Y coordinates for the grid centroid, which we will use as our location for the API query. This means we hit the API 150 times, once for each grid cell. This is well within the free number that Google allows.\n\n\nQuerying our features\n\n\nCode\n# EXTRACT FEATURES ON GRID\n#----------------------------#\n\n# Supported place type names:\n# https://developers.google.com/maps/documentation/places/web-service/supported_types\n# Features: gas_station, bar, liquor_store, night_club, pharmacy, restaurant\n\n# Specify feature type\n# number of grid cells\nfeature <- \"bar\"\nn <- nrow(area_coords)\n\n# First, set up function to query google places\n# for each grid centroid\n# add to a list\n\narea_list <- list()\nfor(i in 1:n){\n  \n  area_list[[i]] <-\n    google_places(location = unlist(area_coords[i,]),\n                  place_type = feature,\n                  radius = r,\n                  key = mykey)\n}\n\n# Function to convert results from above\n# to a dataframe suitable for rbinding\n# then conversion to an sf object\nconvert_to_dataframe <-\n  function(x) {\n    \na <- x$results\n\nb <- tibble(\n  lat = a$geometry$location$lat,\n  lon = a$geometry$location$lng,\n  name = a$name,\n  types = a$types,\n  address = a$vicinity,\n  place_id = a$place_id\n)\n\nreturn(b)\n  }\n\n# Rbind the results, and then un-nest on feature type\n# This creates a long-form dataframe that you can then filter\n# based on feature type\narea_dataframe <-\n  do.call(rbind, lapply(area_list, convert_to_dataframe)) %>%\n  distinct(place_id, .keep_all = TRUE) %>%\n  unnest(types)\n\n# Get just feature requested\nfeature_out <- area_dataframe %>%\n  filter(types %in% feature) %>%\n  distinct(address, .keep_all = TRUE)\n\n\nWe can use the code above to iterate through each grid cell, hit the API, and then store the results in a list. I include a few helper functions to assist with pulling out the names and coordinates, binding them into a dataframe, and setting them up to export. The key bit of code is below, which is the part that queries the API for each of the grid cells:\n\narea_list <- list()\nfor(i in 1:n){\n  \n  area_list[[i]] <-\n    google_places(location = unlist(area_coords[i,]),\n                  place_type = feature,\n                  radius = r,\n                  key = mykey)\n}"
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#calculating-grid-cell-distances",
    "href": "posts/rtm-spatial/rtm.html#calculating-grid-cell-distances",
    "title": "Generating Spatial Risk Features using R",
    "section": "Calculating Grid Cell Distances",
    "text": "Calculating Grid Cell Distances\n\n\nCode\ncompute_distance <- function(grid, feature){\n  # get nearest point from grid to feature\n  nearest <- st_nearest_feature(grid,feature)\n  nearest_dist <- st_distance(grid, feature[nearest,], by_element = TRUE)\n  \n  return(nearest_dist)\n}\n\n# specify grid size (meters)\nr <- 250\n\n# Make a city grid\ncity <- st_make_grid(boundry, cellsize = r, square = FALSE)\ncity <- city[boundry]\n\n# get distances\nbar_dist <- compute_distance(city, bar)\ngas_dist <- compute_distance(city, gas)\n  \n# create long-form dataframe\ntbl <- tibble(city) %>%\n  mutate(bar = bar_dist,\n         gas_station = gas_dist) %>%\n  pivot_longer(-city, \n               names_to = \"feature\", \n               values_to = \"dist\") %>%\n  mutate(dist = as.numeric(dist)) %>%\n  st_as_sf()\n\n\nNow that we have our city boundary and our spatial risk factors, all we need to do now is compute the distance from each grid cell to its nearest risk factor. In the end, what we will want is a dataframe with grid cell ids, and columns corresponding to distance to the nearest feature. After merging them, we can create a nice map like this - showing the location sand distances of our risk factors.\n\n\n\n\n\nYou can then use these features in other kinds of spatial risk models (for a great walk through, see (Wheeler and Steenbeek 2021).The big advantage of this approach is that you have the flexibility to implement any kind of model you want at this point - whether it is a conventional RTM model, or a boosted tree model."
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#full-code",
    "href": "posts/rtm-spatial/rtm.html#full-code",
    "title": "Generating Spatial Risk Features using R",
    "section": "Full Code",
    "text": "Full Code\n\n\nCode\n# GOOGLE PLACES API CODE\n# ================================================= #\n# Giovanni Circo\n# gmcirco42@gmail.com\n#\n# Code to query google place api\n# Divides a boundry shapefile into grid cells\n# of radius r, then queries the api for each cell\n#\n# NOTE:\n# Only requires a free version of the API. Doesn't\n# incur any costs.\n# ================================================= #\n\nlibrary(googleway)\nlibrary(sf)\nlibrary(tidyverse)\n\n# API Key\n# instructions here:\n# https://developers.google.com/maps/documentation/places/web-service/get-api-key\nmykey <- \"[INSERT GOOGLE PLACES API KEY HERE]\"\n\n# Location shapefile\nboundry <- st_read(\"DeSo_Malmö.shp\")\n\n# specify grid size (meters)\nr <- 1200\n\n# Make a grid\nboundry_grid <- st_make_grid(boundry, cellsize = r)\nboundry_grid <- boundry_grid[boundry]\n\n# Transform to lat\\lon\n# Extract coords\narea_coords <- st_transform(boundry_grid, crs = 4326) %>%\n  st_centroid() %>%\n  st_coordinates() %>%\n  data.frame() %>%\n  select(lat = Y, lon = X)\n\nplot(boundry_grid)\n  \n# EXTRACT FEATURES ON GRID\n#----------------------------#\n\n# Supported place type names:\n# https://developers.google.com/maps/documentation/places/web-service/supported_types\n# Features: gas_station, bar, liquor_store, night_club, pharmacy, restaurant\n\n# Specify feature type\n# number of grid cells\nfeature <- \"bar\"\nn <- nrow(area_coords)\n\n# First, set up function to query google places\n# for each grid centroid\n# add to a list\n\narea_list <- list()\nfor(i in 1:n){\n  \n  area_list[[i]] <-\n    google_places(location = unlist(area_coords[i,]),\n                  place_type = feature,\n                  radius = r,\n                  key = mykey)\n}\n\n# Function to convert results from above\n# to a dataframe suitable for rbinding\n# then conversion to an sf object\nconvert_to_dataframe <-\n  function(x) {\n    \na <- x$results\n\nb <- tibble(\n  lat = a$geometry$location$lat,\n  lon = a$geometry$location$lng,\n  name = a$name,\n  types = a$types,\n  address = a$vicinity,\n  place_id = a$place_id\n)\n\nreturn(b)\n  }\n\n# Rbind the results, and then un-nest on feature type\n# This creates a long-form dataframe that you can then filter\n# based on feature type\narea_dataframe <-\n  do.call(rbind, lapply(area_list, convert_to_dataframe)) %>%\n  distinct(place_id, .keep_all = TRUE) %>%\n  unnest(types)\n\n# Get just feature requested\nfeature_out <- area_dataframe %>%\n  filter(types %in% feature) %>%\n  distinct(address, .keep_all = TRUE)\n\n# Now export as a shapefile\n# re-assign the crs of the boundry shapefile\nfeature_out %>%\n  st_as_sf(coords = c('lon','lat'), crs = 4326) %>%\n  st_transform(crs = st_crs(boundry)) %>%\n  st_write(paste0(\"Desktop\\\\\",feature,\".shp\"))"
  },
  {
    "objectID": "posts/synth-impact/synth_impact.html#synthetic-controls-in-statistics",
    "href": "posts/synth-impact/synth_impact.html#synthetic-controls-in-statistics",
    "title": "Synthetic Controls and Small Areas",
    "section": "Synthetic Controls in Statistics",
    "text": "Synthetic Controls in Statistics\nAndrew Gelman recently covered a mildly controversial paper in criminology that suggested that a policy of “de-prosecution” by the Philadelphia District Attorney’s office resulted in an increase in homicides. This has sparked a lot of back-and-forth discussion on the appropriateness of the analysis and the kind of synthetic control method used. I’m not here to discuss any of these things, as many other smart people have already debated this to death (plus, given Hogan is reticent to release his data or code we may never really know exactly what he did).\nHowever, what I do want to discuss is something else Gelman wrote about on his blog:\n\nHogan and the others make comparisons, but the comparisons they make are to that weighted average of Detroit, New Orleans, and New York. The trouble is . . . that’s just 3 cities, and homicide rates can vary a lot from city to city. It just doesn’t make sense to throw away the other 96 cities in your data. The implied counterfactual is that if Philadelphia had continued post-2014 with its earlier sentencing policy, that its homicide rates would look like this weighted average of Detroit, New Orleans, and New York…\n\nWhat Gelman is talking about here is the commonly-used ADH approach (short for Abadie, Diamond, and Hainmueller). In this method you typically have one large “treated” area - such as a city or state - that implements some kind of policy. You then use other comparably large or similar areas to construct a synthetic version of your treated unit to estimate the counterfactual. It’s an appealing method because it is relatively simple to calculate, fairly transparent about where the weights come from, and has good overlap with more conventional difference-in-differences methods (non-negative weights with a sum-to-one constraint). So in a way, I don’t necessarily have the same issues that Gelman does, but he brings up a good point. By using only large aggregate units there is an inherent loss of information. In the ADH method we sort of assume that by matching closely on the outcome variable we can average over a lot of the confounders. Although in the ADH method you can also match on other covariates - but in my experience the vast majority of the synthetic control weights are derived solely from the pre-treatment outcomes.\n\nMicro-Synthetic Controls\nGelman further writes:\n\nMy understanding of a synthetic controls analysis went like this. You want to compare Philadelphia to other cities, but there are no other cities that are just like Philadelphia, so you break up the city into neighborhoods and find comparable neighborhoods in other cities . . . and when you’re done you’ve created this composite “city,” using pieces of other cities, that functions as a pseudo-Philadelphia. In creating this composite, you use lots of neighborhood characteristics, not just matching on a single outcome variable. And then you do all of this with other cities in your treatment group (cities that followed a de-prosecution strategy).\n\nWhich describes another approach that has grown in popularity, especially among criminologists. This so-called “micro-synthetic” approach constructs synthetic controls from many small pieces to comprise a larger treated piece. The classic criminological example might be that you have a neighborhood in your city with 20 census blocks that gets some focused deterrence intervention. You can use the remaining “untreated” census blocks in the city to use as composite pieces as part of the synthetic control. This approach is especially appealing because so much criminological research has a focus on small, disaggregated regions."
  },
  {
    "objectID": "posts/synth-impact/synth_impact.html#an-example-operation-impact-2014",
    "href": "posts/synth-impact/synth_impact.html#an-example-operation-impact-2014",
    "title": "Synthetic Controls and Small Areas",
    "section": "An Example: Operation Impact (2014)",
    "text": "An Example: Operation Impact (2014)\nAs a quick demo, here’s an example I presented for part of NIJ’s Smart Suite. The research question posed here is whether a surge in police activity New York City’s 47th precinct reduced assaults or robberies. There were a number of previous iterations of Operation Impact which an evaluation found a general decrease in crime(MacDonald, Fagan, and Geller 2016). The example here looks at a much later surge in 2014:\n\nThe data I organized for this example contains block-level census data from the American Community Survey, as well as monthly counts of some major crime categories (here: assault, burglary, motor vehicle theft, robbery, and larceny-theft). This is organized in a long-form dataset, which is indexed by precinct * geoid * month.\n\n\n\n\n \n  \n    precinct \n    geoid \n    impact \n    total_pop \n    total_hh \n    total_male \n    total_white \n    total_hispan \n    total_black \n    total_poverty \n    total_snap \n    month \n    assault \n    burglary \n    mvt \n    robbery \n    theft \n  \n \n\n  \n    1 \n    3.6061e+10 \n    0 \n    7315 \n    1090 \n    3539 \n    4594 \n    479 \n    332 \n    586 \n    14 \n    1 \n    0 \n    0 \n    0 \n    0 \n    5 \n  \n  \n    1 \n    3.6061e+10 \n    0 \n    7315 \n    1090 \n    3539 \n    4594 \n    479 \n    332 \n    586 \n    14 \n    2 \n    1 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    1 \n    3.6061e+10 \n    0 \n    7315 \n    1090 \n    3539 \n    4594 \n    479 \n    332 \n    586 \n    14 \n    3 \n    1 \n    3 \n    0 \n    1 \n    7 \n  \n  \n    1 \n    3.6061e+10 \n    0 \n    7315 \n    1090 \n    3539 \n    4594 \n    479 \n    332 \n    586 \n    14 \n    4 \n    0 \n    1 \n    0 \n    1 \n    6 \n  \n  \n    1 \n    3.6061e+10 \n    0 \n    7315 \n    1090 \n    3539 \n    4594 \n    479 \n    332 \n    586 \n    14 \n    5 \n    0 \n    1 \n    0 \n    0 \n    5 \n  \n  \n    1 \n    3.6061e+10 \n    0 \n    7315 \n    1090 \n    3539 \n    4594 \n    479 \n    332 \n    586 \n    14 \n    6 \n    0 \n    0 \n    0 \n    1 \n    3 \n  \n\n\n\n\n\nThe precinct we’re interested in, the 47th, is comprised of 44 census blocks which are each measured at 12 time points. The remainder of the census blocks in the dataset are part of our “donor” pool, which we can use for our synthetic control.\n\nApplying the ‘Microsynth’ approach\nThe R package microsynth does almost all of the heavy lifting(Robbins and Davenport 2021). Without getting too much into the weeds, the general idea here is that we want to re-weight all of our untreated (non-Operation Impact) census blocks in a way that makes them nearly - or exactly - identical to the census blocks in the 47th precinct. Microsynth accomplishes this much in the same way that surveys are weighted to approximate the population of interest. However, instead here we treat the 47th precinct as our “population” and estimate weights to apprxomiate the pre-treatment outcomes and covariates in the treated precinct. The full code to run the model is below:\n\nfit <-\n  microsynth(\n    nyc_impact,\n    idvar = 'geoid',\n    timevar = 'month',\n    intvar = 'impact',\n    start.pre = 1,\n    end.pre = 9,\n    end.post = 12,\n    match.out = c('assault', 'robbery'),\n    match.covar = c(\n      'total_pop',\n      'total_black',\n      'total_hispan',\n      'total_poverty',\n      'total_snap'),\n    result.var = c('assault', 'robbery'),\n    omnibus.var = c('assault', 'robbery'))\n\nAs a start, we can assess whether our synthetic control is appropriately balanced on pre-treatment differences. As we saw above, we matched on both time-varying and non time-varying covariates. Looking at the balance table below we see that we achieved exact balance on all our variables - which is quite good! This should give us more confidence that the outcomes we observe in the post period are due to the intervention, and not a result of systematic differences between treated and control units.\n\n\n\n\n \n  \n      \n    Targets \n    Weighted.Control \n    All.scaled \n  \n \n\n  \n    Intercept \n    23 \n    23 \n    23.00 \n  \n  \n    total_pop \n    77311 \n    77311 \n    89810.67 \n  \n  \n    total_black \n    44079 \n    44079 \n    23373.48 \n  \n  \n    total_hispan \n    15693 \n    15693 \n    26494.13 \n  \n  \n    total_poverty \n    24022 \n    24022 \n    18865.98 \n  \n  \n    total_snap \n    6558 \n    6558 \n    4875.89 \n  \n  \n    assault.9 \n    27 \n    27 \n    18.56 \n  \n  \n    assault.8 \n    35 \n    35 \n    20.69 \n  \n  \n    assault.7 \n    28 \n    28 \n    21.13 \n  \n  \n    assault.6 \n    26 \n    26 \n    21.10 \n  \n  \n    assault.5 \n    35 \n    35 \n    20.43 \n  \n  \n    assault.4 \n    30 \n    30 \n    17.44 \n  \n  \n    assault.3 \n    25 \n    25 \n    17.57 \n  \n  \n    assault.2 \n    15 \n    15 \n    15.14 \n  \n  \n    assault.1 \n    16 \n    16 \n    16.22 \n  \n  \n    robbery.9 \n    30 \n    30 \n    15.89 \n  \n  \n    robbery.8 \n    38 \n    38 \n    16.61 \n  \n  \n    robbery.7 \n    31 \n    31 \n    16.19 \n  \n  \n    robbery.6 \n    26 \n    26 \n    14.36 \n  \n  \n    robbery.5 \n    23 \n    23 \n    15.60 \n  \n  \n    robbery.4 \n    21 \n    21 \n    12.91 \n  \n  \n    robbery.3 \n    18 \n    18 \n    13.50 \n  \n  \n    robbery.2 \n    13 \n    13 \n    13.79 \n  \n  \n    robbery.1 \n    31 \n    31 \n    16.39 \n  \n\n\n\n\n\nWe can print out the results as well. Here we see that the observed number of assaults and robberies in the post-period were 77 and 53 for the 47th precinct, and 77.7 and 73.5 for the synthetic control, respectively. In the case of robbery we estimate that Operation Impact resulted in about a 28% decrease in robberies for the 3-month period in 2014.\n\n\n\n\n\n  \n    \n\n\n \n  \n      \n    Trt \n    Con \n    Pct.Chng \n    Linear.pVal \n    Linear.Lower \n    Linear.Upper \n  \n \n\n  \n    assault \n    77 \n    77.70 \n    -0.01 \n    0.94 \n    -0.19 \n    0.22 \n  \n  \n    robbery \n    53 \n    73.54 \n    -0.28 \n    0.03 \n    -0.45 \n    -0.05 \n  \n  \n    Omnibus \n    NA \n    NA \n    NA \n    0.04 \n    NA \n    NA \n  \n\n\n\n \n  \n\n\n\n\n\nIt’s also helpful to visualize what this looks like. Looking at the results we see that most of the decrease in robberies occurred immediately after the start of the program. For assaults there’s a slight dip early on, but the overall results are more mixed.\n\n\nCode\nplot_microsynth(fit)"
  },
  {
    "objectID": "posts/synth-impact/synth_impact.html#full-code",
    "href": "posts/synth-impact/synth_impact.html#full-code",
    "title": "Synthetic Controls and Small Areas",
    "section": "Full Code",
    "text": "Full Code\n\n\nCode\nlibrary(microsynth)\n\nset.seed(1)\n\n# note: a good vignette is provided here:\n# https://cran.r-project.org/web/packages/microsynth/vignettes/introduction.html\n\n\n# data comes from NYC open data and US Census bureau, compiled by me\n# dependent variable is the number of assaults and robberies in Q3 2014\n# when time > 9\n\n# census-level variables from the ACS 2014 5-year estimates\n# and are presented as raw counts, to facilitate weighting\n# b/c microsynth uses survey weighting via survey::calibrate()\n\n# file url\ndf <- url('https://www.dropbox.com/s/08owr5710bnvxn0/nyc_impact_long.csv?raw=1')\n\n# read into R\nnyc_impact <- read.csv(df)\n\n# MICROSYNTH\n#-----------------------#\n\n# model 1, without permutation-based inference\n# test statistics are calculated as weighted linear model\n\n# each microsynth needs the following:\n# idvar = variable identifying observations\n# timevar = variable indexing observations by time\n# intvar = variable that takes on 1 for treated units, post treatment\n#   is 0 otherwise\n# start.pre, end.pre, end.post define the start of the study period, the end\n#   of the pre-period, and the end of the post period\n# match.out = the time-varying variables that are to be matched exactly\n# match.cov = the time-invariant variables to be matched exactly\n# result.var = the outcome variable(s) of interest\n# omnibus.var = the outcome variable(s) that should be used in the calculation \n#   of an omnibus p-value\n\nfit <-\n  microsynth(\n    nyc_impact,\n    idvar = 'geoid',\n    timevar = 'month',\n    intvar = 'impact',\n    start.pre = 1,\n    end.pre = 9,\n    end.post = 12,\n    match.out = c('assault', 'robbery'),\n    match.covar = c(\n      'total_pop',\n      'total_black',\n      'total_hispan',\n      'total_poverty',\n      'total_snap'),\n    result.var = c('assault', 'robbery'),\n    omnibus.var = c('assault', 'robbery'))\n\n# get the model summary\nsummary(fit)\n\n# plot treated vs synthetic\n# and gap plot of treated - synthetic\nplot_microsynth(fit)\n\n\n# PLACEBO-BASED INFERENCE\n#-----------------------#\n\n# this model is same as above, except we are calculating permutation p-values\n# here, I set the number of permutations to just 100, but ideally you\n# can set this higher. The more permutations you set, the longer the run time\n\nfit2 <-\n  microsynth(\n    nyc_impact,\n    idvar = 'geoid',\n    timevar = 'month',\n    intvar = 'impact',\n    start.pre = 1,\n    end.pre = 9,\n    end.post = 12,\n    match.out = c('assault', 'robbery'),\n    match.covar = c(\n      'total_pop',\n      'total_black',\n      'total_hispan',\n      'total_poverty',\n      'total_snap'),\n    result.var = c('assault', 'robbery'),\n    omnibus.var = c('assault', 'robbery'),\n    perm = 100)\n\nsummary(fit2)\nplot_microsynth(fit2)"
  }
]