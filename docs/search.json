[
  {
    "objectID": "posts/tiny-recid/recid.html",
    "href": "posts/tiny-recid/recid.html",
    "title": "Don’t Evaluate Your Model On a SMOTE Dataset",
    "section": "",
    "text": "I recently found a paper published called “Advancing Recidivism Prediction for Male Juvenile Offenders: A Machine Learning Approach Applied to Prisoners in Hunan Province”. In it, the authors make use of a very small recidivism data set focusing on youth in Hunan province, which originally appears in a 2017 PLOS ONE article here: “Predicting Reoffending Using the Structured Assessment of Violence Risk in Youth (SAVRY): A 5-Year Follow-Up Study of Male Juvenile Offenders in Hunan Province, China”.\nThe authors of the new study explain how they can use machine learning to improve the prediction of recidivism (which, in of itself is a highly contentious topic). In general, it is a pretty harmless paper of virtually zero significance. They’re using an absurdly tiny data set to test out machine learning models that only really work well when you are flush with data. However, a single line stuck out to me when I was scanning their paper:\n\n“The proposed ML models perform best on the oversampled dataset, as illustrated in Fig. 2.”\n\nUh oh.\nLooking at the associated figure and table they show off some impressive metrics. Their random forest model has a precision and recall of 97%! On a recidivism prediction task this alone is highly suspicious. For example, in the competition I participated (and won in several categories), our model only averaged about .78 to .8. Why is theirs so good here? Well, it’s all about that line above. Let’s run through the data and explain:\n\n\n\nBelow, I pull the original data set from the PLOS One paper (thanks to the authors for making it publically available!). There is a single missing value for education, which I impute to the median value.\n\nlibrary(tidyverse)\nlibrary(pROC)\nlibrary(glmnet)\nlibrary(randomForest)\nlibrary(smotefamily)\n\nset.seed(978545)\n\n# load data\n# impute single missing value with median of education (8)\ndf &lt;-\n  haven::read_sav(\"../../../data/savry.sav\") %&gt;%\n  select(Reoffending, age, education, familyincome, contains(\"SAVRY\"), P1:P6) %&gt;%\n  replace_na(list(education = 8)) %&gt;%\n  mutate(across(familyincome:P6, as.factor))\n\n\nTEST_PROP = .2\nN = nrow(df)"
  },
  {
    "objectID": "posts/tiny-recid/recid.html#the-paper",
    "href": "posts/tiny-recid/recid.html#the-paper",
    "title": "Don’t Evaluate Your Model On a SMOTE Dataset",
    "section": "",
    "text": "I recently found a paper published called “Advancing Recidivism Prediction for Male Juvenile Offenders: A Machine Learning Approach Applied to Prisoners in Hunan Province”. In it, the authors make use of a very small recidivism data set focusing on youth in Hunan province, which originally appears in a 2017 PLOS ONE article here: “Predicting Reoffending Using the Structured Assessment of Violence Risk in Youth (SAVRY): A 5-Year Follow-Up Study of Male Juvenile Offenders in Hunan Province, China”.\nThe authors of the new study explain how they can use machine learning to improve the prediction of recidivism (which, in of itself is a highly contentious topic). In general, it is a pretty harmless paper of virtually zero significance. They’re using an absurdly tiny data set to test out machine learning models that only really work well when you are flush with data. However, a single line stuck out to me when I was scanning their paper:\n\n“The proposed ML models perform best on the oversampled dataset, as illustrated in Fig. 2.”\n\nUh oh.\nLooking at the associated figure and table they show off some impressive metrics. Their random forest model has a precision and recall of 97%! On a recidivism prediction task this alone is highly suspicious. For example, in the competition I participated (and won in several categories), our model only averaged about .78 to .8. Why is theirs so good here? Well, it’s all about that line above. Let’s run through the data and explain:\n\n\n\nBelow, I pull the original data set from the PLOS One paper (thanks to the authors for making it publically available!). There is a single missing value for education, which I impute to the median value.\n\nlibrary(tidyverse)\nlibrary(pROC)\nlibrary(glmnet)\nlibrary(randomForest)\nlibrary(smotefamily)\n\nset.seed(978545)\n\n# load data\n# impute single missing value with median of education (8)\ndf &lt;-\n  haven::read_sav(\"../../../data/savry.sav\") %&gt;%\n  select(Reoffending, age, education, familyincome, contains(\"SAVRY\"), P1:P6) %&gt;%\n  replace_na(list(education = 8)) %&gt;%\n  mutate(across(familyincome:P6, as.factor))\n\n\nTEST_PROP = .2\nN = nrow(df)"
  },
  {
    "objectID": "posts/tiny-recid/recid.html#doing-it-the-normal-way",
    "href": "posts/tiny-recid/recid.html#doing-it-the-normal-way",
    "title": "Don’t Evaluate Your Model On a SMOTE Dataset",
    "section": "Doing it the Normal Way",
    "text": "Doing it the Normal Way\nThe normal workflow in this case is to set up a test-train split, fit a model on the training data set, then evaluate the out-of-sample performance on the test set. Despite the very small sample size here I’ll just follow the bog standard approach to illustrate. A 20% test set gives us: \\(246 - (246*.2) = 196.8\\). So just under 200 cases to train on, which is really quite small.\n\n\nCode\n# indices of train-test\ntest_size &lt;- round(N*TEST_PROP)\ntest_idx &lt;- sample(1:N, test_size)\ntrain_idx &lt;- setdiff(1:N, test_idx)\n\n# set up y and X\n# one-hot encoding categorical vars\ny &lt;- df$Reoffending\nX &lt;- model.matrix(~ . - 1, data = df[-1])\n\n# test-train splits\ny_test = y[test_idx]\nX_test = X[test_idx,]\ny_train = y[train_idx]\nX_train = X[train_idx,]\n\n\nDespite the authors of the previous paper using some boosting methods, this data is far, far too small to make use of those approaches usefully. Here, I’m just fitting a logistic regression with no regularization and a random forest with 500 trees.\n\n# fit multiple linear models\n# logit, no regularization & random forest\nfit_1_glm &lt;- glmnet(X_train,y_train, family = \"binomial\", alpha = 0, lambda = 0)\nfit_1_rf &lt;- randomForest(X_train ,as.factor(y_train))\n\nNow we just evaluate the area under the curve:\n\npred_1_glm &lt;- as.numeric(predict(fit_1_glm, X_test, type = 'response'))\npred_1_rf &lt;- as.numeric(predict(fit_1_rf, X_test, \"prob\")[,2])\n\n# get auc\nroc(y_test, pred_1_glm, quiet = T)$auc\n\nArea under the curve: 0.7482\n\nroc(as.factor(y_test), pred_1_rf, quiet = T)$auc\n\nArea under the curve: 0.7197\n\n\nSo about .74 for the logistic regression and .72 for the random forest. Not great, not terrible."
  },
  {
    "objectID": "posts/tiny-recid/recid.html#doing-it-with-smote",
    "href": "posts/tiny-recid/recid.html#doing-it-with-smote",
    "title": "Don’t Evaluate Your Model On a SMOTE Dataset",
    "section": "Doing it with SMOTE",
    "text": "Doing it with SMOTE\nSo the argument with SMOTE is that training models on data sets with very large imbalances in positive vs. negative cases is that the models only learn from the negative cases and not the positive ones. A good example might be a fraud data set where you have 100,000 legitimate credit card transactions and only 500 cases of fraud (so something like .5%). SMOTE is intended to help with training a model by synthesizing a balanced data set where the ratio of positive to negative cases are much closer to 50/50. Without going too much into it, this actually rarely solves and problems and often induces some.\nWhat I suspect the authors of this paper did is that they generated a SMOTE data set with a balanced ratio of positive to negative cases, then created a test-train split from that data set, and evaluated their metrics on a test data set derived from the SMOTE model. That is very, very wrong.\n\nDoing SMOTE the wrong way\nSo let’s try it. I’ll synthesize a SMOTE data set from the full set of cases, then walk through the whole process using only the synthesized data. This creates a data set with 126 new observations, and brings the balance of positive to negative cases to almost exactly 50/50 (rather than 25/75 in the original).\n\n# create a smote dataset from the FULL dataset, then split\nsmote_df_full &lt;- data.frame(X,y)\nsmote_model_full &lt;- SMOTE(smote_df_full[-63], target = smote_df_full[63], dup_size = 2)\n\nX_smote &lt;- smote_model_full$data[-63]\ny_smote &lt;- as.numeric(smote_model_full$data$class)\n\ntable(y_smote)\n\ny_smote\n  0   1 \n183 189 \n\n\nNow we just pull a test-train split on the SMOTE data, then fit the models.\n\n# indices of train-test\ntest_idx_smote &lt;- sample(1:N, test_size)\ntrain_idx_smote &lt;- setdiff(1:N, test_idx_smote)\n\n# test-train splits\ny_test_smote = y_smote[test_idx_smote]\nX_test_smote = as.matrix(X_smote[test_idx_smote,])\ny_train_smote = y_smote[train_idx_smote]\nX_train_smote = as.matrix(X_smote[train_idx_smote,])\n\n# fit and evaluate models\nfit_2_glm_smote &lt;- glmnet(X_train_smote,y_train_smote, family = \"binomial\", alpha = 0, lambda = 0)\nfit_2_rf_smote &lt;- randomForest(X_train_smote ,as.factor(y_train_smote))\n\npred_2_glm_smote &lt;- as.numeric(predict(fit_2_glm_smote, X_test_smote, type = 'response'))\npred_2_rf_smote &lt;- as.numeric(predict(fit_2_rf_smote, X_test_smote, \"prob\")[,2])\n\n# get auc\nroc(y_test_smote, pred_2_glm_smote, quiet = T)$auc\n\nArea under the curve: 0.8408\n\nroc(as.factor(y_test_smote), pred_2_rf_smote, quiet = T)$auc\n\nArea under the curve: 0.9765\n\n\nWow! Look at that, we just increased our AUC for the random forest model from .72 to .97! But what if we do what we’re supposed to and see how it works on real out-of sample data?\n\n\nDoing SMOTE the (less) wrong way\nSame as above, except we create a SMOTE data set from our original training data and then we evaluate our model on the original test data set that is not synthetically balanced.\n\nsmote_df &lt;- data.frame(X_train,y_train)\nsmote_model &lt;- SMOTE(smote_df[-63], target = smote_df[63], dup_size = 2)\n\nX_smote &lt;- smote_model$data[-63]\ny_smote &lt;- as.numeric(smote_model$data$class)\n\nfit_3_glm_smote &lt;- glmnet(X_smote,y_smote, family = \"binomial\", alpha = 0, lambda = 0)\nfit_3_rf_smote &lt;- randomForest(X_smote ,as.factor(y_smote))\n\npred_3_glm_smote &lt;- as.numeric(predict(fit_3_glm_smote, X_test, type = 'response'))\npred_3_rf_smote &lt;- as.numeric(predict(fit_3_rf_smote, X_test, \"prob\")[,2])\n\nroc(y_test, pred_3_glm_smote, quiet = T)$auc\n\nArea under the curve: 0.7077\n\nroc(as.factor(y_test), pred_3_rf_smote, quiet = T)$auc\n\nArea under the curve: 0.7335\n\n\nOh."
  },
  {
    "objectID": "posts/tiny-recid/recid.html#summary",
    "href": "posts/tiny-recid/recid.html#summary",
    "title": "Don’t Evaluate Your Model On a SMOTE Dataset",
    "section": "Summary",
    "text": "Summary\nIn summary, if you wrongfully evaluate your model that was trained on a SMOTE data set against a hold-out sample from that same SMOTE data your out-of-sample metrics will be falsely confident. It is much easier to perform classification on data that are artificially balanced. However, actually using these models in real life entails data that almost never follow this.\nFinally, I don’t mean to focus on these authors specifically. The analysis they are doing is with some good intentions, but is mostly misguided. The data here are mostly unsuited for examining more complex models and processes. In addition, what I see here is a common issue for many data analysts, which is why being thoughtful and careful at the start of your analysis is very important."
  },
  {
    "objectID": "posts/spatial-simulation/spatial-sim.html",
    "href": "posts/spatial-simulation/spatial-sim.html",
    "title": "Creating Synthetic Spatial Data",
    "section": "",
    "text": "In my work as a data scientist I have been working increasingly more with synthetic data generation. Synthetic data can be very useful when you are working on products that need reasonable stand-in values to test deployment. For example, a common issue I see is that a team needs data to populate a SQL table so they can populate a demo dashboard. Many times waiting for real data will take too long and unnecessarily stretch out the development time.\nNow, when we talk about generating synthetic data we’re talking about more than just inserting random values into a table. Good synthetic data should mirror the properties of the original (or the properties of some pre-defined requirements). For example, you may want to have an age and race field that match the same demographics of the U.S. Census. Or you might need the relationship between two fields to be consistent - like a medical procedure field matched with a cost field.\nAn interesting question I posed to myself was “What if I wanted to generate synthetic spatial data?” This is an interesting question, because spatial data need to have additional properties matched. Of course if we want to simulate crimes in a city, we can’t just randomly throw points on the map. Crime does not occur randomly, and so if we want to simulate realistic processes, we need methods that generate them accurately.\n\n\nMy idea here is to try and simulate many different realizations of gas station locations in the region. We don’t necessarily care if they exactly match a real location, but we care more about the intensity of the pattern. The data I’m using is some Hartford crime data that I collected for my quickGrid package.\nBelow is the locations of gas stations (blue) and robbery incidents (red).\n\n\nCode\n# plot robbery ~ gas\nggplot() +\n  geom_sf(data = hartford, fill = 'grey90', color = 'white') +\n  geom_sf(data = robbery, color = \"#EE6677\", size = 1, alpha = .5) +\n  geom_sf(data = gas, color = \"#4477AA\", size = 2) +\n  theme_void()\n\n\n\n\n\nRobberies and gas stations, Hartford CT (2017)\n\n\n\n\nWhat we want to do is take this information and simulate, many times, different synthetic realizations of this pattern (gas stations and robberies). In practice we could use these simulated datasets to test a statistical method, populate some dashboard or map using de-identified data, or as a step in generating some new model."
  },
  {
    "objectID": "posts/spatial-simulation/spatial-sim.html#synthetic-data",
    "href": "posts/spatial-simulation/spatial-sim.html#synthetic-data",
    "title": "Creating Synthetic Spatial Data",
    "section": "",
    "text": "In my work as a data scientist I have been working increasingly more with synthetic data generation. Synthetic data can be very useful when you are working on products that need reasonable stand-in values to test deployment. For example, a common issue I see is that a team needs data to populate a SQL table so they can populate a demo dashboard. Many times waiting for real data will take too long and unnecessarily stretch out the development time.\nNow, when we talk about generating synthetic data we’re talking about more than just inserting random values into a table. Good synthetic data should mirror the properties of the original (or the properties of some pre-defined requirements). For example, you may want to have an age and race field that match the same demographics of the U.S. Census. Or you might need the relationship between two fields to be consistent - like a medical procedure field matched with a cost field.\nAn interesting question I posed to myself was “What if I wanted to generate synthetic spatial data?” This is an interesting question, because spatial data need to have additional properties matched. Of course if we want to simulate crimes in a city, we can’t just randomly throw points on the map. Crime does not occur randomly, and so if we want to simulate realistic processes, we need methods that generate them accurately.\n\n\nMy idea here is to try and simulate many different realizations of gas station locations in the region. We don’t necessarily care if they exactly match a real location, but we care more about the intensity of the pattern. The data I’m using is some Hartford crime data that I collected for my quickGrid package.\nBelow is the locations of gas stations (blue) and robbery incidents (red).\n\n\nCode\n# plot robbery ~ gas\nggplot() +\n  geom_sf(data = hartford, fill = 'grey90', color = 'white') +\n  geom_sf(data = robbery, color = \"#EE6677\", size = 1, alpha = .5) +\n  geom_sf(data = gas, color = \"#4477AA\", size = 2) +\n  theme_void()\n\n\n\n\n\nRobberies and gas stations, Hartford CT (2017)\n\n\n\n\nWhat we want to do is take this information and simulate, many times, different synthetic realizations of this pattern (gas stations and robberies). In practice we could use these simulated datasets to test a statistical method, populate some dashboard or map using de-identified data, or as a step in generating some new model."
  },
  {
    "objectID": "posts/spatial-simulation/spatial-sim.html#simulating-gas-station-locations",
    "href": "posts/spatial-simulation/spatial-sim.html#simulating-gas-station-locations",
    "title": "Creating Synthetic Spatial Data",
    "section": "Simulating Gas Station Locations",
    "text": "Simulating Gas Station Locations\nFor this example we’ll start with gas stations. Ideally we want to simulate an Inhomogeneous Poisson point process. In simple terms, this means that the intensity of the point pattern \\(lambda\\) is not constant across the study region. Logically this makes sense because gas stations are typically confined to commercial areas and don’t appear randomly in the middle of parks or waterways.\nIn R this is easy to do. We can calculate the intensity of gas station locations by computing the kernel density of observed locations, and then use those density values as input for our simulation. We can do the same for robberies as well.\n\n\nCode\n# convert to ppp for spatial stuff\nhartford_sp &lt;- as.owin(hartford)\n\ngas_ppp &lt;- as.ppp(gas, W = hartford_sp)\nmarks(gas_ppp) &lt;- \"gas\"\n\nrobbery_ppp &lt;- as.ppp(robbery, W = hartford_sp)\nmarks(robbery_ppp) &lt;- \"robbery\"\n\ngas_robbery_ppp &lt;- superimpose(gas_ppp, robbery_ppp, W = hartford_sp)\nmarks(gas_robbery_ppp) &lt;- factor(marks(gas_robbery_ppp))\n\n\n# calculate the density of gas stations & robberies\n# replace negative values with 0\ngas_density &lt;- density.ppp(gas_ppp, sigma = 750)\ngas_density[gas_density &lt; 0] &lt;- 0\n\nrobbery_density &lt;- density.ppp(robbery_ppp, sigma = 500)\nrobbery_density[robbery_density &lt; 0] &lt;- 0\n\n\nWe can plot the density of gas station locations. In this case the density values are based on points per unit (so gas stations per square foot).\n\nCode\nplot(gas_density, main = \"Gas Station Density\")\nplot(robbery_density, main = \"Robbery Density\")\n\n\n\n\n\n\n\nKernel density estimate of gas stations\n\n\n\n\n\n\n\nKernel density estimate of robberies\n\n\n\n\n\n\nCalculating K-function\nAs we simulate the position of gas stations in the city, we will want to make sure they are relatively consistent with the patterns observed in reality. For example: we wouldn’t expect all 50ish gas stations to be right on top of each other - nor would we expect to see them scattered randomly. What we can do is compute the clustering intensity of the observed point pattern, and then compare that to our simulations.\n\n\nCode\nN_SIM &lt;- 100\nN_GAS &lt;- nrow(gas)\n\n# observed K-function for gas stations\ngas_kest &lt;- Kest(gas_ppp)\n\n# simulate gas stations\n# generate 100 simulations\ngas_sim &lt;- rpoispp(gas_density,nsim = N_SIM)\nrobbery_sim &lt;- rpoispp(robbery_density, nsim = N_SIM)\n\n\nThis gives us the observed K function for gas stations.\n\n\nCode\nplot(gas_kest)\n\n\n\n\n\n\n\n\n\nNow we can simulate. So we’re going to take the observed density values as probabilities for an inhomogeneous Poisson point process.\n\n\nCode\n# simulate gas stations\n# generate 100 simulations\ngas_sim &lt;- rpoispp(gas_density,nsim = N_SIM)\n\n# get N gas stations generated\n# and intensity of generated function\nsim_N &lt;- sapply(gas_sim, function(x){as.numeric(x$n) })\nsim_L &lt;- lapply(gas_sim, Kest, correction = \"border\", r = gas_kest$r)\n\n# plot envelopes against observed\nX &lt;- sapply(sim_L, function(x){x$border})\n\nXdf &lt;- as.data.frame(X)\nXdf$r &lt;- gas_kest$r\n\n\n# Plot observed K against simulations of K\nobs_K &lt;- data.frame(r = gas_kest$r,\n                    K = gas_kest$border)\n\n\nThis plots the minimum and maximum envelopes (in grey) of the simulations against the observed K values in red. In general, we see that the pairwise relationships between gas stations is fairly close to observed, expect at small spatial scales. We appear to be failing to simulate cases where many gas stations are near each other (such as at 4 way intersections with a station on each corner).\n\n\nCode\nXdf %&gt;%\n  pivot_longer(-r, names_to = \"simulation\", values_to = \"K\") %&gt;%\n  group_by(r) %&gt;%\n  summarise(Kmin = min(K),\n            Kmax = max(K)) %&gt;%\n  ggplot() +\n  geom_ribbon(aes(x = r, ymin = Kmin, ymax = Kmax), alpha = .3) +\n  geom_line(data = obs_K, aes(x = r, y = K), linewidth = 1, color = 'red') + \n  labs(y = 'K(Border)', x = 'Distance (feet)') +\n  theme_bw()\n\n\n\n\n\n100 Simulation envelopes (grey) compared to observed K (red)\n\n\n\n\nWe can also plot the points directly and see what they look like:\n\n\nCode\n#| warning: false\nsim_gas_points &lt;-\n  st_as_sf(\n    data.frame(x = gas_sim$`Simulation 10`$x, y = gas_sim$`Simulation 10`$y),\n    coords = c(\"x\", \"y\")\n  ) %&gt;%\n  `st_crs&lt;-`(. , st_crs(gas))\n\n\nggplot() +\n  geom_sf(data = hartford, fill = 'grey90', color = 'white') +\n  geom_sf(data = robbery, color = \"#EE6677\", size = 1, alpha = .3) +\n  geom_sf(data = sim_gas_points, color = \"#4477AA\", size = 2) +\n  theme_void()\n\n\n\n\n\nObserved robberies and simulated gas station locations"
  },
  {
    "objectID": "posts/spatial-simulation/spatial-sim.html#pairwise-simulation",
    "href": "posts/spatial-simulation/spatial-sim.html#pairwise-simulation",
    "title": "Creating Synthetic Spatial Data",
    "section": "Pairwise Simulation",
    "text": "Pairwise Simulation\nNaturally, it makes sense to use simulations of both robberies and gas stations to create our simulated crime and location data. We should check the cross-K function between our simulated gas stations and simulated robberies. In these cases it is often easier to assess this by performing a transformation of the K function to the variance-stabilized L function. If we subtract the distance at each value of L we get the L-cross - r which is a handy way to visualize a point process.\n\n\nCode\n# compute observed L-function\ngas_robbery_cross &lt;-\n  sf_to_multitype(\n    window = hartford,\n    i = robbery,\n    j = gas,\n    i_name = \"robbery\",\n    j_name = \"gas\"\n  )\n\ngas_robbery_lest &lt;- Lcross(gas_robbery_cross, i = \"gas\", j = \"robbery\", r = gas_kest$r,  correction = \"border\")\n\nobs_L &lt;- data.frame(r = gas_robbery_lest$r,\n                    L = gas_robbery_lest$border) %&gt;%\n  mutate(L = L-r)\n\n\n# gather simulations, plot L function\nsim_list &lt;- lapply(gas_sim, sp_to_sf, crs = st_crs(gas))\nsim_list2 &lt;- lapply(robbery_sim, sp_to_sf, crs = st_crs(gas))\n\nsim_cross_list &lt;- list()\n\nfor(sim in 1:N_SIM){\n  sim_cross_points &lt;-\n    sf_to_multitype(\n      window = hartford,\n      i = sim_list2[[sim]],\n      j = sim_list[[sim]],\n      i_name = \"robbery\",\n      j_name = \"gas\"\n    )\n  sim_cross_list[[sim]] &lt;- sim_cross_points\n}\n\n\n# compute lcross border corrected\nlcross_list &lt;- lapply(sim_cross_list, Lcross, i = \"gas\", j = \"robbery\", r = gas_kest$r,  correction = \"border\")\n\n# plot envelopes against observed\nX_l &lt;- sapply(lcross_list, function(x){x$border})\n\nX_ldf &lt;- as.data.frame(X_l)\nX_ldf$r &lt;- gas_kest$r\n\n\n# plot \nX_ldf %&gt;%\n  pivot_longer(-r, names_to = \"simulation\", values_to = \"L\") %&gt;%\n  mutate(L = L-r) %&gt;%\n  group_by(r) %&gt;%\n  summarise(Lmin = min(L),\n            Lmax = max(L)) %&gt;%\n  ggplot() +\n  geom_ribbon(aes(x = r, ymin = Lmin, ymax = Lmax), alpha = .3) +\n  geom_line(data = obs_L, aes(x = r, y = L), linewidth = 1, color = 'red') + \n  geom_hline(yintercept = 0) +\n  labs(y = 'L-r(Border)', x = 'Distance (feet)') +\n  theme_bw()\n\n\n\n\n\n100 Simulation envelopes (grey) compared to observed L-r (red)\n\n\n\n\nAnd this is why it is important to check. We can clearly see that while our simulated data is fairly close to the observed pattern for most distances 1000 feet and beyond, there is much less clustering at small spatial scales. This makes sense, because of how crime is often geo-coded. When you have crimes that occur directly on the location they will typically share the same coordinates. In our case, our points are more spread out than we would expect to see in real life.\nWhether or not this data is “good enough” will depend on the use case at hand. In many cases very rough synthetic data can suffice for many purposes. In other cases, where high-fidelity synthetic data is needed, considerably more post-processing is required to bring the synthetic data in line with the real."
  },
  {
    "objectID": "posts/presidential-polls-2024/election.html",
    "href": "posts/presidential-polls-2024/election.html",
    "title": "I Made An Election Model",
    "section": "",
    "text": "The 2024 election is over. Donald Trump beat Kamala Harris by about 2 million popular votes and 86 electoral votes. All the posturing, forecasting, and betting is over. So why am I posting about the election now?\nWell, at the outset of the election I became interesting in running my own forecasting model. Nowadays there are plenty of folks running their own forecasting models - from individual bloggers to big news agencies like the New York Times. Poll aggregation is one of those things that is very “Bayesian” and I learned a lot about it reading through various books. To boot, I’ve always been a big fan of Andrew Gelman, as well as some other folks who do election and public policy forecasting - like Nate Silver and Elliot Morris. So I thought, “what the hell” and threw my hat into the ring. I held off on posting this because I figured there was already enough noise out there about the election, and maybe the world didn’t need one more person adding to the cacophony."
  },
  {
    "objectID": "posts/presidential-polls-2024/election.html#revisiting-the-2024-election",
    "href": "posts/presidential-polls-2024/election.html#revisiting-the-2024-election",
    "title": "I Made An Election Model",
    "section": "",
    "text": "The 2024 election is over. Donald Trump beat Kamala Harris by about 2 million popular votes and 86 electoral votes. All the posturing, forecasting, and betting is over. So why am I posting about the election now?\nWell, at the outset of the election I became interesting in running my own forecasting model. Nowadays there are plenty of folks running their own forecasting models - from individual bloggers to big news agencies like the New York Times. Poll aggregation is one of those things that is very “Bayesian” and I learned a lot about it reading through various books. To boot, I’ve always been a big fan of Andrew Gelman, as well as some other folks who do election and public policy forecasting - like Nate Silver and Elliot Morris. So I thought, “what the hell” and threw my hat into the ring. I held off on posting this because I figured there was already enough noise out there about the election, and maybe the world didn’t need one more person adding to the cacophony."
  },
  {
    "objectID": "posts/presidential-polls-2024/election.html#my-model",
    "href": "posts/presidential-polls-2024/election.html#my-model",
    "title": "I Made An Election Model",
    "section": "My Model",
    "text": "My Model\nTo keep it simple, I focused on the simplest kind of model - a poll aggregation model estimating national vote for the two-party candidates. This approach avoids a lot of additional complexity of estimating electoral college votes by not having to model state-level vote share, correlations between states, and many other things. This was truly just meant for a bit of academic fun. As a source of data, I used the very nicely curated tables supplied by 538 here, which was continuously updated during the election cycle. Helpfully, this data contains some other useful variables such as information about pollsters, and 538’s own ranking of pollster’s reliability.\nFor my model I applied filtering to get a subset of polls:\n\nOnly polls that included a Trump-Harris match up\nPolls that were conducted just before Biden dropped out (6/1/2024)\nOnly polls including likely voters, registered voters, or adults\nRemoved overlapping polls\nPolls with larger than 5,000 respondents\n\nDropping polls with more than 5,000 respondents is largely an approach to remove pollsters who “dredge” for lots of responses that are often typically of low quality. In total, this leaves us with about 255 unique polls in the 165 days between 6/1/2024 and 11/4/2024.\n\nThe poll aggregation approach\nFor the modelling approach I drew some inspiration from Andrew Gelman and others. Most of the current poll aggregation models apply some form of hierarchical linear modelling (HLM), combined with time trends, and often a “fundamentals” component. The general idea here is to partially pool information from many different pollsters, who are all utilizing slightly different samples of the population, with slightly different approaches.\nFor any individual poll we want to weight its contribution to the daily estimate based on known variables that affect responses (e.g: is this a partisan poll? is the poll registered or likely voters?), as well as “random” factors that sum to a distribution around a mean value (the distribution of responses by day, and by pollster). In addition, I wanted a model that also updated based on smooth time trends. While there are some nifty approaches using things like random-walk priors, I opted for a very simple cubic spine regression.\n\n\nFitting the models\nTo do this we fit the HLM component by applying fixed effects for partisan identification, the survey method (online, phone, or other), and the self-identified voting population (registered voters, likely voters, or adults). We apply varying intercepts for a day indicator, as well as for a pollster indicator. These varying intercepts apply partial pooling for each polling day as well as for pollster effects. In short, what this does is help pull polling estimates toward a group-level mean, and helps avoid any individual poll pulling the estimates too far in one direction - which is often referred to as “shrinkage”. I’ve actually written about this before in a blog post about estimating rare injury rates. We also fit a very simple cubic spline regression with default brms parameters. The idea here is to get smoothed estimates over time to account for a baseline trend of public opinion.\nWe do all the model fitting using brms using a binomial regression model in the form n_votes |trials(pop) where n_votes is the predicted number of votes for a candidate given a survey sample size pop. Finally, we also weight the polls based on their numeric grade assigned by 538. An example brms model looks something like the following below:\n\n\nCode\n# fitting a poll aggregation model for harris\nfit2.1 &lt;-\n  brm(\n    n_votes |\n      trials(pop) + weights(W_dem) ~ 1 + partisan + method + vtype +\n      (1 | index_t) +\n      (1 | index_p),\n    family = \"binomial\",\n    data = all_polls_df[dem, ],\n    data2 = list(W_dem = W_dem),\n    prior = bprior,\n    chains = 4,\n    cores = 4,\n    file = \"data\\\\election_model\\\\fit2.1\"\n  )\n\n\n\n\nPost-hoc weighting\nI likely deviated from accepted statistical dogma here by weighting the smoothing spline model and HLM using very ad-hoc approach. Early on I decided that I wanted my estimates to be mostly evenly controlled by the HLM and the smoothing spline, but I felt that the smoothing time trend would help eliminate a lot of the spikes on a day-to-day basic. I opted for a 60/40 weighting scheme that gave slightly more weight to the smoothing model. As you can see below, the combined “post-hoc” weighted predictions.\n\n\n\n\n\n\n\n\n\n\nSmoothing spline model predictions\n\n\n\n\n\n\n\n\n\nHierarchical linear model predictions\n\n\n\n\n\n\n\n\n\n\n\nPost-hoc weighted predictions (60/40)\n\n\n\n\n\n\nFor clarity, we can add all the individual poll results showing the estimated support for Harris and Trump, with the size weighted by the number of respondents (larger polls get larger circles). Shown below, on the last day before the election we estimate a popular vote of 49.1% for Trump and 48.9% for Harris. Including the uncertainty basically makes estimating the winner essentially a coinflip.\n\n\n\n\n\nPredicted popular vote for Harris and Trump, as of 11/4/2024\n\n\n\n\n\n\nResults\nAnd below we have a table showing the estimated results for the last day of polling on 11/14/2025.\n\n\n\nElection day estimates, national popular vote\n\n\nparty\nend\nymin\nmedian\nymax\n\n\n\n\nDEM\n2024-11-04\n47.34\n48.92\n50.59\n\n\nREP\n2024-11-04\n47.49\n49.11\n50.76\n\n\n\n\n\nLooking at my estimates, compared to the final count (as of 1/9/2025) the point estimates my model came up with are quite close to the observed results (Trump at 49.9% vs 49.1%, Harris at 48.9% vs 48.4%). Getting within a percentage point of the true value is pretty good, I think for a somewhat half-baked model! That being said, for the purposes of predicting who would ultimately win the day of, the margin of error on the predictions give us essentially no additional confidence beyond a 50/50 chance. This is pretty consistent with a lot of other pollsters who had fancier models. In the end, it was a very close election that was decided by a relatively small number of voters in key areas."
  },
  {
    "objectID": "posts/presidential-polls-2024/election.html#full-script",
    "href": "posts/presidential-polls-2024/election.html#full-script",
    "title": "I Made An Election Model",
    "section": "Full Script",
    "text": "Full Script\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\n\npolls &lt;- read_csv(\"https://projects.fivethirtyeight.com/polls-page/data/president_polls_historical.csv\")\nset.seed(8372)\n\n# set up data, consistent with some other data sources\n# this is just national polls\nmax_size = 5000\nmatchup = c(\"Harris\", \"Trump\")\n\n# get just harris-trump matchups\nharris_trump &lt;-\npolls %&gt;%\n  group_by(poll_id, question_id) %&gt;%\n  summarise(all_reps = paste0(answer, collapse = \",\") ) %&gt;%\n  filter(all_reps %in% c(\"Harris,Trump\",\"Trump,Harris\")) %&gt;%\n  pull(question_id)\n\n\n# select data\n# only national polls where trump-harris are the options\n# remove polls that are overlapping\n\nall_polls_df &lt;- \npolls %&gt;%\n  rename(pop = sample_size,\n         vtype = population) %&gt;%\n  mutate(begin = as.Date(start_date, \"%m/%d/%y\"),\n         end = as.Date(end_date, \"%m/%d/%y\"),\n         t = end - (1 + as.numeric(end-begin)) %/% 2,\n         entry_date = as.Date(created_at, \"%m/%d/%y\")) %&gt;%\n  filter(question_id %in% harris_trump,\n         is.na(state),\n         !is.na(pollscore),\n         !is.na(numeric_grade),\n         answer %in% matchup,\n         end &gt; as.Date(\"2024-06-01\"),\n         t &gt;= begin & !is.na(t) & (vtype %in% c(\"lv\",\"rv\",\"a\")),\n         pop &gt; 1,\n         pop &lt;= max_size) %&gt;%\n  mutate(pollster = str_extract(pollster, pattern = \"[A-z0-9 ]+\") %&gt;% sub(\"\\\\s+$\", \"\", .),\n         pollster = replace(pollster, pollster == \"Fox News\", \"FOX\"), \n         pollster = replace(pollster, pollster == \"WashPost\", \"Washington Post\"),\n         pollster = replace(pollster, pollster == \"ABC News\", \"ABC\"),\n         partisan = ifelse(is.na(partisan), \"NP\", partisan),\n         method = case_when(\n           str_detect(tolower(methodology) ,\"online\") ~ \"online\",\n           str_detect(tolower(methodology) ,\"phone\") ~ \"phone\",\n           TRUE ~ \"other\"),\n         week = floor_date(t - days(2), unit = \"week\") + days(2),\n         day_of_week = as.integer(t - week),\n         index_t = 1 + as.numeric(t) - min(as.numeric(t)),\n         index_w = as.numeric(as.factor(week)),\n         index_p = as.numeric(as.factor(as.character(pollster))),\n         n_votes =  round(pop * (pct/100))) %&gt;%\n  distinct(t, pollster, pop, party, .keep_all = TRUE) %&gt;%\n  select(poll_id, t, begin, end, entry_date, pollster, partisan, numeric_grade, pollscore, vtype, method, pop, n_votes, pct,party,answer, week, day_of_week, starts_with(\"index_\"))\n\n# remove overlapping polls\nall_polls_df &lt;-\n  all_polls_df %&gt;%\n  group_by(entry_date, pollster, pop, party) %&gt;%\n  arrange(desc(entry_date), desc(end)) %&gt;%\n  slice(1)\n\n# drop polls with combined 2-party vote share &lt; 85%\nlow_vote &lt;-\n  all_polls_df %&gt;%\n  group_by(poll_id, entry_date, pollster, pop) %&gt;%\n  summarise(total_votes = sum(n_votes), .groups = 'drop') %&gt;%\n  mutate(prop = total_votes / pop) %&gt;%\n  filter(prop &lt; .85) %&gt;%\n  select(poll_id, entry_date, pollster, pop)\n\nall_polls_df &lt;- anti_join(all_polls_df, low_vote)\n\n# plot the 2 party pct\nall_polls_df %&gt;%\n  ggplot(aes(x = t, y = pct, color = party)) +\n  geom_point(aes(size = pop, fill = party),shape = 21, alpha = .2) +\n  geom_smooth(aes(color = party)) +\n  scale_color_manual(values = c(\"#00AEF3\",\"#E81B23\")) +\n  scale_fill_manual(values = c(\"#00AEF3\",\"#E81B23\")) +\n  scale_y_continuous(limits = c(30, 60)) +\n  theme_minimal() +\n  theme(legend.position = 'none')\n\n\n\n# RUN MODELS\n# -------------------- #\n\nextract_posterior_predictions &lt;- function(x){\n  # get the median and 95% credible interval\n  ypred &lt;- posterior_predict(x)\n  ypred &lt;- apply(ypred, 2, quantile, probs = c(.025, .5, .975)) %&gt;% round()\n  data.frame(t(ypred)) %&gt;% set_names(c(\"ymin\",\"median\",\"ymax\"))\n}\n\nstack_extract_posterior_predictions_naive &lt;- function(x1, x2, weights = c(.5, .5), return = \"agg\"){\n  # average weighted predictions from two models\n  pred1 &lt;- posterior_predict(x1)\n  pred2 &lt;- posterior_predict(x2)\n  pred_avg &lt;- (pred1 * weights[1]) + (pred2 * weights[2])\n  \n  if(return==\"agg\"){\n    ypred &lt;- apply(pred_avg, 2, quantile, probs = c(.025, .5, .975)) %&gt;% round()\n    out &lt;- data.frame(t(ypred)) %&gt;% set_names(c(\"ymin\",\"median\",\"ymax\"))\n  }\n  else if(return==\"raw\"){\n    out &lt;- pred_avg\n  }\n  \n  return(out)\n}\n\nstack_extract_posterior_predictions &lt;- function(x1, x2){\n  # get the median and 95% credible interval\n  ypred &lt;- pp_average(x1, x2, summary = FALSE)\n  ypred &lt;- apply(ypred, 2, quantile, probs = c(.025, .5, .975)) %&gt;% round()\n  data.frame(t(ypred)) %&gt;% set_names(c(\"ymin\",\"median\",\"ymax\"))\n}\n\n# politcal idx\ndem &lt;- all_polls_df$party == 'DEM'\ngop &lt;- all_polls_df$party == 'REP'\n\n# adjust for poll type, partisan\nbprior &lt;- c(\n  prior(normal(0, 0.5), class = 'Intercept'),\n  prior(normal(0, 0.5), class = 'b'),               \n  prior(student_t(3, 0, 1), class = 'sd')                \n)\n\nsprior &lt;- c(prior(normal(0, 0.5), class = 'Intercept'),\n            prior(student_t(3, 0, 1), class = 'sds'))\n\n# rescaled polster weights, mean of 1\nW &lt;- all_polls_df$numeric_grade/mean(all_polls_df$numeric_grade, na.rm = TRUE)\nW_dem = W[dem]\nW_gop = W[gop]\n\n# aggregation model\nfit2.1 &lt;-\n  brm(\n    n_votes |\n      trials(pop) + weights(W_dem) ~ 1 + partisan + method + vtype +\n      (1 | index_t) +\n      (1 | index_p),\n    family = \"binomial\",\n    data = all_polls_df[dem, ],\n    data2 = list(W_dem = W_dem),\n    prior = bprior,\n    chains = 4,\n    cores = 4,\n    file = \"data\\\\election_model\\\\fit2.1\"\n  )\n\nfit2.2 &lt;-\n  brm(\n    n_votes |\n      trials(pop) + weights(W_gop) ~ 1 + partisan + method + vtype +\n      (1 | index_t) +\n      (1 | index_p),\n    family = \"binomial\",\n    data = all_polls_df[gop, ],\n    data2 = list(W_gop = W_gop),\n    prior = bprior,\n    chains = 4,\n    cores = 4,\n    file = \"data\\\\election_model\\\\fit2.2\"\n  )\n\n# using a cubic regression spline for smoothing\nfit2.1s &lt;-\n  brm(n_votes | trials(pop) + weights(W_dem) ~ 1 + s(index_t, bs = 'cr'),\n    data = all_polls_df[dem, ],\n    data2 = list(W_dem = W_dem),\n    family = \"binomial\",\n    prior = sprior,\n    chains = 4,\n    cores = 4,\n    control = list(adapt_delta = 0.99),\n    file = \"data\\\\election_model\\\\fit2.1s\"\n  )\n\nfit2.2s &lt;-\n  brm(n_votes |trials(pop) + weights(W_gop) ~ 1 + s(index_t, bs = 'cr'),\n    data = all_polls_df[gop, ],\n    family = \"binomial\",\n    prior = sprior,\n    data2 = list(W_gop = W_gop),\n    chains = 4,\n    cores = 4,\n    control = list(adapt_delta = 0.99),\n    file = \"data\\\\election_model\\\\fit2.2s\"\n  )\n\n# add predictions back to dataframe with weighted predictions\n# weight 1 = hlm, weight 2 = smoothing model\n# more weight on #2 = more smoothing\n# default is about 40% hlm, 60% smoothing\nweights = c(.4, .6)\npred_dem &lt;- cbind.data.frame(stack_extract_posterior_predictions_naive(fit2.1, fit2.1s, weights = weights), all_polls_df[dem,])\npred_gop &lt;- cbind.data.frame(stack_extract_posterior_predictions_naive(fit2.2, fit2.2s, weights = weights), all_polls_df[gop,])\n\n\ntest &lt;-\n  rbind.data.frame(pred_dem, pred_gop)  %&gt;%\n  mutate(across(ymin:ymax, function(x)\n    (x / pop)*100)) %&gt;%\n  group_by(party, end) %&gt;%\n  summarise(across(ymin:ymax, mean)) %&gt;%\n  ungroup()\n\nplot1&lt;-\ntest %&gt;%\n  group_by(party, end) %&gt;%\n  summarise(across(ymin:ymax, mean)) %&gt;%\n  ggplot(aes(x = end)) +\n  geom_line(aes(y = median, group = party, color = party), linewidth = 1.2) +\n  scale_color_manual(values = c(\"#00AEF3\",\"#E81B23\")) +\n  scale_fill_manual(values = c(\"#00AEF3\",\"#E81B23\")) +\n  scale_y_continuous(limits = c(30, 60)) +\n  geom_hline(yintercept = 30, color = 'grey20') +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.major.y = element_line(color = \"grey90\", linewidth = 1 ),\n        panel.grid.minor.y = element_line(color = \"grey90\", linewidth = 1),\n        axis.ticks.x = element_line(lineend = \"round\", linewidth = 1, color = 'grey50'),\n        axis.title = element_blank(),\n        axis.text = element_text(size = 10, color = 'grey50', face = 'bold'),\n        axis.text.y = element_text(vjust = -0.5))\nplot1\n\n# labels for end points\nend_labels &lt;- test %&gt;%\n  filter(end == max(end)) %&gt;%\n  group_by(party) %&gt;%\n  slice(1)\n\n\n# w/o error bars\nplot1 +\n  geom_point(data = all_polls_df, aes(x = end, y = pct, color = party, fill = party), shape = 21, size = 2, alpha = .2) \n\n# add point sizes and label\nplot1 +\n  geom_point(data = all_polls_df, aes(x = end, y = pct, color = party, fill = party, size = pop), alpha = .2) +\n  geom_point(data = end_labels, aes(x = end, y = median, color = party), size = 2.5) +\n  geom_label(data = end_labels, aes(x = end, y = median, label = round(median,1), fill = party), color = 'white', fontface = 'bold', nudge_x = 5, nudge_y = c(-.75,.75), size = 3.2)\n\n # facet plots\nplot1 +\n  geom_point(data = all_polls_df, aes(x = end, y = pct, color = party, fill = party, size = pop), alpha = .2) +\n  geom_ribbon(aes(ymin = ymin, ymax = ymax, group = party, fill = party), color = 'white', alpha = .2)\n\n\n# difference on max data\nraw_dem &lt;- stack_extract_posterior_predictions_naive(fit2.1, fit2.1s, weights = weights, return = \"raw\")\nraw_gop &lt;- stack_extract_posterior_predictions_naive(fit2.2, fit2.2s, weights = weights, return = \"raw\")\n\n\n# compute dem margin based on most recent 10 days worth of polls\ndem_margin &lt;- raw_dem / (raw_dem + raw_gop)\n\nmax_date &lt;- max(pred_dem$end)\ndate_idx &lt;- seq.Date(from = max_date,by = \"-1 day\",length.out = 10)\nT &lt;- as.numeric(rownames(pred_dem[pred_dem$end %in% date_idx, ]))\n\nmean(apply(dem_margin[,T], 1, function(x) mean(x &gt; .5)))\n\n# 95% CI and mean \n# predicted share of 2-party vote\nmean(apply(dem_margin[,T], 1, quantile, probs = c(0.025)))\nmean(apply(dem_margin[,T], 1, mean))\nmean(apply(dem_margin[,T], 1, quantile, probs = c(0.975)))\n\n# predicted % times that dem candidate wins popular vote\n# NOT the same as winning the election !!!\nmean(dem_margin[,T] &gt; .5)\n\nhist(dem_margin)"
  },
  {
    "objectID": "posts/presidential-polls-2024/election.html#comments",
    "href": "posts/presidential-polls-2024/election.html#comments",
    "title": "I Made An Election Model",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/pca-anomaly/pca_anomaly.html",
    "href": "posts/pca-anomaly/pca_anomaly.html",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "I strongly believe in “learning by doing”. One of the things I have been working on quite a bit lately is unsupervised anomaly detection. As with many machine-learning tools, ensembles are incredibly powerful and useful for a variety of circumstances.\nAnomaly detection ensembles are no exception to that rule. To better understand how each of the individual pieces of a anomaly detection ensemble works, I’ve decided two build one myself “from scratch”. I put that in giant quotes here because I’ll still rely on some existing frameworks in R to help built the underlying tools.\nMy idea is to create an ensemble of several heterogeneous anomaly detection methods in a way that maximizes their individual benefits. Following some of the guidance proposed by Aggarwal & Sathe I will use:\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\nhosp &lt;- read_csv(\"Hospital_Inpatient_Discharges__SPARCS_De-Identified___2021.csv\")\n\n\nThe data for this post comes from the state of New York’s Hospital Inpatient Discharges (SPARCS) data for 2021. This data contains about 2.1 million records on hospital discharges, including some de-identified patient information, procedure types, and costs. From an anomaly detection standpoint, it might make sense to see if we can identify hospitals with anomalous costs relative to other New York Hospitals. Like the New York Times reported, hospitals make up a very substantial portion of what we spent on healthcare.\nWe’ll create a quick feature set based on a few key payment variables. Here I aggregate over each of the hospitals by calculating their (1) average stay length, (2) average charges, (3) average costs, (4) their average cost-per-stay, (5) the ratio between costs to total charges and (6) the average procedure pay difference.\nThis last feature is a bit more complex, but what I am essentially doing is finding the median cost per-procedure by case severity (assuming more severe cases cost more) and then finding out how much each hospital diverges, on average, from the global cost. It’s a indirect way of measuring how much more or less a given procedure costs at each hospital. This is a easy measure to utilize because a 0 indicates that the hospital bills that procedure at near the global median, a 1 means they bill 100% more than the median and -.5 means they bill 50% less than the median.\n\n\nCode\n# compute and aggregate feature set\ndf &lt;-\n  hosp %&gt;%\n  group_by(`CCSR Procedure Code`, `APR Severity of Illness Code`) %&gt;%\n  mutate(\n    proc_charge = median(`Total Costs`),\n    charge_diff = (`Total Costs` - proc_charge)/proc_charge,\n    `Length of Stay` = as.numeric(ifelse(\n      `Length of Stay` == '120+', 120, `Length of Stay`\n    ))\n  ) %&gt;%\n  group_by(id = `Permanent Facility Id`) %&gt;%\n  summarise(\n    stay_len = mean(`Length of Stay`, na.rm = T),\n    charges = mean(`Total Charges`),\n    costs = mean(`Total Costs`),\n    diff = mean(charge_diff),\n    cost_ratio = mean(`Total Costs`/`Total Charges`),\n    cost_per_stay = costs / stay_len\n  )"
  },
  {
    "objectID": "posts/pca-anomaly/pca_anomaly.html#the-problem",
    "href": "posts/pca-anomaly/pca_anomaly.html#the-problem",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "I strongly believe in “learning by doing”. One of the things I have been working on quite a bit lately is unsupervised anomaly detection. As with many machine-learning tools, ensembles are incredibly powerful and useful for a variety of circumstances.\nAnomaly detection ensembles are no exception to that rule. To better understand how each of the individual pieces of a anomaly detection ensemble works, I’ve decided two build one myself “from scratch”. I put that in giant quotes here because I’ll still rely on some existing frameworks in R to help built the underlying tools.\nMy idea is to create an ensemble of several heterogeneous anomaly detection methods in a way that maximizes their individual benefits. Following some of the guidance proposed by Aggarwal & Sathe I will use:\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\nhosp &lt;- read_csv(\"Hospital_Inpatient_Discharges__SPARCS_De-Identified___2021.csv\")\n\n\nThe data for this post comes from the state of New York’s Hospital Inpatient Discharges (SPARCS) data for 2021. This data contains about 2.1 million records on hospital discharges, including some de-identified patient information, procedure types, and costs. From an anomaly detection standpoint, it might make sense to see if we can identify hospitals with anomalous costs relative to other New York Hospitals. Like the New York Times reported, hospitals make up a very substantial portion of what we spent on healthcare.\nWe’ll create a quick feature set based on a few key payment variables. Here I aggregate over each of the hospitals by calculating their (1) average stay length, (2) average charges, (3) average costs, (4) their average cost-per-stay, (5) the ratio between costs to total charges and (6) the average procedure pay difference.\nThis last feature is a bit more complex, but what I am essentially doing is finding the median cost per-procedure by case severity (assuming more severe cases cost more) and then finding out how much each hospital diverges, on average, from the global cost. It’s a indirect way of measuring how much more or less a given procedure costs at each hospital. This is a easy measure to utilize because a 0 indicates that the hospital bills that procedure at near the global median, a 1 means they bill 100% more than the median and -.5 means they bill 50% less than the median.\n\n\nCode\n# compute and aggregate feature set\ndf &lt;-\n  hosp %&gt;%\n  group_by(`CCSR Procedure Code`, `APR Severity of Illness Code`) %&gt;%\n  mutate(\n    proc_charge = median(`Total Costs`),\n    charge_diff = (`Total Costs` - proc_charge)/proc_charge,\n    `Length of Stay` = as.numeric(ifelse(\n      `Length of Stay` == '120+', 120, `Length of Stay`\n    ))\n  ) %&gt;%\n  group_by(id = `Permanent Facility Id`) %&gt;%\n  summarise(\n    stay_len = mean(`Length of Stay`, na.rm = T),\n    charges = mean(`Total Charges`),\n    costs = mean(`Total Costs`),\n    diff = mean(charge_diff),\n    cost_ratio = mean(`Total Costs`/`Total Charges`),\n    cost_per_stay = costs / stay_len\n  )"
  },
  {
    "objectID": "posts/pca-anomaly/pca_anomaly.html#creating-a-pca-anomaly-detector",
    "href": "posts/pca-anomaly/pca_anomaly.html#creating-a-pca-anomaly-detector",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Creating a PCA Anomaly Detector",
    "text": "Creating a PCA Anomaly Detector\nLet’s now get into the nuts and bolts of actually creating a PCA-based anomaly detector. Now, there are a few ways we can go about this, but I’m going to rely on the approach suggested by Charu Aggarwal in his book Outlier Analysis. What he essentially proposes is a “soft” version of principal components where the eigenvectors are weighted by their variance instead of choosing only the eigenvectors that explain the highest proportion of the overall variance. This “soft” approach has some overlap with the mahalanobis distance. This is the same approach taken by the PCA anomaly detector in the Python pyod package if weighting is specified.\n\nPerforming the “soft” PCA\nTranslating this approach from the formula to code is actually pretty straightforward. Agarwal gives us the following:\n\\[Score(\\bar{X}) = \\sum^d_{j=1} \\frac{|(\\bar{X} - \\bar{\\mu}) *\\bar{e_j}|^2}{\\lambda_j}\\] Which we can code into the following below. Before we add our data to the PCA we scale it to have mean 0 and standard deviation 1 so that the input features are scale-invariant. We then work through the process of extracting the eigenvectors, computing the variance for each, and then performing the “soft” PCA approach.\n\n\n\n\n\n\nNote\n\n\n\nA few notes - strictly speaking the the part \\(\\bar{X} - \\bar{\\mu}\\) isn’t totally necessary in most cases because the eigenvectors are already scaled to have a mean of zero (you can also ensure this by specifying cor=TRUE in the princomp() function). This does help in unusual cases where \\(\\bar{\\mu}\\) is not zero.\n\n\n\n# \"Soft\" PCA\n\n# scale input attributes\nX &lt;- df[, 2:7]\nX &lt;- scale(X)\n\n# pca anomaly detection\n# extract eigenvectors & variance\npca &lt;- princomp(X, cor = TRUE)\ne &lt;- pca$scores\nev &lt;- diag(var(e))\nmu &lt;- apply(e, 2, mean)\nn &lt;- ncol(e)\n\n# compute anomaly scores\nalist &lt;- vector(mode = \"list\", length = n)\nfor(i in 1:n){\n  alist[[i]] &lt;- abs( (e[, i] - mu[i])^2 / ev[i])\n}\n\n# extract values & export\nXscore &lt;- as.matrix(do.call(cbind, alist))\nanom &lt;- apply(Xscore, 1, sum)\n\nThis “soft” PCA is in contrast to so-called “hard” PCA where a specific number of principal components are chosen and the remainder are discarded. The “hard” PCA primarily focuses on reconstruction error along the components with the most variance, while the “soft” approach weights outliers on the lower variance components higher. This is useful because the data vary much less on these components, so outliers are often more obvious along these dimensions.\n\n\nCode\npca$scores %&gt;%\n  data.frame() %&gt;%\n  pivot_longer(cols = starts_with(\"Comp\")) %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = value), bins = 30, fill = '#004488') +\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_minimal() +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\nOutliers are often more visable along components with lower variance.\n\n\n\n\nThe code below implements the PCA anomaly detector. Most of the work involves taking the scores, putting them into a matrix, and then re-weighting them by their variance. The final score is just the row-wise sum across the re-weighted columns. The output is just a vector of anomaly scores anom.\n\n\nCode\nggplot(tibble(anom)) +\n  geom_histogram(aes(x = anom), bins = 25, color = 'white', fill = '#004488')+\n  labs(x = \"Anomaly Score\", y = \"Observations\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nDistribution of outlier scores. More anomalous observations have higher scores\n\n\n\n\n\n\nFlagging Outliers\nFor flagging outliers we can rely on the \\(\\chi^2\\) distribution. This is handy because the \\(\\chi^2\\) distribution is formed as the sum of the squares of \\(d\\) independent standard normal random variables. For the purposes of this example we might just choose 1% as our threshold for anomalies. After flagging we can plot the data along different 2d distributions to see where they lie. For example, the 8 anomalous hospitals generally have both longer stay lengths and charge more for comparable procedures relative to other hospitals.\n\n\nCode\n# compute a p-value for anomalies and\n# append anomaly score and p-values to new dataframe\np = sapply(anom, pchisq, df=6, ncp = mean(anom), lower.tail=F)\n\nscored_data &lt;- data.frame(df, anom, p)\n\nflag &lt;- scored_data$p &lt;= 0.01\n\nggplot() +\n  geom_point(data = scored_data, aes(x = stay_len, y = diff), color = '#004488', size = 2, alpha = .25) +\n  geom_point(data = scored_data[flag,], aes(x = stay_len, y = diff), color = '#BB5566', size = 2.5) +\n  labs(x = \"Stay Length\", y = \"Avg. Payment Difference\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nPlotting inliers (blue) and outliers (red) in 2D space.\n\n\n\n\n\n\nPCA Anaomaly Detector: Example Function\nHere’s a minimal working example of the procedure above. As we build our ensemble, we’ll come back to this function later.\n\n\nCode\n# Run a principal components anomaly detector\nadPCA &lt;- function(X){\n  \n  # pca anomaly detection\n  # extract eigenvectors & variance\n  pca &lt;- princomp(X, cor = TRUE)\n  e &lt;- pca$scores\n  ev &lt;- diag(var(e))\n  mu &lt;- apply(e, 2, mean)\n  n &lt;- ncol(e)\n  \n  # compute anomaly scores\n  alist &lt;- vector(mode = \"list\", length = n)\n  for(i in 1:n){\n    alist[[i]] &lt;- abs( (e[, i] - mu[i])^2 / ev[i])\n  }\n  \n  # extract values & export\n  Xscore &lt;- as.matrix(do.call(cbind, alist))\n  anom &lt;- apply(Xscore, 1, sum)\n  \n  return(anom)\n}"
  },
  {
    "objectID": "posts/nlp-falls/nlp_falls.html",
    "href": "posts/nlp-falls/nlp_falls.html",
    "title": "Using NLP To Classify Medical Falls",
    "section": "",
    "text": "There’s no question that natural language processing (NLP) facilitated by deep learning has exploded in popularity (much of which is popularized by the ChatGPT family of models). This is an exciting time to be involved in AI and machine learning. However, for the kinds of tasks I typically work on in my day job, a lot of the deep learning models don’t provide much benefit. In fact, for most tabular data problems, random forests + boosting tend to work incredibly well. Areas where deep learning excels, like unstructured text or image input, are not things I find myself working on. That being said, I am always sharpening my skills and dipping my toes into areas where I am least familiar.\nA huge advantage today, compared to even ten years ago, is the ecosystem of open data and pre-trained models. HuggingFace in particular has a lot of easily obtainable pre-trained models. Stuff like the Transformers library make it easy for a neophyte like me to hop in and start doing work without too much overhead."
  },
  {
    "objectID": "posts/nlp-falls/nlp_falls.html#natural-language-processing-and-deep-learning",
    "href": "posts/nlp-falls/nlp_falls.html#natural-language-processing-and-deep-learning",
    "title": "Using NLP To Classify Medical Falls",
    "section": "",
    "text": "There’s no question that natural language processing (NLP) facilitated by deep learning has exploded in popularity (much of which is popularized by the ChatGPT family of models). This is an exciting time to be involved in AI and machine learning. However, for the kinds of tasks I typically work on in my day job, a lot of the deep learning models don’t provide much benefit. In fact, for most tabular data problems, random forests + boosting tend to work incredibly well. Areas where deep learning excels, like unstructured text or image input, are not things I find myself working on. That being said, I am always sharpening my skills and dipping my toes into areas where I am least familiar.\nA huge advantage today, compared to even ten years ago, is the ecosystem of open data and pre-trained models. HuggingFace in particular has a lot of easily obtainable pre-trained models. Stuff like the Transformers library make it easy for a neophyte like me to hop in and start doing work without too much overhead."
  },
  {
    "objectID": "posts/nlp-falls/nlp_falls.html#predicting-elderly-falls-from-medical-narratives",
    "href": "posts/nlp-falls/nlp_falls.html#predicting-elderly-falls-from-medical-narratives",
    "title": "Using NLP To Classify Medical Falls",
    "section": "Predicting Elderly Falls from Medical Narratives",
    "text": "Predicting Elderly Falls from Medical Narratives\nFor this example I am going to rely on some data from DrivenData - an organization that hosts data competitions. The data here are verified fall events for adults aged 65+. This sample comes more broadly from the National Electronic Injury Survellience System(NEISS). This is useful because the sample of cases here are human-verified falls cases, in which case we have a source of truth. While you could probably get pretty far just doing some regex like str.match(\"FALL|FELL|SLIPPED\") but it would likely miss more subtle cases. This is where having something like a BERT model is useful.\nLet’s say we have a set of verified falls narratives (which we do) and we have a large set of miscellanous narratives that contain falls cases, as well as other injuries that are not falls. Our goal is to find narratives that are likely to be related to elderly fall cases. To do this, we will use the verified falls cases narratives from DataDriven as our “training data” so to speak, and we will use an NLP model to find cases that are semantically similar to these verified falls cases.\n\nData Setup\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport re\nfrom sentence_transformers import SentenceTransformer, util\n\nnp.random.seed(1)\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# load raw data\nfalls = pd.read_csv(\"../../../data/falls/falls.csv\")\nneis = pd.read_csv(\"../../../data/falls/neis.csv\")\n\n# process datetime\nfalls['treatment_date'] = pd.to_datetime(falls['treatment_date'])\n\nTo get set up we read in the verified falls narratives, as well as the full sample of NEIS cases from 2022. After reading in our data we can perform some minor data cleaning to the narratives. Specifically, because we want to isolate narrative characterics associated with falls we should exclude the leading information about the patient’s age and sex, as well as some other medical terminology. We can also remap some abbreviations to English and properly extract the actual age of the patient from the narrative.\n\n# define remappings of abbreviations\n# and strings to remove from narratives\n\nremap = {\n    \"FX\": \"FRACTURE\",\n    \"INJ\": \"INJURY\",\n    \"LAC\": \"LACERATION\",\n    \"CONT\": \"CONTUSION\",\n    \"CHI\" : \"CLOSED HEAD INJURY\",\n    \"ETOH\": \"ALCOHOL\",\n    \"SDH\": \"SUBDURAL HEMATOMA\",\n    \"NH\": \"NURSING HOME\",\n    \"PT\": \"PATIENT\",\n    \"LT\": \"LEFT\",\n    \"RT\": \"RIGHT\",\n    \"&\" : \" AND \"\n}\nstr_remove = \"YOM|YOF|MOM|MOF|C/O|S/P|H/O|DX\"\n\n\ndef process_text(txt):\n    words = txt.split()\n    new_words = [remap.get(word, word) for word in words]\n    txt = \" \".join(new_words)\n\n    txt = re.sub(\"[^a-zA-Z ]\", \"\", txt)\n    txt = re.sub(str_remove, \"\", txt)\n\n    return re.sub(r\"^\\s+\", \"\", txt)\n\ndef narrative_age(string):\n    age = re.match(\"^\\d+\",string)\n\n    if not age:\n        age = 0\n    else:\n        age = age[0]\n        \n    return age\n\nWe then apply these to our verified falls data and our raw NEIS data from 2022:\n\n# process narrative text and extract patient age from narrative\nfalls['processed_narrative'] = falls['narrative'].apply(process_text)\nneis['processed_narrative'] = neis['Narrative_1'].apply(process_text)\n\nfalls['narrative_age'] = falls['narrative'].apply(narrative_age).astype(int)\nneis['narrative_age'] = neis['Narrative_1'].apply(narrative_age).astype(int)\n\n# neis cases are from 2022, remove from verified falls\nfalls = falls[falls['treatment_date'] &lt; \"2022-01-01\"]\n\n# filter narrative ages to 65+\nfalls = falls[falls['narrative_age'] &gt;= 65]\nneis = neis[neis['narrative_age'] &gt;= 65]\n\nWe can see that our coding changes the narratives subtly. For example this string:\n\nfalls['narrative'][15]\n\n'87YOF HAD A FALL TO THE FLOOR AT THE NH STRUCK BACK OF HEAD HEMATOMA TO SCALP'\n\n\nIs changed to this:\n\nfalls['processed_narrative'][15]\n\n'HAD A FALL TO THE FLOOR AT THE NURSING HOME STRUCK BACK OF HEAD HEMATOMA TO SCALP'\n\n\nThis minimal amount of pre-processing should help the model identify similar cases without being affected by too much extranenous information. In addition, because the typical model has about 30,000 words encoded we need to make sure we avoid abbreviations which will be absent from the model dictionary.\n\n\nImplementing the Transformer model\nWe can grab all of our verified fall narratives as well as a random sample of narratives from the 2022 NEIS data. Below we’ll take a sample of 250 cases and run them through our model.\n\nN = 250\nidx = np.random.choice(neis.shape[0], N, replace=False)\n\nfall_narrative = np.array(falls['processed_narrative'])\nneis_narrative = np.array(neis['processed_narrative'])[idx]\n\nWe take the processed narratives and convert them to tokens using the pre-trained sentence transformer:\n\nembed_train = model.encode(fall_narrative)\nembed_test = model.encode(neis_narrative)\n\nWe then compute the cosine similarity between the two tensors. What we will end up with is the distance from our NEIS narratives and the verified fall cases. Cases with larger distances should be less likely to contain information about elderly fall cases.\n\ncos_sim = util.cos_sim(embed_test, embed_train)\n\nFor simplicity we scale the distances between 0 and 1, so that 1 is most similar and 0 is least similar. We can then just compare the rank-ordered narratives.\n\ndists = cos_sim.mean(1)\nd_min, d_max = dists.min(), dists.max()\n\ndists = (dists - d_min)/(d_max - d_min)\ndists = np.array(dists)\n\nout = dict(zip(neis_narrative, dists)) \n\nPlotting a histogram of the minmax scaled cosine similarity scores shows a lot of narratives that are very similar and a long tail of those that are not so similar. Of course, there isn’t a single cut point of what we would consider acceptable for classification purposes, but we could certainly use these scores in a regression to determine a suitible cut point if we were so interested.\n\n\nCode\ncparams = {\n    \"axes.spines.left\": False,\n    \"axes.spines.right\": False,\n    \"axes.spines.top\": False,\n    \"axes.spines.bottom\": False,\n    \"grid.linestyle\": \"--\"\n}\n\nsns.set_theme(style=\"ticks\", rc = cparams)\n\n(\n    sns.histplot(dists, color=\"#004488\"),\n    plt.xlabel(\"Cosine Similarity (minmax scaled)\")\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nResults\nTime to actually see the results. Our results are stored in a dictionary which allows us to just pull narratives by similarity score. Let’s test it out by looking at the top 10 most similar NEIS narratives:\n\nsorted(out, key=out.get, reverse=True)[:10]\n\n['FELL TO THE FLOOR AT THE NURSING HOME  CLOSED HEAD INJURY',\n 'FELL ON THE FLOOR  CLOSED HEAD INJURY',\n 'WAS AT THE NURSING HOME AND SLIPPED AND FELL TO THE FLOOR STRIKING HIS HEAD  SCALP LACERATION',\n 'PRESENTS AFTER A FALL WHILE WALKING ACROSS THE LIVING ROOM AND HE TRIPPED AND FELL TO THE FLOOR REPORTS HE HIT HIS HEAD AND LEFT SHOULDER ON A RUG  FALL CLOSED FRACTURE OF CERVICAL VERTEBRA',\n 'FELL TO THE FLOOR STRUCK HEAD  CLOSED HEAD INJURY ABRASION TO KNEES',\n 'WAS GETTING INTO BED AND FELL TO THE FLOOR ONTO HEAD  CLOSED HEAD INJURY',\n 'FELL ON FLOOR AT NH INJURY  AND  BODY PATIENT NS  FALL',\n 'FELL BACKWARDS FROM  STEPS CLOSED HEAD INJURY ABRASION HAND',\n 'PRESENTS WITH HEAD INJURY AFTER A FALL REPORTS HE WAS FOUND ON THE FLOOR IN A RESTAURANT AFTER HE SLIPPED AND FELL HITTING HIS HEAD  INJURY OF HEAD',\n 'WENT TO SIT DOWN AND SOMEONE MOVED HER CHAIR AND SHE FELL BACKWARDS HITTING HER HEAD ON THE FLOOR  FALL BLUNT HEAD TRAUMA TAIL BONE PAIN']\n\n\nAnd the 10 least similar narratives:\n\nsorted(out, key=out.get, reverse=False)[:10]\n\n['SYNCOPAL EPISODE WHILE FOLDING NS CLOTHINGSYNCOPE',\n 'WAS COOKING SOME SALMON AND THEN SPRAYED A  AEROSOL DEODORANT DUE TO THE SMELL WHICH CAUSED HER TO FEEL THAT SOMETHING WAS STUCK IN HER THROAT FOREIGN BODY SENSATION IN THROAT',\n 'WAS PLAYING GOLF AND DEVELOPED AMS AND PASSED OUT  SYNCOPE',\n 'CO STABBING RIGHT CHEST PAIN RADIATES TO HER BACK SHORTNESS OF BREATH AFTER HER HHA WAS MOPPING THE FLOOR LAST NIGHT W STRONGPOTENT CLEANING AGENT THAT TRIGGERED HER ASTHMA  CHEST PAIN ASTHMA',\n 'WITH FISH HOOK IN HIS RIGHT INDEX FNIGER HAPPENED AT A LAKE FB RIGHT INDEX FINGER',\n 'CUT THUMB WITH BROKEN BOTTLE NO OTHER DETAILS LWOT NO ',\n 'EXERCISING FELT PAIN IN RIGHT LOWER LEG  LOWER LEG PAIN',\n 'CO LEFT SIDED CHEST PAIN FOR THE PAST THREE DAYS AFTER WORKING OUT AT THE GYM  LEFT PECTORALIS MUSCLE STRAIN',\n 'PRESENTS AFTER BEING IN A ROOM FILLED WITH SMOKE FOR  HOURS AFTER THERE WAS A FIRE IN HER NEIGHBORS APARTMENT UNKNOWN IF FIRE DEPARTMENT INVOLVED  SMOKE INHALATION PAIN IN THROAT ELEVATED TROPONIN',\n 'ON  FOR AF WAS WASHING DISHES AND SLASHED ARM ON A KNIFE  LACERATION OF RIGHT FOREARM']\n\n\nSo in general, it did a pretty good job. The most similar cases are all clearly related to falls, while the least similar ones are all a mix of other injuries. While I don’t have any tests here (coming soon!) I suspect this does better than very simple regex queries. If only because it has the ability to find similarities without needing to match on specific strings."
  },
  {
    "objectID": "posts/nlp-falls/nlp_falls.html#singular-queries",
    "href": "posts/nlp-falls/nlp_falls.html#singular-queries",
    "title": "Using NLP To Classify Medical Falls",
    "section": "Singular Queries",
    "text": "Singular Queries\nWe can extend this model a bit and create a small class object that will take a single query in, and return the \\(K\\) most similar narratives. Below, we bundle our functions into a NarrativeQuery class object. After encoding the narrative we can provide query strings to find sementically similar narratives.\n\nclass NarrativeQuery:\n    def __init__(self, narrative):\n        self.narrative = narrative\n        self.narrative_embedding = None\n        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n    def encode(self):\n        self.narrative_embedding = self.model.encode(self.narrative)\n\n    def search_narrative(self, query, K = 5):\n        embed_query = self.model.encode(query)\n\n        query_out = self.cos_sim(self.narrative_embedding, embed_query)\n\n        return sorted(query_out, key=query_out.get, reverse=True)[:K]\n\n    def cos_sim(self, embed, embed_query):\n        cs = util.cos_sim(embed, embed_query)\n\n        dists = cs.mean(1)\n        d_min, d_max = dists.min(), dists.max()\n\n        dists = (dists - d_min)/(d_max - d_min)\n        dists = np.array(dists)\n\n        return dict(zip(self.narrative, dists))\n\nThis sets it up:\n\nFallsQuery = NarrativeQuery(neis_narrative)\nFallsQuery.encode()\n\n…and this performs the search. Here we’re just looking for narratives where a person slipped in a bathtub.\n\nFallsQuery.search_narrative(query=\"SLIPPED IN BATHTUB\", K = 10)\n\n['SLIPPED AND FELL IN THE SHOWER LANDING ONTO BUTTOCKS  CONTUSION TO BUTTOCKS',\n 'SLIPPED ON FLOOR AND FELL AT HOME  FALL',\n 'PRESENTS AFTER A SLIP AND FALL IN THE TUB STRIKING HER HEAD ON THE WALL  SYNCOPE FALL HEAD STRIKE',\n 'FELL IN THE SHOWER  FRACTURED UPPER BACK',\n 'PATIENT FELL IN THE SHOWER AND HIT HER HEAD AND HER LEFT ELBOW  LACERATION OF SCALP WITHOUT FOREIGN BODY STRUCK BATH TUB WITH FALL ABRASION OF LEFT ELBOW',\n 'WAS WALKING FROM THE BATHROOM TO THE BEDROOM AND PASSED OUT FALLING TO THE FLOOR CAUSING A SKIN TEAR TO HIS LEFT ELBOW  SKIN TEAR OF LEFT ELBOW',\n 'SLIPPED AND FELL IN FLOOR AT HOME  R HIP FRACTURE',\n 'FELL IN THE SHOWER AT HOME TWISTING RIGHT KNEE  RUPTURE RIGHT PATELAR TENDON',\n 'WEARING SOCKS SLIPPED AND FELLHEAD INJURYFX FEMUR',\n 'SLIPEPD AND FELL IN THE SHOWER STRUCK HEAD  CLOSED HEAD INJURY CONTUSION TO LEFT HIP']\n\n\nNow this is cool. Using the sentence transformer we are able to get passages that are similar in style to what we searched, without sharing the exact same language. For example, the search query is \"SLIPPED IN BATHTUB\" but we get results like \"FELL IN THE SHOWER\" and \"SLIP AND FALL IN THE TUB\". If we were looking specifically for passages related to falls in the bathtub these obviously make sense (many bathtubs are also just showers as well)."
  },
  {
    "objectID": "posts/nlp-falls/nlp_falls.html#finally",
    "href": "posts/nlp-falls/nlp_falls.html#finally",
    "title": "Using NLP To Classify Medical Falls",
    "section": "Finally",
    "text": "Finally\nNow, this isn’t probably news to most people that actually regularly work with language models. However, it is quite impressive that with a pre-trained model and very minimal pre-processing, you can obtain reasonable results off-the-shelf. I’ll definitely be keeping my eyes on these models in the future and looking for ways where they can improve my workflow."
  },
  {
    "objectID": "posts/knn-anomaly/knn_anomaly.html",
    "href": "posts/knn-anomaly/knn_anomaly.html",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "This is the second part of a 3-part series. In the previous post I talked a bit about my desire to work on building the pieces of an outlier ensemble from “scratch” (e.g. mostly base R code with some helpers).\nIn the first post I talked about my approach building a principal components analysis anomaly detector. In this post I’ll work on the K-nearest neighbors anomaly detector using the same base data.\nTo date, the three parts of the ensemble contain:\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector"
  },
  {
    "objectID": "posts/knn-anomaly/knn_anomaly.html#part-2-the-k-nearest-neighbor-anomaly-detector",
    "href": "posts/knn-anomaly/knn_anomaly.html#part-2-the-k-nearest-neighbor-anomaly-detector",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "This is the second part of a 3-part series. In the previous post I talked a bit about my desire to work on building the pieces of an outlier ensemble from “scratch” (e.g. mostly base R code with some helpers).\nIn the first post I talked about my approach building a principal components analysis anomaly detector. In this post I’ll work on the K-nearest neighbors anomaly detector using the same base data.\nTo date, the three parts of the ensemble contain:\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector"
  },
  {
    "objectID": "posts/knn-anomaly/knn_anomaly.html#creating-the-knn-anomaly-detector",
    "href": "posts/knn-anomaly/knn_anomaly.html#creating-the-knn-anomaly-detector",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Creating the KNN anomaly detector",
    "text": "Creating the KNN anomaly detector\n\nDefining distance\nIn a way, the K-nearest neighbors anomaly detector is incredibly simple. To compute the anomalousness of a single point we measure its distance to its \\(k\\) nearest neighbors. We then use either the maximum or average distance among those \\(k\\) points as its anomaly score. However, there is some additional complexity here regarding the choice of \\(k\\) in an unsupervised setting - but we’ll get to that in a moment.\nOne issue is that computing all pairs of nearest neighbors has \\(O(N^2)\\) time complexity. However, we only need to know the number of nearest neighbors up to our value of \\(k\\). Therefore, we can avoid computing nearest unnecessary distances by applying more efficient algorithms - like k-d trees. In the case for \\(k\\) nearest neighbors the time complexity is \\(O(N * log(N)\\). The RANN package in R does this fairly efficiently. We’ll use the same data as in the previous post for this example.\n\n# scale input attributes\nX &lt;- df[, 2:7]\nX &lt;- scale(X)\n\n# compute NN distance between all points\n# set n neighbors\nk = 5\n\n# compute k nearest neighbor distances\n# using kd-trees\nd &lt;- RANN::nn2(X, k = k+1)\nd &lt;- d[[2]][,1:k+1]\n\nYou will notice that we set \\(k\\) to \\(k+1\\) to avoid calculating the nearest-neighbor distance to the each point itself (which is always zero). The nn2 package gives us the Euclidean nearest-neighbor distances for each point arranged from nearest to farthest. For example if we look at the top 3 rows of the distance matrix we see:\n\n\nCode\nd[1:3,]\n\n\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 0.1457822 0.3123632 0.3311984 0.3641609 0.3726815\n[2,] 0.5261689 0.6887312 0.9636189 1.0087124 1.0097114\n[3,] 0.2874466 0.3044159 0.3723676 0.4139428 0.4251863\n\n\nWhich gives us the standardized (Z-score) distance to the \\(k\\) nearest neighbor of point \\(i\\). Now all we need to do is decide on how we will summarize this distance."
  },
  {
    "objectID": "posts/knn-anomaly/knn_anomaly.html#distance-measures",
    "href": "posts/knn-anomaly/knn_anomaly.html#distance-measures",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Distance Measures",
    "text": "Distance Measures\nWe have a few options for distance measures. The most common, and simplest to understand arguably, is to compute a score based on the distance to each observations farthest \\(k\\) neighbor (so if \\(k=5\\) the score is the largest distance among those 5 neighbors). We can accomplish this by just getting the max value from each row.\n\nanom_max &lt;- apply(d, 1, max)\n\nWe also have another option. Rather than choosing the maximum of the 5 nearest neighbors, we can average over a larger number of neighbors. This has the advantage of removing some of the variance implicit in the choice of \\(k\\). For example, imagine the distance to the 6 nearest neighbors of one point are \\([1, 2, 5, 7, 8, 100]\\). If we chose \\(k=5\\) we would miss the obvious outlier that would have been found had we instead chosen \\(k=6\\). A good alternative to is set \\(k\\) much higher and get the average of all neighbors within that range. Here, we might set \\(k\\) to 20 and average over the distances.\n\nanom_mean &lt;- adKNN(d, k = 20, method = \"mean\")\n\nIn many cases there will be very strong overlap between the two metrics, but in my personal experience I find that in unsupervised cases it is usually better to err on the safe side and go with metrics that do not depend on a single decision (hence, the entire purpose of outlier ensembling!)\nLike before we can specify a cut-point to flag outliers. Here, it might be reasonable to set it to the highest 5% of anomaly scores. Unfortunately, there’s not necessarily a clear p-value available here like there was with the PCA anomaly detector. Ideally, if we have some prior information about the expected amount of contamination we could use that as our threshold instead. The plot below displays the 10 observations with the highest anomaly scores. The values flagged here look very similar to the ones that were identified by the PCA anomaly detector as well, which should give us some confidence.\n\n\nCode\nscored_data &lt;- data.frame(df,anom_mean)\n\nflag &lt;- scored_data$anom_mean &gt;= quantile(scored_data$anom_mean, .95)\n\nggplot() +\n  geom_point(data = scored_data, aes(x = stay_len, y = diff), color = '#004488', size = 2, alpha = .25) +\n  geom_point(data = scored_data[flag,], aes(x = stay_len, y = diff), color = '#BB5566', size = 2.5) +\n  labs(x = \"Stay Length\", y = \"Avg. Payment Difference\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nInliers (blue) and outliers (red) displayed in 2D space\n\n\n\n\n\nKNN Anaomaly Detector: Example Function\nHere’s a minimal working example of the procedure above. As we build our ensemble, we’ll come back to this function later.\n\n\nCode\n# Run a principal components anomaly detector\nadKNN &lt;- function(X, k = 5, method = 'max'){\n  \n  # compute k nearest neighbor distances\n  # using kd-trees\n  d &lt;- RANN::nn2(X, k = k+1)\n  d &lt;- d[[2]][,1:k+1]\n  \n  # aggregate scores\n  if(method == 'max')\n    anom &lt;- apply(d, 1, max)\n  else if(method == 'mean')\n    anom &lt;- apply(d, 1, mean)\n  else\n    print(\"Function not found\")\n  \n  return(anom)\n  \n}"
  },
  {
    "objectID": "posts/hello-world/hello.html",
    "href": "posts/hello-world/hello.html",
    "title": "Executing Python Scripts from the Command Line",
    "section": "",
    "text": "Here’s a short one for a Friday afternoon.\nAt my day job we have a pretty simple coding test for first-round interviews for data scientists. I think we are actually pretty realistic and reasonable with what we ask experienced people to do in an interview. We don’t play around with leetcode or hackerrank stuff. We’re generally just looking for people who have a solid statistics and data science background, and who can demonstrate programming prowess. On our team many of us are expected to be “full-stack” (or nearly so), where we handle the development of models and deployment from the ground up.\nWe have a few questions, but one of the ones that seems to trip up a lot of people who claim experience with python is phrased something like this:\n\n“Please create a python script to print ‘Hello World’ and execute it from the command line”\n\nAmazingly, about half of candidates who have big resumes can’t do this! Here’s a tweet from a team member:\n\nUnfortunately, a lot of this is due to how folks are instructed into data science. Lots of programs focus on highly containerized examples where everything is handled within a single notebook. Notebooks can be great for experimenting, but when it comes to model deployment you will need to learn how to set up environments, manage dependencies, etc…"
  },
  {
    "objectID": "posts/hello-world/hello.html#code-tests",
    "href": "posts/hello-world/hello.html#code-tests",
    "title": "Executing Python Scripts from the Command Line",
    "section": "",
    "text": "Here’s a short one for a Friday afternoon.\nAt my day job we have a pretty simple coding test for first-round interviews for data scientists. I think we are actually pretty realistic and reasonable with what we ask experienced people to do in an interview. We don’t play around with leetcode or hackerrank stuff. We’re generally just looking for people who have a solid statistics and data science background, and who can demonstrate programming prowess. On our team many of us are expected to be “full-stack” (or nearly so), where we handle the development of models and deployment from the ground up.\nWe have a few questions, but one of the ones that seems to trip up a lot of people who claim experience with python is phrased something like this:\n\n“Please create a python script to print ‘Hello World’ and execute it from the command line”\n\nAmazingly, about half of candidates who have big resumes can’t do this! Here’s a tweet from a team member:\n\nUnfortunately, a lot of this is due to how folks are instructed into data science. Lots of programs focus on highly containerized examples where everything is handled within a single notebook. Notebooks can be great for experimenting, but when it comes to model deployment you will need to learn how to set up environments, manage dependencies, etc…"
  },
  {
    "objectID": "posts/hello-world/hello.html#the-question",
    "href": "posts/hello-world/hello.html#the-question",
    "title": "Executing Python Scripts from the Command Line",
    "section": "The Question",
    "text": "The Question\nSo how do we go about answering this question? Well, I can break it down quite simply:\n\nStep 1\nFirst I will assume you have python installed, and it is on your PATH (see a StackExchange post here if you are not familiar). This is necessary so you can call python directly from the command line. When you’re installing python for the first time you typically can just click an option to add python to the PATH variables on Windows.\n\n\nStep 2\nNow we just need to write the file and save it somewhere:\n\nprint(\"Hello World\")\n\nYou can literally just open a text file, add the code above, and then save the text file with a .py extension. I just saved it as demo.py.\n\n\n\nHere is our lonely ‘Hello World’ file\n\n\n\n\nStep 3\nFinally, we just need to execute the python file. Book up command prompt (can click the little window icon and type cmd to get it to pop up). Just navigate to where ever the file is (or call the full file path directly) and append python in front of it.\n\n\n\nVoila! We’ve passed the first question!"
  },
  {
    "objectID": "posts/hello-world/hello.html#so-what",
    "href": "posts/hello-world/hello.html#so-what",
    "title": "Executing Python Scripts from the Command Line",
    "section": "So What?",
    "text": "So What?\nI know this is a bit of an “old man yells at cloud” kind of post, but I do have a deeper point here. Back when I was teaching stats as a professor many, many of my students did not know how file paths worked, how to navigate through directories, or even how to find a file that they downloaded off the class page! I don’t blame them at all - moreso I blame how most children and teens interact with computers - which is almost exclusively through apps and things like Chromebooks. They certainly make the user experience easy, but leave you with little understanding of how these tools actually work. For most people who don’t need to work in-depth with computers, it might be a huge issue. However, when we have data scientists who are flaunting 4-5 years of python experience and can’t execute a file using command prompt, it makes me a bit more concerned.\nPersonally, I think a lot of these “bootcamps” that promise to turn you into a fully-fledged data scientist in 12 weeks don’t really give you much more information than how to run off-the-shelf models in a notebook. Definitely good for a start, but woefully inadequate for real-world work. As a self-taught programmer I would highly encourage people interested in data-focused jobs to expand their horizons and broaden their skills.\nIncidentally, my friend Andy Wheeler has a book which is a really great into to python on his personal website here. If you are at all interested in dipping your toes into python (especially if you are a crime analyst) I would highly reccomend it."
  },
  {
    "objectID": "posts/gmm/gmm.html",
    "href": "posts/gmm/gmm.html",
    "title": "Good Mythical Morning: Blind Fast Food Taste Tests",
    "section": "",
    "text": "So, its no secret that my wife and I are fans of Rhett and Link’s YouTube series Good Mythical Morning. Specifically, we are fans of the large variety of food- related content like Will it Burger?, Worst Halloween Candy Taste Test, International Burger King Taste Test, and 100 Years of Party Snacks Taste Test.\nWhile normally this would qualify as pretty generic YouTube content, Good Mythical Morning keeps it interesting by “gamifying” many of these segments. These including making the hosts try gross food mash-ups, throw darts at a map to guess the food’s country of origin, or use a shuffleboard to guess the decade the food originated from.\nOne of the games that is common on the channel is the Blind Taste Test where Rhett and Link are presented with food items from different fast food or restaurant chains, and are tasked with guessing which items come from which location. For example, in the Blind Chicken Nugget Taste Test they are given nuggets from 6 locations (McDonalds, Wendys, Chic-Fil-A, Hardees, KFC, and Frozen Tyson Nuggets) and have to try and place them based on taste alone. It’s silly content, but fun."
  },
  {
    "objectID": "posts/gmm/gmm.html#good-mythical-morning---a-whole-lotta-food",
    "href": "posts/gmm/gmm.html#good-mythical-morning---a-whole-lotta-food",
    "title": "Good Mythical Morning: Blind Fast Food Taste Tests",
    "section": "",
    "text": "So, its no secret that my wife and I are fans of Rhett and Link’s YouTube series Good Mythical Morning. Specifically, we are fans of the large variety of food- related content like Will it Burger?, Worst Halloween Candy Taste Test, International Burger King Taste Test, and 100 Years of Party Snacks Taste Test.\nWhile normally this would qualify as pretty generic YouTube content, Good Mythical Morning keeps it interesting by “gamifying” many of these segments. These including making the hosts try gross food mash-ups, throw darts at a map to guess the food’s country of origin, or use a shuffleboard to guess the decade the food originated from.\nOne of the games that is common on the channel is the Blind Taste Test where Rhett and Link are presented with food items from different fast food or restaurant chains, and are tasked with guessing which items come from which location. For example, in the Blind Chicken Nugget Taste Test they are given nuggets from 6 locations (McDonalds, Wendys, Chic-Fil-A, Hardees, KFC, and Frozen Tyson Nuggets) and have to try and place them based on taste alone. It’s silly content, but fun."
  },
  {
    "objectID": "posts/gmm/gmm.html#how-good-are-they",
    "href": "posts/gmm/gmm.html#how-good-are-they",
    "title": "Good Mythical Morning: Blind Fast Food Taste Tests",
    "section": "How Good Are They?",
    "text": "How Good Are They?\nOne nagging question I’ve always had is whether their performance on these segments were any better than if they just randomly guessed each time. Blind taste-testing is actually very hard, and distinguishing between 6 different pieces of mass-produced fried meat is probably even harder. Indeed, their performance on a lot of these segments is actually not great. To answer this, I gathered data from all the blind fast food taste tests that I could find, and collected it into a Google Sheet. Most of the games have 6 options, but a few have 5. For simplicity I will focus on the 26 segments where there were six choices available.\n\nChecking Out the Data\n\n\nCode\nlibrary(googlesheets4)\nlibrary(tidyverse)\n\nset.seed(7132322)\n\n# Load Data\n# ---------------\n\ngsheet &lt;-\n  \"https://docs.google.com/spreadsheets/d/1P_LIZnnnaoCZHwmmrBM3O26sPAGCekVj_3qmWyyxTpc/edit?usp=sharing\"\n\ntaste_test &lt;- read_sheet(ss = gsheet) %&gt;%\n  filter(options == 6)\n\n\nThe sheet here has columns corresponding to each game played food, with Rhett and Link’s number of correct guesses stored in their own columns.\n\n\nCode\nhead(taste_test)\n\n\n# A tibble: 6 × 4\n  food          options rhett  link\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 fries               6     4     3\n2 nuggets             6     6     3\n3 fried chicken       6     0     2\n4 bbq pork            6     2     3\n5 donut               6     0     1\n6 taco                6     2     2\n\n\nFirst, let’s look at the distribution of correct guesses for Rhett and Link. We can generate just a simple bar chart here:\n\n\nCode\n# Plot R v L\n# ---------------\n\n# plot observed correct answers for R&L\npar(mfrow = c(1,2))\nbarplot(prop.table(table(taste_test$rhett)), \n        main = \"Rhett\", \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$rhett),3)),\n        col = '#e6862c')\nbarplot(prop.table(table(taste_test$link)), \n        col = '#4ec726', \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$link),3)),\n        main = \"Link\")\n\n\n\n\n\n\n\n\n\nStrangely enough, based on the 26 games that I collected, Rhett and Link have the exact same number of correct guesses. We can see, however, that their distributions are quite different (Rhett has a single game with all 6 right, and several with 4 correct, but also many more games with none correct). As expected, across all games we see their average is about 1.9 correct answers. Now, recall, our question here is “is Rhett and Link’s performance better than chance alone?”\n\n\nRandom Guessing With Replacement\nSo if we want to compare their performance to “chance” - that is, totally random guessing - we need a probability distribution. Here, the binomial is logical starting point. Let’s start with the simplest assumption of randomly guessing one of the six options with replacement (meaning you can guess the same option twice). While this doesn’t necessarily seem like an ideal strategy, Rhett and Link often do guess the same location two or three times.\nWe can use the probability density function for the binomial distribution to calculate the probability of guessing between 0 and 6 items correctly, where each trial is assumed to be independent. Below, we see that there is about a 67% probability of guessing at least one of the items correctly, and a 40% chance of getting exactly one correct. Getting more than 4 correct is very rare. If we look at the observed results above, we see this mostly matches up. Both Rhett and Link rarely get more that 3 correct.\n\n\nCode\n# totally random (w replacement)\nround(dbinom(0:6, 6, 1/6), 3)\n\n\n[1] 0.335 0.402 0.201 0.054 0.008 0.001 0.000\n\n\n\n\nRandom Guessing Without Replacement\nNow, another strategy might be randomly guess, but not repeat any previous guesses. Instead of the guesses being independent, they are now dependent on prior guesses. That means we need to use a different method to calculate this. The hypergeometric distribution is commonly used to calculate this (e.g. dhyper()). This also falls within the realm of permutation-based math ala derangements. However, I am a simple man with a good computer and a less-good maths background, so I will simply simulate it.\n\n\nCode\n# totally random (w/o replacement)\n# simulate\nn = 1e5\n\nalist &lt;- vector(mode = \"list\", length = n)\nfor(j in 1:n){\n  truth &lt;- sample(1:6, 6)\n  guess &lt;- sample(1:6, 6)\n  \n  alist[j] &lt;- sum(guess == truth)\n  \n}\n\nres &lt;- sapply(alist, sum)\nprop.table(table(res))\n\n\nres\n      0       1       2       3       4       6 \n0.37032 0.36459 0.18739 0.05538 0.02081 0.00151 \n\n\nHere we see that if we limit our guesses to only options not previously guessed, there is a roughly equal probability of getting between 0 and 1 answers correct, and about a 28% chance of getting more than 1 correct. Note, under this strategy it is not possible to get 5 answers correct (because if you incorrectly order at least one item, by definition at least one other item is also incorrectly ordered)"
  },
  {
    "objectID": "posts/gmm/gmm.html#are-rhett-and-link-better-than-random-guessing",
    "href": "posts/gmm/gmm.html#are-rhett-and-link-better-than-random-guessing",
    "title": "Good Mythical Morning: Blind Fast Food Taste Tests",
    "section": "Are Rhett and Link Better than Random Guessing?",
    "text": "Are Rhett and Link Better than Random Guessing?\nLet’s review our results here. Here’s what random guessing looks like under our two scenarios:\n\n\nCode\n# plot two different 'null' distributions\npar(mfrow = c(1, 2))\nbarplot(prop.table(round(dbinom(0:6, 6, 1 / 6), 3)),\n        names.arg = 0:6, \n        main = \"With Replacement\", \n        col = \"#dbd895\")\n\nbarplot(prop.table(table(res)), \n        main = \"Without Replacement\",\n        col = \"#dbd895\")\n\n\n\n\n\n\n\n\n\nAnd here’s what Rhett and Link’s actual results look like:\n\n\nCode\n# plot observed correct answers for R&L\npar(mfrow = c(1,2))\nbarplot(prop.table(table(taste_test$rhett)), \n        main = \"Rhett\", \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$rhett),3)),\n        col = '#e6862c')\nbarplot(prop.table(table(taste_test$link)), \n        col = '#4ec726', \n        sub = paste0(\"Avg. Correct: \",round(mean(taste_test$link),3)),\n        main = \"Link\")\n\n\n\n\n\n\n\n\n\nThe expected value of random guessing in both scenarios is, approximately 1, meaning that Rhett and Link’s scores of 1.8 mean that, on average, they perform marginally better than if you guessed completely at random. This is on the order of about 1 additional item correct compared to just guessing 100% randomly.\n\n\nCode\n# expected value\nmean(rbinom(1e5, 6, 1/6))\n\n\n[1] 1.00159\n\n\nCode\nmean(res)\n\n\n[1] 0.99781\n\n\nSo, congrats Rhett and Link! You are a little bit better at blind tasting fast food than rolling a six-sided dice!"
  },
  {
    "objectID": "posts/distance-plotting/distance-plotting.html",
    "href": "posts/distance-plotting/distance-plotting.html",
    "title": "How to Draw Lines Between Pairs of Points in R",
    "section": "",
    "text": "Here’s a quick one. I was recently asked how you might plot the travel of individuals over time on a map. For example, if you had longitudinal data recording the residences of respondents over a course of many years, it might be interesting to see to where and how far they traveled. Doing this in R isn’t too difficult, but it isn’t quite straightforward either. Below I’ll show off my approach using the sf package.\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\nset.seed(111424)\n\n# load data\n# cities data: https://simplemaps.com/data/us-cities\n# usa states: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip\n\ncities &lt;- read_csv(\"./data/uscities.csv\")\nusa &lt;- st_read(\"./data/usa_states/cb_2018_us_state_500k.shp\")\n\n# create a base layer map\nusa_map &lt;-\n  usa %&gt;%\n  filter(NAME %in% state.name,!NAME %in% c(\"Alaska\", \"Hawaii\")) %&gt;%\n  st_transform(crs = 4326)\n\n# subset 50 largest us cities\ncities_sub &lt;- cities %&gt;%\n  filter(state_name %in% state.name,!state_name %in% c(\"Alaska\", \"Hawaii\")) %&gt;%\n  slice(1:50)\n\n\nLet’s say we have some data which lists the name of a person, the cities they’ve been to, and the dates they moved. We want to create a plot that draws a line (in order) of their travel between cities. A sample dataset might look something like this below. We have a person identifier and a sequence of dates that display the dates they lived in a location, along with the associated latitude and longitude.\n\n\nCode\nN &lt;- sample(1:length(cities_sub), 3)\n\n# sample of data\nsample_d &lt;-\n  cities_sub %&gt;%\n  slice(N) %&gt;%\n  mutate(person_id = 'a12345',\n         from_date = as.Date(c('2016-12-31',\n                               '2018-04-07',\n                               '2024-03-03'))) %&gt;%\n  select(person_id, city, from_date, lat, lng)\n\nkable(sample_d)\n\n\n\n\n\nperson_id\ncity\nfrom_date\nlat\nlng\n\n\n\n\na12345\nSeattle\n2016-12-31\n47.6211\n-122.3244\n\n\na12345\nHouston\n2018-04-07\n29.7860\n-95.3885\n\n\na12345\nPhoenix\n2024-03-03\n33.5722\n-112.0892\n\n\n\n\n\n\n\nNow what we want to do is find a way to plot these as a linestring on a map. To do this we can create a simple function that will take this dataframe as input, and assume that for each sequence of points they are ordered from oldest to newest. The function will then extract the points and create an st_linestring object that links them together. Because sf objects interface well with ggplot you can easily make a direct call to plot ontop of a base map.\n\n\nCode\n# function to iterate through n number of points\n\n# given some input distance data 'dd'\n# function expects to see a lng, lat\n# and rows sorted by sequence\n\ndistance_linestring &lt;- function(dd){\n  points_list &lt;- list()\n  idx = 1\n  for(i in 1:nrow(dd)){\n    points_list[[idx]] &lt;- st_point(c(dd$lng[idx],dd$lat[idx]))\n    idx = idx+1\n  }\n  ls = st_linestring(do.call(rbind, points_list)) %&gt;% st_sfc(crs = 4326)\n  \n  return(ls)\n}\n\n# let's draw a line between three random cities\nd1 = distance_linestring(sample_d)\n\n\nAfter calling our distance_linestring function we take the output d1 and plot it on our basemap.\n\n\nCode\n# set up base map\nbase_map &lt;- ggplot() + \n  geom_sf(data = usa_map, fill = '#FFFFFF', color = '#BBBBBB')\n\nbase_map +\n  geom_sf(data = d1, color = '#BB5566', linewidth = 1) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nAnd there we go! A single journey.\n\n\nA more common example might ask us to visualize patterns that many people take - for instance, all participants of a longitudinal survey. We can easily extend the function defined above and wrap it in a for-loop. To illustrate what this looks like I simulate some data for 100 theoretical trips between 2 and 5 cities:\n\n\nCode\n# OK, let's simulate 100 people travelling up to 5 cities\n# then we store the results in a list and plot them on a base map\nlinestring_list &lt;- list()\niter = 100\nmax_N = 5\n\nfor(i in 1:iter){\n k &lt;- sample(2:max_N,1)\n N &lt;- sample(1:length(cities_sub),k)\n \n linestring_list[[i]] &lt;- distance_linestring(cities_sub[N,])\n}\n\n\n# reset basemap\nbase_map &lt;- ggplot() + \n  geom_sf(data = usa_map, fill = '#FFFFFF', color = '#BBBBBB')\n\n# iterate through the list of locations and add each to the plot\nfor(p in 1:length(linestring_list)){\n  base_map = base_map + geom_sf(data = linestring_list[[p]], color = '#BB5566', linewidth = 1, alpha = .2)\n}\n\n\nSo we just simulate a lot of journeys that go between 2 and 5 states, store them in a list, then run our linestring_list function over it. The for-loop to add lines is a bit hack-y, but it works. We can then just plot them out:\n\n\nCode\nbase_map +\n  theme_void()\n\n\n\n\n\n\n\n\n\nAnd if we want to know how far, on average, each person traveled, we can just compute the sum of distances across our list. Simple!\n\n\nCode\n# distance in meters\ndists_m &lt;- sapply(linestring_list, st_length)\n\nhist(dists_m/1609, xlab = \"Distance in Miles\", main = \"Miles Travelled\")"
  },
  {
    "objectID": "posts/distance-plotting/distance-plotting.html#drawing-sequences-of-lines",
    "href": "posts/distance-plotting/distance-plotting.html#drawing-sequences-of-lines",
    "title": "How to Draw Lines Between Pairs of Points in R",
    "section": "",
    "text": "Here’s a quick one. I was recently asked how you might plot the travel of individuals over time on a map. For example, if you had longitudinal data recording the residences of respondents over a course of many years, it might be interesting to see to where and how far they traveled. Doing this in R isn’t too difficult, but it isn’t quite straightforward either. Below I’ll show off my approach using the sf package.\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\nset.seed(111424)\n\n# load data\n# cities data: https://simplemaps.com/data/us-cities\n# usa states: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip\n\ncities &lt;- read_csv(\"./data/uscities.csv\")\nusa &lt;- st_read(\"./data/usa_states/cb_2018_us_state_500k.shp\")\n\n# create a base layer map\nusa_map &lt;-\n  usa %&gt;%\n  filter(NAME %in% state.name,!NAME %in% c(\"Alaska\", \"Hawaii\")) %&gt;%\n  st_transform(crs = 4326)\n\n# subset 50 largest us cities\ncities_sub &lt;- cities %&gt;%\n  filter(state_name %in% state.name,!state_name %in% c(\"Alaska\", \"Hawaii\")) %&gt;%\n  slice(1:50)\n\n\nLet’s say we have some data which lists the name of a person, the cities they’ve been to, and the dates they moved. We want to create a plot that draws a line (in order) of their travel between cities. A sample dataset might look something like this below. We have a person identifier and a sequence of dates that display the dates they lived in a location, along with the associated latitude and longitude.\n\n\nCode\nN &lt;- sample(1:length(cities_sub), 3)\n\n# sample of data\nsample_d &lt;-\n  cities_sub %&gt;%\n  slice(N) %&gt;%\n  mutate(person_id = 'a12345',\n         from_date = as.Date(c('2016-12-31',\n                               '2018-04-07',\n                               '2024-03-03'))) %&gt;%\n  select(person_id, city, from_date, lat, lng)\n\nkable(sample_d)\n\n\n\n\n\nperson_id\ncity\nfrom_date\nlat\nlng\n\n\n\n\na12345\nSeattle\n2016-12-31\n47.6211\n-122.3244\n\n\na12345\nHouston\n2018-04-07\n29.7860\n-95.3885\n\n\na12345\nPhoenix\n2024-03-03\n33.5722\n-112.0892\n\n\n\n\n\n\n\nNow what we want to do is find a way to plot these as a linestring on a map. To do this we can create a simple function that will take this dataframe as input, and assume that for each sequence of points they are ordered from oldest to newest. The function will then extract the points and create an st_linestring object that links them together. Because sf objects interface well with ggplot you can easily make a direct call to plot ontop of a base map.\n\n\nCode\n# function to iterate through n number of points\n\n# given some input distance data 'dd'\n# function expects to see a lng, lat\n# and rows sorted by sequence\n\ndistance_linestring &lt;- function(dd){\n  points_list &lt;- list()\n  idx = 1\n  for(i in 1:nrow(dd)){\n    points_list[[idx]] &lt;- st_point(c(dd$lng[idx],dd$lat[idx]))\n    idx = idx+1\n  }\n  ls = st_linestring(do.call(rbind, points_list)) %&gt;% st_sfc(crs = 4326)\n  \n  return(ls)\n}\n\n# let's draw a line between three random cities\nd1 = distance_linestring(sample_d)\n\n\nAfter calling our distance_linestring function we take the output d1 and plot it on our basemap.\n\n\nCode\n# set up base map\nbase_map &lt;- ggplot() + \n  geom_sf(data = usa_map, fill = '#FFFFFF', color = '#BBBBBB')\n\nbase_map +\n  geom_sf(data = d1, color = '#BB5566', linewidth = 1) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nAnd there we go! A single journey.\n\n\nA more common example might ask us to visualize patterns that many people take - for instance, all participants of a longitudinal survey. We can easily extend the function defined above and wrap it in a for-loop. To illustrate what this looks like I simulate some data for 100 theoretical trips between 2 and 5 cities:\n\n\nCode\n# OK, let's simulate 100 people travelling up to 5 cities\n# then we store the results in a list and plot them on a base map\nlinestring_list &lt;- list()\niter = 100\nmax_N = 5\n\nfor(i in 1:iter){\n k &lt;- sample(2:max_N,1)\n N &lt;- sample(1:length(cities_sub),k)\n \n linestring_list[[i]] &lt;- distance_linestring(cities_sub[N,])\n}\n\n\n# reset basemap\nbase_map &lt;- ggplot() + \n  geom_sf(data = usa_map, fill = '#FFFFFF', color = '#BBBBBB')\n\n# iterate through the list of locations and add each to the plot\nfor(p in 1:length(linestring_list)){\n  base_map = base_map + geom_sf(data = linestring_list[[p]], color = '#BB5566', linewidth = 1, alpha = .2)\n}\n\n\nSo we just simulate a lot of journeys that go between 2 and 5 states, store them in a list, then run our linestring_list function over it. The for-loop to add lines is a bit hack-y, but it works. We can then just plot them out:\n\n\nCode\nbase_map +\n  theme_void()\n\n\n\n\n\n\n\n\n\nAnd if we want to know how far, on average, each person traveled, we can just compute the sum of distances across our list. Simple!\n\n\nCode\n# distance in meters\ndists_m &lt;- sapply(linestring_list, st_length)\n\nhist(dists_m/1609, xlab = \"Distance in Miles\", main = \"Miles Travelled\")"
  },
  {
    "objectID": "posts/distance-plotting/distance-plotting.html#full-data",
    "href": "posts/distance-plotting/distance-plotting.html#full-data",
    "title": "How to Draw Lines Between Pairs of Points in R",
    "section": "Full Data",
    "text": "Full Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\nset.seed(111424)\n\n# load data\n# cities data: https://simplemaps.com/data/us-cities\n# usa states: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip\n\ncities &lt;- read_csv(\"./data/uscities.csv\")\nusa &lt;- st_read(\"./data/usa_states/cb_2018_us_state_500k.shp\")\n\n# create a base layer map\nusa_map &lt;-\n  usa %&gt;%\n  filter(NAME %in% state.name,!NAME %in% c(\"Alaska\", \"Hawaii\")) %&gt;%\n  st_transform(crs = 4326)\n\n# subset 50 largest us cities\ncities_sub &lt;- cities %&gt;%\n  filter(state_name %in% state.name,!state_name %in% c(\"Alaska\", \"Hawaii\")) %&gt;%\n  slice(1:50)\n\n# function to iterate through n number of points\n\n# given some input distance data 'dd'\n# function expects to see a lng, lat\n# and rows sorted by sequence\n\ndistance_linestring &lt;- function(dd){\n  points_list &lt;- list()\n  idx = 1\n  for(i in 1:nrow(dd)){\n    points_list[[idx]] &lt;- st_point(c(dd$lng[idx],dd$lat[idx]))\n    idx = idx+1\n  }\n  ls = st_linestring(do.call(rbind, points_list)) %&gt;% st_sfc(crs = 4326)\n  \n  return(ls)\n}\n\n# let's draw a line between three random cities\nd1 = distance_linestring(sample_d)\n\n# set up base map\nbase_map &lt;- ggplot() + \n  geom_sf(data = usa_map, fill = '#FFFFFF', color = '#BBBBBB')\n\nbase_map +\n  geom_sf(data = d1, color = '#BB5566', linewidth = 1) +\n  theme_void()\n\n# OK, let's simulate 100 people travelling up to 5 cities\n# then we store the results in a list and plot them on a base map\nlinestring_list &lt;- list()\niter = 100\nmax_N = 5\n\nfor(i in 1:iter){\n k &lt;- sample(2:max_N,1)\n N &lt;- sample(1:length(cities_sub),k)\n \n linestring_list[[i]] &lt;- distance_linestring(cities_sub[N,])\n}\n\n\n# reset basemap\nbase_map &lt;- ggplot() + \n  geom_sf(data = usa_map, fill = '#FFFFFF', color = '#BBBBBB')\n\n# iterate through the list of locations and add each to the plot\nfor(p in 1:length(linestring_list)){\n  base_map = base_map + geom_sf(data = linestring_list[[p]], color = '#BB5566', linewidth = 1, alpha = .2)\n}\n\nbase_map +\n  theme_void()\n\n# distance in meters\ndists_m &lt;- sapply(linestring_list, st_length)\n\nhist(dists_m/1609, xlab = \"Distance in Miles\", main = \"Miles Travelled\")"
  },
  {
    "objectID": "posts/coffee/coffee.html",
    "href": "posts/coffee/coffee.html",
    "title": "The Great American Coffee Taste Test",
    "section": "",
    "text": "In October I was lucky enough to participate in popular coffee YouTuber James Hoffman’s Great American Coffee Taste Test. In short, participants got 4 samples of coffee and were able to brew, taste, and rate them live. One the interesting parts of this was that the data was freely shared after the tasting was completed. As both a data and coffee nerd, I couldn’t resit a dive into this dataset.\nThe one we’re going to focus on is the unusual Coffee ‘D’. In contrast to the other coffees in the taste test, Coffee ‘D’ was a natural process. The difference between a washed coffee and natural coffee is:\n\nWashed coffee and natural process coffee differ primarily in their processing methods. Washed coffee involves removing the outer fruit from the coffee bean before drying, using water to ferment and wash away the pulp, resulting in a clean, bright flavor profile with pronounced acidity. In contrast, natural process coffee involves drying the whole coffee cherry intact, allowing the bean to ferment inside the fruit, imparting a fruitier, sometimes wine-like, sweetness with a heavier body due to prolonged contact with the fruit, often exhibiting complex, earthy, or fermented flavor notes. These distinct processes significantly influence the taste and characteristics of the final brew, offering a spectrum of flavors to coffee enthusiasts.\n\nThe tl;dr is that natural process coffees tend to have more fermented, fruity flavors that are prized by some consumers, but often disliked by others. This is the one we’re going to focus our attention on here.\n\n\nWhile there were numerous questions on the survey, my focus was primarily on the following:\n\nAge\nGender\nSelf-rated coffee expertise\n\nI categorized ages into groups (18-24, 25-34, 35-44, 45-54, and 55+), and gender into (Male, Female). The self-rated coffee expertise was on a scale from 1 to 10, with 1 representing “I’m a novice” and 10 representing “I’m a coffee expert.”\nInitially, we need to convert the survey data from a wide format to a long one. In the current data view, each person’s response is repeated four times (once for each coffee type), while their age, gender, and self-reported coffee expertise remain constant. This approach allows us to model responses more efficiently and retain information across different coffee types.\n\n\n\n\n\nage\ngender\nexpertise\ncoffee\nranking\n\n\n\n\n18-24\nOther (please specify)\n10\npref_a\n1\n\n\n18-24\nOther (please specify)\n10\npref_b\n1\n\n\n18-24\nOther (please specify)\n10\npref_c\n1\n\n\n18-24\nOther (please specify)\n10\npref_d\n1\n\n\n55+\nNot Provided\n7\npref_a\n3\n\n\n55+\nNot Provided\n7\npref_b\n3\n\n\n55+\nNot Provided\n7\npref_c\n3\n\n\n55+\nNot Provided\n7\npref_d\n3\n\n\n25-34\nFemale\n6\npref_a\n3\n\n\n25-34\nFemale\n6\npref_b\n3"
  },
  {
    "objectID": "posts/coffee/coffee.html#the-great-american-coffee-taste-test",
    "href": "posts/coffee/coffee.html#the-great-american-coffee-taste-test",
    "title": "The Great American Coffee Taste Test",
    "section": "",
    "text": "In October I was lucky enough to participate in popular coffee YouTuber James Hoffman’s Great American Coffee Taste Test. In short, participants got 4 samples of coffee and were able to brew, taste, and rate them live. One the interesting parts of this was that the data was freely shared after the tasting was completed. As both a data and coffee nerd, I couldn’t resit a dive into this dataset.\nThe one we’re going to focus on is the unusual Coffee ‘D’. In contrast to the other coffees in the taste test, Coffee ‘D’ was a natural process. The difference between a washed coffee and natural coffee is:\n\nWashed coffee and natural process coffee differ primarily in their processing methods. Washed coffee involves removing the outer fruit from the coffee bean before drying, using water to ferment and wash away the pulp, resulting in a clean, bright flavor profile with pronounced acidity. In contrast, natural process coffee involves drying the whole coffee cherry intact, allowing the bean to ferment inside the fruit, imparting a fruitier, sometimes wine-like, sweetness with a heavier body due to prolonged contact with the fruit, often exhibiting complex, earthy, or fermented flavor notes. These distinct processes significantly influence the taste and characteristics of the final brew, offering a spectrum of flavors to coffee enthusiasts.\n\nThe tl;dr is that natural process coffees tend to have more fermented, fruity flavors that are prized by some consumers, but often disliked by others. This is the one we’re going to focus our attention on here.\n\n\nWhile there were numerous questions on the survey, my focus was primarily on the following:\n\nAge\nGender\nSelf-rated coffee expertise\n\nI categorized ages into groups (18-24, 25-34, 35-44, 45-54, and 55+), and gender into (Male, Female). The self-rated coffee expertise was on a scale from 1 to 10, with 1 representing “I’m a novice” and 10 representing “I’m a coffee expert.”\nInitially, we need to convert the survey data from a wide format to a long one. In the current data view, each person’s response is repeated four times (once for each coffee type), while their age, gender, and self-reported coffee expertise remain constant. This approach allows us to model responses more efficiently and retain information across different coffee types.\n\n\n\n\n\nage\ngender\nexpertise\ncoffee\nranking\n\n\n\n\n18-24\nOther (please specify)\n10\npref_a\n1\n\n\n18-24\nOther (please specify)\n10\npref_b\n1\n\n\n18-24\nOther (please specify)\n10\npref_c\n1\n\n\n18-24\nOther (please specify)\n10\npref_d\n1\n\n\n55+\nNot Provided\n7\npref_a\n3\n\n\n55+\nNot Provided\n7\npref_b\n3\n\n\n55+\nNot Provided\n7\npref_c\n3\n\n\n55+\nNot Provided\n7\npref_d\n3\n\n\n25-34\nFemale\n6\npref_a\n3\n\n\n25-34\nFemale\n6\npref_b\n3"
  },
  {
    "objectID": "posts/coffee/coffee.html#fitting-a-bayesian-model",
    "href": "posts/coffee/coffee.html#fitting-a-bayesian-model",
    "title": "The Great American Coffee Taste Test",
    "section": "Fitting A Bayesian Model",
    "text": "Fitting A Bayesian Model\nWe’re employing an ordinal regression model with a cumulative link function, which is a typical method for analyzing Likert-style data. Gender remains constant across all coffee categories, while we permit the effects of age and expertise to differ for each type of coffee. Essentially, this suggests that we assume broader gender differences for all coffees, while acknowledging that the effects of age and expertise may differ across various coffee types.\n\n# set reasonable priors\nprior &lt;- c(prior(normal(0,2), class = Intercept),\n           prior(normal(0,2), class = b),\n           prior(normal(0,2), class = sd))\n\n# hlm with varying slopes for expertise\n# and varying intercepts for age\nfit2 &lt;-\n  brm(\n    ranking ~ 1 + \n      gender +\n      (age + expertise | coffee),\n    data = coffee_ranking,\n    prior = prior,\n    family = cumulative(\"probit\"),\n    chains = 4,\n    cores = 4,\n    iter = 2000,\n    control = list(adapt_delta = 0.9)\n  )\n\nThere are a few divergent transitions after fitting, but that can be fixed by upping the adapt_delta parameter. In general I’m satisfied with the fit, as all of our Rhat values are equal to 1.00, and the Bulk_ESS and Tail_ESS look fine too.\nAfter fitting the model, we can get some initial insights by plotting out some of the model coefficients. Specifically, we probably want to focus our attention on the varying effects for age and expertise on coffee preferences. We can do this by extracting the random effects from the model and plotting them. Here, we can see the estimated effect of age on preferences for each coffee (relative to the 18-24 group).\n\n\n\n\n\nEffect of age on coffee preferences. Older individuals tend to rate coffee D lower.\n\n\n\n\n…and the effect of self-reported expertise:\n\n\n\n\n\nEffect of expertise on coffee preferences. Individuals with higher expertise tend to rate coffee ‘A’ and ‘D’ higher.\n\n\n\n\nFrom this we see results that largely fit with what was reported in the video. Older people tend to dislike Coffee ‘A’ and ‘D’ more, and people with higher expertise tend to like them more.\n\nDigging a little deeper: Males vs. Females\nLet’s start by looking at all the predicted rankings for coffee ‘D’. In this case we have 500 different predictions, corresponding to all possible age x gender x expertise x ranking combinations. Each individual line represents the estimated ranking for a particular consumer.\n\n\n\n\n\nPredictions for all consumer groups. Each line represents a single age, gender, and expertise combination.\n\n\n\n\nUsing this data we can do some simple comparisons. For example: what is the estimated difference between males and females ranking coffee ‘D’? In his summary, James Hoffman highlighted that females were much more likely to strongly dislike coffee ‘D’ relative to males. However, if you look at the data, females in the survey also generally reported much less expertise in coffee.\nSo what if we control for expertise, and see what the estimated difference in gender is assuming they are otherwise comparable coffee consumers. Below, we set the expertise for predictions to the median, which is 6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Coffee rankings for males and females, across all age groups\n\n\n\nAnd here are the estimates for males and females, holding age constant at 25-34 and self-reported expertise to the median (6). As we can see both males and females with similar expertise within the same age groups largely rate coffee D similarly, with the largest difference being for ranking it a 5.\n\n\n\n\n\nEffect of gender on preferences for coffee ‘D’ holding age constant to 25-34.\n\n\n\n\nSo really, there aren’t very large difference in regards to gender after we control for expertise. The large gender difference that James sees in the survey is likely mostly an artifact of differences in experience with coffee between males and females1.\n\n\n\n\n\nFemale respondents were less likely to report high expertise with coffee, relative to males.\n\n\n\n\nIn fact, if we look at the differences in self-reported expertise we have estimated rankings for a male, aged 25-34 for self-reported expertise at 1, 5, and 10. As is fairly clear, the distribution of estimated rankings is substantially different across levels of expertise. For example, we estimate about an 80% probability that a person with a self-assessed expertise of 10 to rate coffee ‘D’ a 4 or 5. In contrast that drops to about 45% for a person with an expertise of 5, and 20% for a person with an expertise of 1.\n\n\n\n\n\nEffect of expertise on preferences for coffee ‘D’ holding age and gender constant to 25-34 and male. Persons with higher coffee expertise are more likely to rate coffee ‘D’ higher.\n\n\n\n\nIf it’s not already clear, the biggest differences in rating the unusual natural-process coffee is not really related to gender, but rather it is mostly based on an individuals expertise or ‘expertise’ with coffee."
  },
  {
    "objectID": "posts/coffee/coffee.html#comparing-two-hypothetical-consumers",
    "href": "posts/coffee/coffee.html#comparing-two-hypothetical-consumers",
    "title": "The Great American Coffee Taste Test",
    "section": "Comparing two hypothetical consumers",
    "text": "Comparing two hypothetical consumers\nSo now that we have our model, we can use it to pose any number of comparisons by relying on posterior draws.\nLet’s first look at the most common age-gender-expertise combinations. Below we see that males largely make up the most common individuals who completed the survey. From a hypothetical perspective, let’s compare how the most common male respondent (between 25-34 years old with an expertise of 7) would rate a given coffee compared to the most common female respondent (25-34 with an expertise of 5).\n\n\nCode\ncoffee_data %&gt;%\n  count(age,gender,expertise) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice(1:10) %&gt;%\n  kable()\n\n\n\n\n\nage\ngender\nexpertise\nn\n\n\n\n\n25-34 years old\nMale\n7\n383\n\n\n25-34 years old\nMale\n6\n305\n\n\n25-34 years old\nMale\n8\n199\n\n\n35-44 years old\nMale\n7\n168\n\n\n25-34 years old\nMale\n5\n159\n\n\n35-44 years old\nMale\n6\n159\n\n\n35-44 years old\nMale\n8\n114\n\n\n25-34 years old\nFemale\n5\n84\n\n\n25-34 years old\nMale\n4\n81\n\n\n25-34 years old\nFemale\n6\n80\n\n\n\n\n\n\n\nFirst we need to compute 4000 posterior draws for each hypothetical user. These will be probabilities for our hypothetical person scoring a given coffee a 1 though a 5. We can get this by calling the posterior_epred function.\n\n# get 4000 posterior draws for two hypothetical individuals\n# for scoring a given coffee\n\n# helper function for generating predictions\nmake_comparisons &lt;-\n  function(fit,\n           pred_coffee,\n           pred_exp,\n           pred_age,\n           pred_gender) {\n    newdata &lt;-\n      data.frame(\n        expertise = pred_exp,\n        age = pred_age,\n        gender = pred_gender,\n        coffee = pred_coffee\n      )\n    \n    return(posterior_epred(fit2, newdata = newdata))\n  }\n\n# get posterior draws\nZ &lt;-\n  make_comparisons(\n    fit2,\n    pred_coffee = c(\"pref_d\"),\n    pred_exp = c(7, 5),\n    pred_age = \"25-34\",\n    pred_gender = c(\"Male\", \"Female\")\n  )\n\n# matrix of estimated differences\n# p1 - p2\np_diff &lt;- Z[, 1, 1:5] - Z[, 2, 1:5]\n\nThen we can put them into a dataframe and plot their distributions. As we can see, they are quite far apart. A 25-34 year old male with an expertise of 7 has a probability of about 35% of scoring coffee ‘D’ a 5, compared to 19% for female with an expertise of 5.\n\n\n\n\n\n\n\n\n\nOne nice thing about a Bayesian approach is that we have access to the full posterior, so we can compute any kind of comparisons. For example, what is the predicted median difference between these two individuals rating coffee ‘D’ a 5, with an 89% credible interval?\n\nquantile(p_diff[,5], probs = c(.06, .5, .94))\n\n       6%       50%       94% \n0.1386954 0.1519386 0.1651294 \n\n\nOr we can plot the estimated difference of males and females for each of the response categories:\n\n\n\n\n\n\n\n\n\nOr what if we had two consumers with similarly rated expertise, but one was much older?\n\nZ2 &lt;-\n  make_comparisons(\n    fit2,\n    pred_coffee = \"pref_d\",\n    pred_exp = 6,\n    pred_age = c(\"25-34\", \"55+\"),\n    pred_gender = \"Male\"\n  )\n\np_diff_2 &lt;- Z2[, 1, 1:5] - Z2[, 2, 1:5]\n\nquantile(p_diff_2[,5], probs = c(.06, .5, .94))\n\n       6%       50%       94% \n0.1952684 0.2140120 0.2323127 \n\n\nWhich suggests that the probability of a 25-34 year old rating coffee ‘D’ a 5 is about 21 percentage points higher than a 55+ year old individual. That’s a huge age difference!"
  },
  {
    "objectID": "posts/coffee/coffee.html#full-data",
    "href": "posts/coffee/coffee.html#full-data",
    "title": "The Great American Coffee Taste Test",
    "section": "Full Data",
    "text": "Full Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(broom.mixed)\n\n# load survey data here\ncoffee &lt;- read_csv(\"/gatt.csv\")\n\ncol_pal &lt;- c( '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377')\n\nset.seed(123)\n\n## Function to extract random effects from brms model\npull_ranef &lt;- function(x, idx){\n  return(\n    data.frame(x[,,idx]) %&gt;%\n  mutate(coffee = rownames(.), variable = idx) \n  )\n}\n\n\n# Setup data\ncoffee_data &lt;-\n  coffee %&gt;%\n  select(\n    age = `What is your age?`,\n    gender = `Gender`,\n    expertise = `Lastly, how would you rate your own coffee expertise?`,\n    pref_a = `Coffee A - Personal Preference`,\n    pref_b = `Coffee B - Personal Preference`,\n    pref_c = `Coffee C - Personal Preference`,\n    pref_d = `Coffee D - Personal Preference`\n  ) %&gt;%\n  replace_na(\n    list(\n      age = 'Not Provided',\n      gender = 'Not Provided',\n      race = 'Not Provided',\n      education = 'Not Provided'\n    )\n  )\n\ncoffee_ranking &lt;-\n  coffee_data %&gt;%\n  na.omit() %&gt;%\n  select(age, gender, expertise, pref_a:pref_d) %&gt;%\n  pivot_longer(cols = starts_with(\"pref\"),\n               names_to = \"coffee\",\n               values_to = \"ranking\") %&gt;%\n  mutate(age = case_when(\n    age %in% c(\"&lt;18 years old\",\"18-24 years old\") ~ \"18-24\",\n    age == \"25-34 years old\" ~ \"25-34\",\n    age == \"35-44 years old\" ~ \"35-44\",\n    age == \"45-54 years old\" ~ \"45-54\",\n    age %in% c(\"55-64 years old\", \"&gt;65 years old\") ~ \"55+\"\n  ))\n\n# fit model\nfit2 &lt;-\n  brm(\n    ranking ~ 1 + \n      gender +\n      (age + expertise | coffee),\n    data = coffee_ranking,\n    prior = prior,\n    family = cumulative(\"probit\"),\n    chains = 4,\n    cores = 4,\n    iter = 2000,\n    control = list(adapt_delta = 0.99)\n  )\n\nstrata &lt;- coffee_ranking %&gt;%\n  filter(gender %in% c(\"Male\",\"Female\")) %&gt;%\n  distinct(expertise, age, gender, coffee) %&gt;%\n  complete(expertise,age,gender,coffee)\n\nfit1_preds &lt;-\n  predict(fit2, newdata = strata) %&gt;%\n  data.frame()\n\npred_data &lt;-\n  tibble(strata, fit1_preds) %&gt;%\n  set_names(c(\n    'expertise',\n    'age',\n    'gender',\n    'coffee',\n    'p1',\n    'p2',\n    'p3',\n    'p4',\n    'p5'\n  )) %&gt;%\n  mutate(\n    gender = fct_relevel(gender, \"Male\"),\n    age = fct_relevel(\n      age,\n      \"18-24\",\n      \"25-34\",\n      \"35-44\",\n      \"45-54\",\n      '55+'\n    )\n  ) %&gt;%\n  pivot_longer(cols = starts_with(\"p\"),\n               names_to = 'ranking',\n               values_to = 'prob')\n\n# random effects\nfit2_summary &lt;- ranef(fit2)[[1]]\nvals &lt;- c(\"Intercept\",\"age25M34\",\"age35M44\",\"age45M54\",\"age55P\",\"expertise\")\n\n# pull into nice dataframe\nres &lt;-\n  sapply(vals, function(x) {\n    pull_ranef(fit2_summary, x)\n  },\n  simplify = FALSE) %&gt;%\n  do.call(rbind, .)"
  },
  {
    "objectID": "posts/coffee/coffee.html#footnotes",
    "href": "posts/coffee/coffee.html#footnotes",
    "title": "The Great American Coffee Taste Test",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGiven the viewership demographics for James’ channel, I actually assume that many of the female ratings are the partners of the male viewers, and are probably not as experienced as their partners↩︎"
  },
  {
    "objectID": "posts/bayes-study-1/bayes-study-1.html",
    "href": "posts/bayes-study-1/bayes-study-1.html",
    "title": "Going Back to (Bayesian) School",
    "section": "",
    "text": "This is just a bit of fun, mostly for myself. In my day-to-day as a data scientist, I don’t get to use Bayesian statistics as much as I would like. In my role, a lot of the tools we use are things like boosted tree models, anomaly detection ensembles, and other less inferential models. That’s not to say I don’t think Bayesian methods can be used at my job, but there is definitely a different toolset involved.\nI do a bit of side work (some for money, others just for fun) where I really do prefer to use Bayesian statistics. I’ve always liked the flexibility it allows in contrast to frequentist methods. In addition, I find it invaluable for stuff like psychometrics (e.g. IRT models) and survey analysis. I have to admit, I am a bit rusty with some of the foundations, and I’ve decided to make an abbreviated run through one of my favorite recent stats books: “Regression and Other Stories”. The previous book that was absolutely invaluable for me in grad school was the 2007 “Data Analysis Using Regression and Multilevel/Hierarchical Models”. So for personal growth and fun, I’m going to work on a few interesting examples from this more recent book, starting from the beginning of the book all the way through the end - tossing in a bit of my own opinions on this."
  },
  {
    "objectID": "posts/bayes-study-1/bayes-study-1.html#working-with-bayesian-statistics",
    "href": "posts/bayes-study-1/bayes-study-1.html#working-with-bayesian-statistics",
    "title": "Going Back to (Bayesian) School",
    "section": "",
    "text": "This is just a bit of fun, mostly for myself. In my day-to-day as a data scientist, I don’t get to use Bayesian statistics as much as I would like. In my role, a lot of the tools we use are things like boosted tree models, anomaly detection ensembles, and other less inferential models. That’s not to say I don’t think Bayesian methods can be used at my job, but there is definitely a different toolset involved.\nI do a bit of side work (some for money, others just for fun) where I really do prefer to use Bayesian statistics. I’ve always liked the flexibility it allows in contrast to frequentist methods. In addition, I find it invaluable for stuff like psychometrics (e.g. IRT models) and survey analysis. I have to admit, I am a bit rusty with some of the foundations, and I’ve decided to make an abbreviated run through one of my favorite recent stats books: “Regression and Other Stories”. The previous book that was absolutely invaluable for me in grad school was the 2007 “Data Analysis Using Regression and Multilevel/Hierarchical Models”. So for personal growth and fun, I’m going to work on a few interesting examples from this more recent book, starting from the beginning of the book all the way through the end - tossing in a bit of my own opinions on this."
  },
  {
    "objectID": "posts/bayes-study-1/bayes-study-1.html#chapter-8-fitting-regression-models",
    "href": "posts/bayes-study-1/bayes-study-1.html#chapter-8-fitting-regression-models",
    "title": "Going Back to (Bayesian) School",
    "section": "Chapter 8: Fitting Regression Models",
    "text": "Chapter 8: Fitting Regression Models\nBefore working on anything too complicated, I think it is refreshing to start with a very simple regression problem. I know this already Chapter 8, but the first several chapters of the book are mostly a very basic intro into Bayesian statistics, data analysis, and some other more standard stuff. I’m choosing to skip ahead a bit. Chapter 8 starts with an introduction to a very simple bivariate regression using an interesting dataset from Douglas Hibbs’ fundementals-based election forecasting model (oddly appropriate, given when I am writing this):\n\nThe Hibbs “Bread and Peace” Model\nThe data below is part of Douglas Hibb’s “Bread and Peace” model of U.S. elections. The data below show the proportion of the 2-party vote share for the incumbent party against a measure of personal income growth during that party’s tenure. As seen in the plot, there appears to be a fairly strong relationship between the economic status under the incumbent party and their share of the 2 party vote.\n\n“Bread and Peace” Raw Data“Bread and Peace” by Year\n\n\n\n\nCode\nplot1.1 &lt;- \n  ggplot(election) +\n  geom_point(aes(x = growth, y = vote), size = 2, shape = 22, fill = '#004488', color = '#004488') +\n  labs(x = \"Growth in Personal Income\", y = \"Incumbent vote share\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 11))\nplot1.1\n\n\n\n\n\nGrowth in personal income is associated with higher vote shares for the incumbent party\n\n\n\n\n\n\n\n\nCode\nplot1.2 &lt;- \n  ggplot(election) +\n  geom_text(aes(x = growth, y = vote, label = year), size = 3.5, fontface = 'bold', color = '#004488') +\n  labs(x = \"Growth in Personal Income\", y = \"Incumbent vote share\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 11))\nplot1.2\n\n\n\n\n\nThe 1980 election saw Jimmy Carter lose decsively to Ronald Regan amid a floundering economy.\n\n\n\n\n\n\n\nGiven this data we, therefore, have the simple regression equation:\n\\[y = \\beta_0 + \\beta_{1}income + \\epsilon\\]\nWhere the expected two party vote share \\(y\\) is assumed to be a function of the change in personal income growth \\(\\beta_1\\) plus unmeasured residual error \\(\\epsilon\\). So far this isn’t any different than your bog-standard lm(y~x) type model. The difference is how a Bayesian model handles the model parameters. In short, while a frequentist model views parameters as “fixed”, a Bayesian model views them as random variables modeled as draws from a posterior distribution. This posterior distribution is formed from a likelihood (the observed data), and a prior distribution.\nBecause this isn’t meant to be a full introduction to Bayesian inference, feel free to review some good articles here, or here.\n\n\nThe Bayesian approach\nFor our models we have two sets of priors. One is a ‘flat’ prior which is drawn from a uniform distribution. This essentially adds zero additional information to the model. The other we can provide is an “informative” prior that contains some knowledge about the value of the estimates. This can be “weakly-informative” which just keeps the estimates within some reasonable bounds, or truly informative which reflects real-world knowledge. Here, I will add an informative prior to the second model reflecting my belief that a 1% increase in personal income growth is likely to increase vote share by about 3%, which is unlikely to be higher 5% or lower than 1%.\n\n\nCode\n# let's regress!\nfit2_prior &lt;- c(prior(normal(3, 1), class = 'b'))\n\n# model 1 has default (flat) priors on betas\n# 1 percentage point in growth is associated with ~ 3% increase in vote share\nfit1 &lt;-\n  brm(vote ~ growth,\n      data = election,\n      family = \"gaussian\",\n      file = \"hibbs_fit1.Rdata\")\ntidy(fit1) %&gt;% kable(digits = 2, caption = \"Flat prior\")\n\n\n\nFlat prior\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ncomponent\ngroup\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\nfixed\ncond\nNA\n(Intercept)\n46.19\n1.75\n42.63\n49.66\n\n\nfixed\ncond\nNA\ngrowth\n3.08\n0.76\n1.54\n4.57\n\n\nran_pars\ncond\nResidual\nsd__Observation\n4.05\n0.81\n2.82\n5.97\n\n\n\n\n\nCode\n# same model, with much tighter priors\n# assuming a mean effect of about 2.5% +- 1.5%\nfit2 &lt;-\n  brm(\n    vote ~ growth,\n    data = election,\n    family = \"gaussian\",\n    prior = fit2_prior,\n    file = \"hibbs_fit2.Rdata\"\n  )\ntidy(fit2) %&gt;% kable(digits = 2, caption = \"Informative prior\")\n\n\n\nInformative prior\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ncomponent\ngroup\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\nfixed\ncond\nNA\n(Intercept)\n46.24\n1.51\n43.19\n49.12\n\n\nfixed\ncond\nNA\ngrowth\n3.03\n0.58\n1.91\n4.17\n\n\nran_pars\ncond\nResidual\nsd__Observation\n4.01\n0.82\n2.83\n6.04\n\n\n\n\n\nIf we look at the parameter estimates from the two models we see they are roughly the same. A 1% increase in personal income growth is associated with a 3% increase increase in vote share for the incumbent party. Our choice of prior doesn’t really affect the point estimate, but we can see that restricting the plausible range has shrunk the standard error and credible interval range. This is because we are adding additional data to the model, based on our prior beliefs. Bayesian statistics in action!\n\n\nPredictions for a new value of x\nOf course, we can also create a prediction for y given a new value of x. For example, what would the expected vote share be, given the growth in personal income was 2%? Here we can access the posterior distribution for a prediction of y. This prediction itself is constructed as a distribution from the posterior, which gives us a range of values for the prediction. Below we see the mean prediction for incumbent vote share where \\(x=2\\) is 52.3, but this could plausibly range between as low as 44 or as high as 61.\n\nEstimate DistributionPoint Estimate and Intervals\n\n\n\n\nCode\n# what is the predicted vote share given a 2% growth rate?\n# ~52%, but with a pretty big margin of error\nnewgrowth = 2.0\nnewprobs = c(.025, .25, .75, 0.975)\n\npred1 &lt;- posterior_predict(fit1, newdata = data.frame(growth=newgrowth))\n\nypred = mean(pred1) \nypred_quantile = quantile(pred1, newprobs)\n\n\nggplot(data.frame(x = pred1)) + geom_density(aes(x = x), linewidth = 1, color = '#DDAA33', fill = '#DDAA33', alpha = .2) +\n  labs(x = \"Predicted Vote Share (x=2)\", y = \"Probability Density\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 11))\n\n\n\n\n\nDistribution of estimates for y, given x = 2\n\n\n\n\n\n\n\n\nCode\n# plot the 50% and 95% credible intervals for the point estimate of 2% growth\nplot1.1 +\n  annotate(geom = \"linerange\", x = newgrowth, ymin = ypred_quantile[2], ymax = ypred_quantile[3], color = '#DDAA33',  linewidth = 1) +\n  annotate(geom = \"linerange\", x = newgrowth, ymin = ypred_quantile[1], ymax = ypred_quantile[4], color = '#DDAA33') +\n  annotate(geom = \"point\", x = newgrowth, y = ypred, fill = '#DDAA33', color = 'white', stroke = 2, size = 3, shape = 21)\n\n\n\n\n\nPoint estimate, and 50% and 95% credible intervals for x=2\n\n\n\n\n\n\n\nWe can also access simulations for the intercepts and slopes of each model. Below we have 100 draws from the posterior distribution, showing a range of values for the intercept and slope. Together, this gives us a range of plausible values for the predicted relationship between personal income growth and vote share.\n\n\nCode\nplot_posterior(fit2)"
  },
  {
    "objectID": "posts/bayes-study-1/bayes-study-1.html#wrapping-it-up",
    "href": "posts/bayes-study-1/bayes-study-1.html#wrapping-it-up",
    "title": "Going Back to (Bayesian) School",
    "section": "Wrapping it Up",
    "text": "Wrapping it Up\nOne lesson down. In this one I just ran through the most basic task of fitting an extremely simple regression. However, all the major parts of the Bayesian workflow are present:\n\nSetting priors\nEstimating parameters from the model\nAccessing posterior draws for prediction\n\nThe third one is probably the most useful and important. Having access to the full posterior gives us incredible flexibility of what we want to do with the model (more on this soon!)"
  },
  {
    "objectID": "posts/bayes-study-1/bayes-study-1.html#full-code",
    "href": "posts/bayes-study-1/bayes-study-1.html#full-code",
    "title": "Going Back to (Bayesian) School",
    "section": "Full Code",
    "text": "Full Code\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom.mixed)\nlibrary(brms)\nlibrary(knitr)\n\nset.seed(55432)\n\n# list of files\ndir1 &lt;- \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectionsEconomy/data/hibbs.dat\"\n\n# EXAMPLE 1: GDP GROWTH AND ELECTION VOTE SHARE\n# quick simple example, using bayes for a linear regression\n# what is the impact of economic growth on incumbent vote share?\nelection &lt;- read_delim(dir1, delim = \" \")\n\n\n# plot 100 simulations from the posterior\nplot_posterior &lt;- function(fit){\n  M &lt;- as.matrix(fit)\n  sims &lt;- 100\n  sims_idx &lt;- sample(1:nrow(M), size=sims)\n  \n  model_sims &lt;-\n    M[sims_idx, ][, c(1, 2)] %&gt;% data.frame() %&gt;% setNames(c(\"intercept\", \"slope\"))\n  \n  plot1.1 +\n    geom_abline(data=model_sims, aes(slope = slope, intercept = intercept), alpha = .2, color = '#004488')\n}\n\nplot1.1 &lt;- \n  ggplot(election) +\n  geom_point(aes(x = growth, y = vote), size = 2, shape = 22, fill = '#004488', color = '#004488') +\n  labs(x = \"Growth in Personal Income\", y = \"Incumbent vote share\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 11))\nplot1.1\n\nplot1.2 &lt;- \n  ggplot(election) +\n  geom_text(aes(x = growth, y = vote, label = year), size = 3.5, fontface = 'bold', color = '#004488') +\n  labs(x = \"Growth in Personal Income\", y = \"Incumbent vote share\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 11))\nplot1.2\n\n# let's regress!\nfit2_prior &lt;- c(prior(normal(3, 1), class = 'b'))\n\n# model 1 has default (flat) priors on betas\n# 1 percentage point in growth is associated with ~ 3% increase in vote share\nfit1 &lt;-\n  brm(vote ~ growth,\n      data = election,\n      family = \"gaussian\",\n      file = \"hibbs_fit1.Rdata\")\ntidy(fit1) %&gt;% kable(digits = 2, caption = \"Flat prior\")\n\n# same model, with much tighter priors\n# assuming a mean effect of about 2.5% +- 1.5%\nfit2 &lt;-\n  brm(\n    vote ~ growth,\n    data = election,\n    family = \"gaussian\",\n    prior = fit2_prior,\n    file = \"hibbs_fit2.Rdata\"\n  )\ntidy(fit2) %&gt;% kable(digits = 2, caption = \"Informative prior\")\n\n# what is the predicted vote share given a 2% growth rate?\n# ~52%, but with a pretty big margin of error\nnewgrowth = 2.0\nnewprobs = c(.025, .25, .75, 0.975)\n\npred1 &lt;- posterior_predict(fit1, newdata = data.frame(growth=newgrowth))\n\nypred = mean(pred1) \nypred_quantile = quantile(pred1, newprobs)\n\n\nggplot(data.frame(x = pred1)) + geom_density(aes(x = x), linewidth = 1, color = '#DDAA33', fill = '#DDAA33', alpha = .2) +\n  labs(x = \"Predicted Vote Share (x=2)\", y = \"Probability Density\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 11))\n\n# plot the 50% and 95% credible intervals for the point estimate of 2% growth\nplot1.1 +\n  annotate(geom = \"linerange\", x = newgrowth, ymin = ypred_quantile[2], ymax = ypred_quantile[3], color = '#DDAA33',  linewidth = 1) +\n  annotate(geom = \"linerange\", x = newgrowth, ymin = ypred_quantile[1], ymax = ypred_quantile[4], color = '#DDAA33') +\n  annotate(geom = \"point\", x = newgrowth, y = ypred, fill = '#DDAA33', color = 'white', stroke = 2, size = 3, shape = 21)\n\nplot_posterior(fit2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gio Circo, Ph.D.",
    "section": "",
    "text": "Information Retrieval Using the Retrieve and Rerank Method\n\n\nExtracting injury narratives from the NEISS\n\n\n\nPython\n\n\nData Science Applications\n\n\n\n\n\n\n\n\n\nJan 24, 2025\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nI Made An Election Model\n\n\nA (very, very late) discussion of election forecasting\n\n\n\nR\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\nJan 13, 2025\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nWhere Do Crime Guns in Your State Come From?\n\n\nTracking gun seizures by state\n\n\n\nR\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExecuting Python Scripts from the Command Line\n\n\nOr: how to pass a job interview\n\n\n\nPython\n\n\nData Science Applications\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nGoing Back to (Bayesian) School\n\n\nSelf-study with Regression and Other Stories\n\n\n\nR\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\nAug 24, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nDear Crime Analysts: Why You Should Use SQL Inside of R\n\n\nUsing duckDB in R to speed up analysis\n\n\n\nR\n\n\nSQL\n\n\n\n\n\n\n\n\n\nJul 23, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Synthetic Spatial Data\n\n\nSimulating Gas Stations and Robberies\n\n\n\nR\n\n\nSpatial Statistics\n\n\n\n\n\n\n\n\n\nJun 15, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nIf You Order Chipotle Online, You Are Probably Getting Less Food\n\n\nComparing weights of orders\n\n\n\nR\n\n\nMiscellaneous\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t Evaluate Your Model On a SMOTE Dataset\n\n\nor: try this one weird trick to increase your AUC\n\n\n\nR\n\n\nMachine Learning & Prediction\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nAn Outsider’s Perspective On Media Mix Modelling\n\n\nA Bayesian Approach to MMM\n\n\n\nR\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\nMar 18, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Draw Lines Between Pairs of Points in R\n\n\nVisualizing journeys between cities\n\n\n\nR\n\n\nSpatial Statistics\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing NLP To Classify Medical Falls\n\n\nOr: An Old Dog Learning New Tricks\n\n\n\nPython\n\n\nData Science Applications\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Great American Coffee Taste Test\n\n\nA deeper dive with Bayes\n\n\n\nR\n\n\nBayesian Statistics\n\n\nMiscellaneous\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic Controls and Small Areas\n\n\nA short discussion on ‘microsynthetic’ controls\n\n\n\nR\n\n\nCausal Inference\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Spatial Risk Features using R\n\n\nCreating ‘RTM’ style map data\n\n\n\nR\n\n\nSpatial Statistics\n\n\nMachine Learning & Prediction\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nGood Mythical Morning: Blind Fast Food Taste Tests\n\n\nHow good are Rhett and Link?\n\n\n\nR\n\n\nMiscellaneous\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an Outlier Ensemble from ‘Scratch’\n\n\nPart 3: Histogram-based anomaly detector\n\n\n\nR\n\n\nAnomaly Detection\n\n\nMachine Learning & Prediction\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an Outlier Ensemble from ‘Scratch’\n\n\nPart 2: K-nearest neighbors anomaly detector\n\n\n\nR\n\n\nAnomaly Detection\n\n\nMachine Learning & Prediction\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection for Time Series\n\n\nApplying a PCA anomaly detector\n\n\n\nR\n\n\nAnomaly Detection\n\n\nMachine Learning & Prediction\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an Outlier Ensemble from ‘Scratch’\n\n\nPart 1: Principal components anomaly detector\n\n\n\nR\n\n\nAnomaly Detection\n\n\nMachine Learning & Prediction\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nAn Alternative to Buffers for Spatial Merging\n\n\nCar Crashes in Austin\n\n\n\nR\n\n\nSpatial Statistics\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Power of Ensembles\n\n\nAdventures in outlier detection\n\n\n\nPython\n\n\nAnomaly Detection\n\n\nMachine Learning & Prediction\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nGio Circo, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nInjuries at Amazon Warehouses - A Bayesian Approach\n\n\n\n\n\n\nR\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nGio Circo, Ph.D\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m Gio Circo, a Ph.D. in criminology and former assistant professor. I currently work at Gainwell Technologies as a data scientist. My work at Gainwell focuses primarily on models for payment integrity (fraud detection, and anomaly detection), synthetic data generation, and deep learning models for insurance subrogation.\nOutside of work I love running, cooking, making cocktails, and drinking wine."
  },
  {
    "objectID": "about.html#who-am-i",
    "href": "about.html#who-am-i",
    "title": "About Me",
    "section": "",
    "text": "I’m Gio Circo, a Ph.D. in criminology and former assistant professor. I currently work at Gainwell Technologies as a data scientist. My work at Gainwell focuses primarily on models for payment integrity (fraud detection, and anomaly detection), synthetic data generation, and deep learning models for insurance subrogation.\nOutside of work I love running, cooking, making cocktails, and drinking wine."
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "About Me",
    "section": "Contact Me",
    "text": "Contact Me\nIf you’re interested in data science, Bayesian statistics, or criminology - please feel free to reach out to me directly, or hit up my Twitter/X or Bluesky socials!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nMichigan State University | Lansing, MI | PhD in Criminal Justice | August 2013 - May 2018\nIllinois State University| Normal, IL | B.A in Criminal Justice | August 2010 - June 2012"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nGainwell Technologies | Data Scientist | June 2022 - present University of New Haven | Assistant Professor | August 2018 - May 2022"
  },
  {
    "objectID": "about.html#resume",
    "href": "about.html#resume",
    "title": "About Me",
    "section": "Resume",
    "text": "Resume\nNon-Academic Resume"
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html",
    "href": "posts/austin-vehicle/spatial_merging.html",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\n\n\n# DATA SOURCES AND INFO\n# -----------------------------#\n# https://data.austintexas.gov\n#   vehicle crashes: 'Transportation-and-Mobility/Vision-Zero-Crash-Report-Data-Crash-Level-Records'\n#   traffic cameras: 'Transportation-and-Mobility/Traffic-Cameras/'\n#   austin council map: 'dataset/Boundaries-City-of-Austin-Council-Districts'\n\n# Select a specific Austin Council District and year\n# see: https://maps.austintexas.gov/GIS/CouncilDistrictMap/\ncnl &lt;- 3\nyr &lt;- 2022\n\n\n# DATA LOADING\n# -----------------------------#\n\n# Get Austin shapefile, pull only the district we need\naustin &lt;- st_read(\"C:/Users/gioc4/Documents/blog/data/austin_city.shp\", quiet = TRUE) %&gt;%\n  st_transform(crs = 32614) %&gt;%\n  filter(council_di %in% cnl)\n\n# Read traffic camera data & vehicle crash data\n# Limit crashes to a specific year, conver to spatial\ncamera &lt;- st_read(\"C:/Users/gioc4/Documents/blog/data/traffic_camera.shp\", quiet = TRUE) %&gt;%\n  filter(camera_sta == \"TURNED_ON\") %&gt;%\n  distinct(geometry, .keep_all = TRUE) %&gt;%\n  st_transform(crs = 32614) %&gt;%\n  mutate(camera_X = st_coordinates(.)[,1],\n         camera_Y = st_coordinates(.)[,2])\n\ncrash &lt;- read_csv(unz(\"C:/Users/gioc4/Documents/blog/data/austin_crash.zip\",\"crash_data.csv\")) %&gt;%\n  mutate(crash_date = strptime(crash_date, format=\"%m/%d/%Y %H:%M\")) %&gt;%\n  filter(year(crash_date) == yr)\n\n# Convert crash to sf, extract coordinates\ncrash_sf &lt;- crash %&gt;%\n  filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n  st_as_sf(coords = c('longitude', 'latitude')) %&gt;%\n  st_set_crs(4326) %&gt;%\n  st_transform(crs = st_crs(camera)) %&gt;%\n  mutate(crash_X = st_coordinates(.)[,1], \n         crash_Y = st_coordinates(.)[,2]) %&gt;%\n  select(crash_id, crash_date, crash_X,crash_Y)\n\n# Clip to region\ncamera &lt;- camera[austin,]\ncrash_sf &lt;- crash_sf[austin,]\n\n\nThis is a bit of a mini-blog post based on a workflow that I have used based on some of my own work. A common issue in spatial analysis - and especially in criminology - is the need to analyze points that are merged to another point.\nIn criminology we might say that assaults occurring right outside of a bar are within it’s “spatial influence”. Typically what is done is we define a “buffer” around each of the points \\(j\\) (like bars, or gas stations) of interest and merge all of the crime incidents \\(i\\) that are within each of the \\(j\\) points’ buffer area. This is something I’ve done before looking at the effect of CCTV cameras on crime at businesses in Detroit. This is pretty common across a lot of criminology research (e.g. finding all crime that occurs within a 1-block radius of bars and liquor stores).\nWhile I used to use the “buffer” method, I think there is a more efficient way of doing this via Voronai polygons which accomplishes the same goal, and allows for more flexibility in analysis. Let’s illustrate this using some data from the city of Austin. In this example we are going to look at the incidence of car crashes \\(i\\) around traffic cameras \\(j\\). Our goal will be to merge car crashes to the nearest traffic camera within a defined spatial range.\nHere’s the study area - one of the Austin city council districts, showing the traffic cameras in blue, and the location of crashes in red. In the picture below there are 58 cameras and about 1,700 car accidents. For this example we’re restricting our analysis to only accidents that occurred in 2022 and using cameras that were active (TURNED_ON) at the time. We can see that there are a lot of accidents, many of them quite far from a traffic camera. Let’s say we want to define a study area around each traffic camera of about 300 meters - or about 980 feet.\n\n\nCode\nggplot() +\n  geom_sf(data = austin, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = crash_sf, color = '#BB5566', size = .6, alpha = .7) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nLocation of car crahes (red) and traffic cameras (blue)."
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html#the-problem-merging-points-to-points",
    "href": "posts/austin-vehicle/spatial_merging.html#the-problem-merging-points-to-points",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\n\n\n# DATA SOURCES AND INFO\n# -----------------------------#\n# https://data.austintexas.gov\n#   vehicle crashes: 'Transportation-and-Mobility/Vision-Zero-Crash-Report-Data-Crash-Level-Records'\n#   traffic cameras: 'Transportation-and-Mobility/Traffic-Cameras/'\n#   austin council map: 'dataset/Boundaries-City-of-Austin-Council-Districts'\n\n# Select a specific Austin Council District and year\n# see: https://maps.austintexas.gov/GIS/CouncilDistrictMap/\ncnl &lt;- 3\nyr &lt;- 2022\n\n\n# DATA LOADING\n# -----------------------------#\n\n# Get Austin shapefile, pull only the district we need\naustin &lt;- st_read(\"C:/Users/gioc4/Documents/blog/data/austin_city.shp\", quiet = TRUE) %&gt;%\n  st_transform(crs = 32614) %&gt;%\n  filter(council_di %in% cnl)\n\n# Read traffic camera data & vehicle crash data\n# Limit crashes to a specific year, conver to spatial\ncamera &lt;- st_read(\"C:/Users/gioc4/Documents/blog/data/traffic_camera.shp\", quiet = TRUE) %&gt;%\n  filter(camera_sta == \"TURNED_ON\") %&gt;%\n  distinct(geometry, .keep_all = TRUE) %&gt;%\n  st_transform(crs = 32614) %&gt;%\n  mutate(camera_X = st_coordinates(.)[,1],\n         camera_Y = st_coordinates(.)[,2])\n\ncrash &lt;- read_csv(unz(\"C:/Users/gioc4/Documents/blog/data/austin_crash.zip\",\"crash_data.csv\")) %&gt;%\n  mutate(crash_date = strptime(crash_date, format=\"%m/%d/%Y %H:%M\")) %&gt;%\n  filter(year(crash_date) == yr)\n\n# Convert crash to sf, extract coordinates\ncrash_sf &lt;- crash %&gt;%\n  filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n  st_as_sf(coords = c('longitude', 'latitude')) %&gt;%\n  st_set_crs(4326) %&gt;%\n  st_transform(crs = st_crs(camera)) %&gt;%\n  mutate(crash_X = st_coordinates(.)[,1], \n         crash_Y = st_coordinates(.)[,2]) %&gt;%\n  select(crash_id, crash_date, crash_X,crash_Y)\n\n# Clip to region\ncamera &lt;- camera[austin,]\ncrash_sf &lt;- crash_sf[austin,]\n\n\nThis is a bit of a mini-blog post based on a workflow that I have used based on some of my own work. A common issue in spatial analysis - and especially in criminology - is the need to analyze points that are merged to another point.\nIn criminology we might say that assaults occurring right outside of a bar are within it’s “spatial influence”. Typically what is done is we define a “buffer” around each of the points \\(j\\) (like bars, or gas stations) of interest and merge all of the crime incidents \\(i\\) that are within each of the \\(j\\) points’ buffer area. This is something I’ve done before looking at the effect of CCTV cameras on crime at businesses in Detroit. This is pretty common across a lot of criminology research (e.g. finding all crime that occurs within a 1-block radius of bars and liquor stores).\nWhile I used to use the “buffer” method, I think there is a more efficient way of doing this via Voronai polygons which accomplishes the same goal, and allows for more flexibility in analysis. Let’s illustrate this using some data from the city of Austin. In this example we are going to look at the incidence of car crashes \\(i\\) around traffic cameras \\(j\\). Our goal will be to merge car crashes to the nearest traffic camera within a defined spatial range.\nHere’s the study area - one of the Austin city council districts, showing the traffic cameras in blue, and the location of crashes in red. In the picture below there are 58 cameras and about 1,700 car accidents. For this example we’re restricting our analysis to only accidents that occurred in 2022 and using cameras that were active (TURNED_ON) at the time. We can see that there are a lot of accidents, many of them quite far from a traffic camera. Let’s say we want to define a study area around each traffic camera of about 300 meters - or about 980 feet.\n\n\nCode\nggplot() +\n  geom_sf(data = austin, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = crash_sf, color = '#BB5566', size = .6, alpha = .7) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nLocation of car crahes (red) and traffic cameras (blue)."
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html#spatial-merging-using-voronoi-polygons",
    "href": "posts/austin-vehicle/spatial_merging.html#spatial-merging-using-voronoi-polygons",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "Spatial Merging using Voronoi Polygons",
    "text": "Spatial Merging using Voronoi Polygons\n\nVoronoi Polygons\nVoronoi polygons(or tessellations) are useful for a number of purposes. Given a set of points \\(j_n\\) we define a set of \\(n\\) regions where all spaces within each region has a single nearest neighbor of the initial point \\(i\\). Practically speaking, this just means we sub-divide a study area into smaller areas corresponding to the proximity to a point. This has many useful properties, such as determining nearest-neighbor distances from points to points. Let’s see how we can do this in R.\nTo start, we’ll first use a helper function to convert the Voronoi tessellation to an sf object that is suitable for merging. We’ll then merge the camera data to the polygon we just created (using st_intersection) and pull a few of the variables we’ll want for this example.\n\n\nCode\n# Helper function to simplify tessellation\n# borrowed from: \n# https://gis.stackexchange.com/questions/362134\nst_voronoi_point &lt;- function(points){\n  ## points must be POINT geometry\n  # check for point geometry and execute if true\n  if(!all(st_geometry_type(points) == \"POINT\")){\n    stop(\"Input not  POINT geometries\")\n  }\n  g = st_combine(st_geometry(points)) # make multipoint\n  v = st_voronoi(g)\n  v = st_collection_extract(v)\n  return(v[unlist(st_intersects(points, v))])\n}\n\n\n# create Voronoi tessellation over cameras\ncamera_poly &lt;- st_voronoi_point(camera) %&gt;%\n  st_intersection(austin) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(camera_id = camera$camera_id,\n         camera_X = camera$camera_X,\n         camera_Y = camera$camera_Y)\n\n\nNow we can plot the result. Below we see we now have a defined set of regions corresponding to the areas nearest to each camera. Therefore, any crashes that occur in one of the Voronoi polygons is also its nearest camera. This saves us the step of determining which point is its nearest neighbor.\n\n\nCode\nggplot() +\n  geom_sf(data = camera_poly, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nAll spaces within each Voronoi polygon are a nearest neighbor to a camera.\n\n\n\n\n\n\nSpatial Merging\nAfter we’ve created the Voronoi regions, all we need to do is merge each point to the region it falls within (which implies the camera there is its nearest neighbor) and then compute the euclidean distance from the crash to the camera. The code below uses a for-loop to get the pairwise distances after spatial joining and then limits the output to only crashes that are within 300 feet of the nearest camera.\n\n\nCode\n# JOIN AND MERGE\n# ----------------------- #\n\n# compute euclidean distance\nedist &lt;- function(a,b){\n  sqrt(sum((a - b) ^ 2))\n}\n\n# get x-y coords for crashes and cameras\n# convert to matrix\ncamera_crash &lt;-  st_join(crash_sf,camera_poly) %&gt;%\n  tibble() %&gt;%\n  select(camera_id, \n         crash_id, \n         camera_X, \n         camera_Y, \n         crash_X, \n         crash_Y)\n\ndmat &lt;- matrix(c(camera_crash$camera_X, \n                 camera_crash$camera_Y, \n                 camera_crash$crash_X, \n                 camera_crash$crash_Y),\n               ncol = 4)\n\n# compute pairwise distances\ndlist &lt;- list()\nfor(i in 1:nrow(dmat)){\n  dlist[[i]] &lt;- edist(c(dmat[i,1], dmat[i,2]), c(dmat[i,3], dmat[i,4]))\n}\n\ncamera_crash$dist &lt;- unlist(dlist)\n\n# get ids of within 300 meters\ndist_ids &lt;- camera_crash$dist &lt;= 300\n\n\nNow we can plot the results. As we see below we now only have crashes that are within 300 feet or less of the nearest camera. One advantage of this approach is that we can make any adjustments to the spatial region we’re interested in by just adjusting the filter above - or we can use the full range of distances in our analysis and look at decay effects (for example, the effect of CCTV cameras on crime clearance).\n\n\nCode\nggplot() +\n  geom_sf(data = austin, fill = \"#88CCEE\", alpha = .3) +\n  geom_sf(data = crash_sf[dist_ids,], color = '#BB5566', size = .6, alpha = .7) +\n  geom_sf(data = camera, color = '#004488', shape = 3, size = 2, stroke = 1.5) +\n  theme_void()\n\n\n\n\n\nCar crashes within 300 meters of a traffic camera.\n\n\n\n\nWith this done, we can do any kind of further investigation. For example, which camera observed the greatest number of crashes? Here, the top-ranked camera is at a 4-way intersection leading to the highway. Also, due to its proximity to the highway, it’s very likely that our distance size (300 meters, or about 900 feet) is picking up accidents that are occurring on the highway below. Of course, this is just a demonstration of method of spatial merging, not an investigation into traffic accidents in Austin!\n\n\nCode\ncamera_crash %&gt;%\n  filter(crash_id %in% crash_sf[dist_ids,]$crash_id) %&gt;%\n  count(camera_id) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice(1:5)\n\n\n# A tibble: 5 × 2\n  camera_id     n\n  &lt;chr&gt;     &lt;int&gt;\n1 919          72\n2 674          59\n3 948          46\n4 155          44\n5 962          35"
  },
  {
    "objectID": "posts/austin-vehicle/spatial_merging.html#summary",
    "href": "posts/austin-vehicle/spatial_merging.html#summary",
    "title": "An Alternative to Buffers for Spatial Merging",
    "section": "Summary",
    "text": "Summary\nThis little mini-blog highlighted some approaches that can be taken to perform a relatively common spatial procedure. Using Voronoi polygons we looked at how we can use them to easily calculate nearest-neighbor distances. These types of spatial approaches aren’t necessarily the sexiest topics, but I find they often help considerably with modelling pipelines down the road. Sometimes have a good foundation can help with further analysis later.\n\nAn Aside: An even (easier) method?\nOf course, another method is to simply use the a convenient function embedded in the sf library aptly named st_nearest_feature(). This takes two sf objects and returns the indexes of \\(y\\) that are nearest to \\(x\\). While the solution here is equivalent to the one above, it might not necessarily be available in your given software package. Also, while I have no testing to support this, I expect that this would likely be slow in case of many pairwise distances. The presence of the polygons helps avoid the unnecessary computation of distances between points that are not nearest neighbors.\n\n# get index of cameras nearest to each point\nidx &lt;- st_nearest_feature(crash_sf, camera)\nid_dist &lt;- st_distance(camera[idx,], crash_sf, by_element = TRUE)\n\nid_dist[1:5]\n\nUnits: [m]\n         1          2          3          4          5 \n  49.88593 1202.17589  784.61324 1412.67832  282.73844"
  },
  {
    "objectID": "posts/chipotle-weight/youtube-food.html",
    "href": "posts/chipotle-weight/youtube-food.html",
    "title": "If You Order Chipotle Online, You Are Probably Getting Less Food",
    "section": "",
    "text": "Here’s a quick one. The question posed here is “do you get less food if you order your Chipotle order online versus in person?” There are plenty of posts going back years claiming that their orders are smaller if they order online.\nI happened to be watching a video from YouTuber Zackary Smigel who decided to eat nothing but Chipotle for 30 days. One of the interesting things in his video is that he provided his data for 30 consecutive Chipotle visits here. what Zackary might not have known is that he unintentionally created a very nice blocked experiment design. What this means is we can easily identify sources of variation in order weight by controlling for other variables present.\nI downloaded the data from the Google Sheet, saved a few of the columns locally as a .csv and did some minimal processing.\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# load the data locally\ndf &lt;- read_csv(\"../../../data/chipotle.csv\") %&gt;%\n  select(-Chips)\n\n# function to convert formatted lbs, oz to grams\nlbs_oz_to_grams &lt;- function(lbs_oz) {\n  parts &lt;- unlist(strsplit(lbs_oz, \"\\\\.\"))\n  pounds_in_grams &lt;- as.numeric(parts[1]) * 453.592\n  ounces_in_grams &lt;- as.numeric(parts[2]) * 28.3495\n  total_grams &lt;- pounds_in_grams + ounces_in_grams\n  return(total_grams)\n}\n\nfood_weight_data &lt;-\n  df %&gt;%\n  pivot_longer(\n    cols = c(\"Burrito\", \"Bowl\"),\n    names_to = \"food\",\n    values_to = \"weight\"\n  ) %&gt;%\n  na.omit()\n\n# convert weight to grams\nfood_weight_data$weight &lt;- sapply(food_weight_data$weight, lbs_oz_to_grams)\n\nhead(food_weight_data[,1:5]) %&gt;% kable(digits = 2, caption = \"Chipotle Food Order Data\")\n\n\n\nChipotle Food Order Data\n\n\nOrder\nMeat\nStore\nfood\nweight\n\n\n\n\nPerson\nChicken\nStore 1\nBurrito\n992.23\n\n\nPerson\nChicken\nStore 1\nBurrito\n907.18\n\n\nOnline\nChicken\nStore 1\nBowl\n907.18\n\n\nOnline\nCarnitas\nStore 1\nBurrito\n850.48\n\n\nPerson\nCarnitas\nStore 1\nBowl\n1048.93\n\n\nPerson\nChicken\nStore 1\nBurrito\n850.48\n\n\n\n\n\nSo for every order he made, we can control for whether it was in-person or online, the type of meat used in the order, the store it originated from, and the type of food (either a burrito or a bowl)."
  },
  {
    "objectID": "posts/chipotle-weight/youtube-food.html#how-inconsistent-are-chipotle-orders",
    "href": "posts/chipotle-weight/youtube-food.html#how-inconsistent-are-chipotle-orders",
    "title": "If You Order Chipotle Online, You Are Probably Getting Less Food",
    "section": "",
    "text": "Here’s a quick one. The question posed here is “do you get less food if you order your Chipotle order online versus in person?” There are plenty of posts going back years claiming that their orders are smaller if they order online.\nI happened to be watching a video from YouTuber Zackary Smigel who decided to eat nothing but Chipotle for 30 days. One of the interesting things in his video is that he provided his data for 30 consecutive Chipotle visits here. what Zackary might not have known is that he unintentionally created a very nice blocked experiment design. What this means is we can easily identify sources of variation in order weight by controlling for other variables present.\nI downloaded the data from the Google Sheet, saved a few of the columns locally as a .csv and did some minimal processing.\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# load the data locally\ndf &lt;- read_csv(\"../../../data/chipotle.csv\") %&gt;%\n  select(-Chips)\n\n# function to convert formatted lbs, oz to grams\nlbs_oz_to_grams &lt;- function(lbs_oz) {\n  parts &lt;- unlist(strsplit(lbs_oz, \"\\\\.\"))\n  pounds_in_grams &lt;- as.numeric(parts[1]) * 453.592\n  ounces_in_grams &lt;- as.numeric(parts[2]) * 28.3495\n  total_grams &lt;- pounds_in_grams + ounces_in_grams\n  return(total_grams)\n}\n\nfood_weight_data &lt;-\n  df %&gt;%\n  pivot_longer(\n    cols = c(\"Burrito\", \"Bowl\"),\n    names_to = \"food\",\n    values_to = \"weight\"\n  ) %&gt;%\n  na.omit()\n\n# convert weight to grams\nfood_weight_data$weight &lt;- sapply(food_weight_data$weight, lbs_oz_to_grams)\n\nhead(food_weight_data[,1:5]) %&gt;% kable(digits = 2, caption = \"Chipotle Food Order Data\")\n\n\n\nChipotle Food Order Data\n\n\nOrder\nMeat\nStore\nfood\nweight\n\n\n\n\nPerson\nChicken\nStore 1\nBurrito\n992.23\n\n\nPerson\nChicken\nStore 1\nBurrito\n907.18\n\n\nOnline\nChicken\nStore 1\nBowl\n907.18\n\n\nOnline\nCarnitas\nStore 1\nBurrito\n850.48\n\n\nPerson\nCarnitas\nStore 1\nBowl\n1048.93\n\n\nPerson\nChicken\nStore 1\nBurrito\n850.48\n\n\n\n\n\nSo for every order he made, we can control for whether it was in-person or online, the type of meat used in the order, the store it originated from, and the type of food (either a burrito or a bowl)."
  },
  {
    "objectID": "posts/chipotle-weight/youtube-food.html#do-you-get-less-food-online",
    "href": "posts/chipotle-weight/youtube-food.html#do-you-get-less-food-online",
    "title": "If You Order Chipotle Online, You Are Probably Getting Less Food",
    "section": "Do you get less food online?",
    "text": "Do you get less food online?\nLet’s answer this question. To start, we can look at the general distribution of weights. Below we see that the median weight of an order is just under 800 grams, or about 1.7 pounds. The largest order he got was a whopping 2.3 pounds, and the smallest was 1.1 pounds.\n\n\nCode\nquantile(food_weight_data$weight)\n\n\n       0%       25%       50%       75%      100% \n 510.2910  715.8249  793.7860  907.1840 1048.9315 \n\n\nIf we plot out a little density plot we can see the distribution of weights is (approximately) normal, with online orders appearing to be a bit lighter than in-person ones. Without adjusting for anything else, the median weight of online orders is about 709 grams, and in-person orders are 907. However, just based on this visual we can’t be certain it isn’t due to other factors (for example, maybe he ordered more heavy items only online).\n\nBox PlotDensity PlotHistogram\n\n\n\n\nCode\nggplot(food_weight_data) +\n  geom_boxplot(aes(x = weight, y = Order, fill = Order), alpha = .8) +\n  labs(x = \"Weight (grams)\", y = \"Order Type\", title = \"Box Plot, Chipotle Order Weights (n=30)\") +\n  scale_fill_manual(values = c('#4477AA', '#EE6677')) +\n  theme_bw() +\n  theme(legend.position = 'none',\n        legend.title = element_blank(),\n        legend.text = element_text(face = \"bold\"))\n\n\n\n\n\nVisually, in-person orders appear to be larger\n\n\n\n\n\n\n\n\nCode\nggplot(food_weight_data) +\n  geom_density(aes(x = weight, fill = Order), color = 'white', linewidth = 1, alpha = .8) +\n  labs(x = \"Weight (grams)\", y = \"Probability Density\", title = \"Density Plot, Chipotle Order Weights (n=30)\", subtitle = \"Y axis is smoothed PDF\") +\n  scale_fill_manual(values = c('#4477AA', '#EE6677')) +\n  theme_bw() +\n  theme(legend.position = c(.1,.9),\n        legend.title = element_blank(),\n        legend.text = element_text(face = \"bold\"))\n\n\n\n\n\nVisually, in-person orders appear to be larger\n\n\n\n\n\n\n\n\nCode\nggplot(food_weight_data) +\n  geom_histogram(aes(x = weight, fill = Order), color = 'white', linewidth = 1, alpha = .8, bins = 10) +\n  labs(x = \"Weight (grams)\", y = \"Count\", title = \"Histogram, Chipotle Order Weights (n=30)\") +\n  scale_fill_manual(values = c('#4477AA', '#EE6677')) +\n  theme_bw() +\n  theme(legend.position = c(.1,.9),\n        legend.title = element_blank(),\n        legend.text = element_text(face = \"bold\"))\n\n\n\n\n\nVisually, in-person orders appear to be larger\n\n\n\n\n\n\n\n\nResults\nTo determine whether online orders weigh less than in-person orders, we can just apply some simple statistics. Here, I fit a linear regression with each blocking factor. In this way we “control” for variables we know affect order weight (e.g. bowls weigh more than burritos, or store 3 gives you more food than store 1). In the end all we care about is the coefficient for the effect of ordering online relative to ordering in-person.\n\n\nCode\n# in-person ordering yields about 160g more food, or about more 20% on average\nfit1 &lt;- lm(weight ~ Order + Meat + Store + food, data = food_weight_data)\n\nbroom::tidy(fit1) %&gt;%\n  kable(digits = 2, caption = \"Linear regression on order weight\") %&gt;%\n  kable_styling(\"striped\") %&gt;%\n  row_spec(2, bold = T, hline_after = T)\n\n\n\nLinear regression on order weight\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n843.22\n36.42\n23.15\n0.00\n\n\nOrderPerson\n160.62\n30.22\n5.31\n0.00\n\n\nMeatChicken\n-29.47\n32.48\n-0.91\n0.37\n\n\nStoreStore 2\n-62.26\n36.74\n-1.69\n0.10\n\n\nStoreStore 3\n-93.44\n36.74\n-2.54\n0.02\n\n\nfoodBurrito\n-82.38\n30.22\n-2.73\n0.01\n\n\n\n\n\n\n\nSo what does this mean? Well, after controlling for the other variables we see that the effect of ordering in-person relative to online is 160 grams more food (or about 5.6 ounces). Based on the orders he submitted that roughly equates to 20% more food from ordering in-person rather than online!\nSo, yes, in this case it does appear that ordering online resulted in smaller orders relative to in-person.\n\n\nCaveats\nAll of this analysis assumes that the online orders were roughly equivalent to the in-person one (e.g. ordering the same ingredients online as in-person). This would not be valid if he tended to order more additional items when in the store rather then online. However, from watching the video and looking at the listed cost of all items ($11.00) it appears that the orders are comparable.\nI would also caution against extrapolation. The data here comes for 30 orders from a single city in the US. It is a limited set of data with a limited number of observations. So, as always with science, some degree of skepticism is always warranted.\n\n\nFull Model\n\n\nCode\nsummary(lm(weight ~ Order + Meat + Store + food, data = food_weight_data))\n\n\n\nCall:\nlm(formula = weight ~ Order + Meat + Store + food, data = food_weight_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-147.79  -43.47   -8.17   52.23  129.06 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    843.22      36.42  23.154  &lt; 2e-16 ***\nOrderPerson    160.62      30.22   5.314 1.88e-05 ***\nMeatChicken    -29.47      32.48  -0.907   0.3733    \nStoreStore 2   -62.26      36.74  -1.694   0.1031    \nStoreStore 3   -93.44      36.74  -2.543   0.0178 *  \nfoodBurrito    -82.38      30.22  -2.726   0.0118 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 81.84 on 24 degrees of freedom\nMultiple R-squared:  0.6358,    Adjusted R-squared:   0.56 \nF-statistic: 8.381 on 5 and 24 DF,  p-value: 0.0001068"
  },
  {
    "objectID": "posts/crime-guns/crime-guns.html",
    "href": "posts/crime-guns/crime-guns.html",
    "title": "Where Do Crime Guns in Your State Come From?",
    "section": "",
    "text": "For this post I rely on some data posted by David Johnson. He kindly made ATF gun trace data freely available here, which I learned about via a post on Twitter:\n\nGoing to have a new paper out soon that uses ATF gun trace data. Data is state level (2010-21) and has info on exported and imported crime guns … for example, exported crime guns are, from the context of FL, guns from FL but recovered in GA. Imported crime guns are guns from GA but recovered in FL.\n\nThe ATF data details the source of guns seized by the agency that were involved in crimes. This includes the state that the gun was originally registered in, the state that the gun was seized in, and the number of days from the date of registration to the gun’s involvement in a crime (a statistic referred to as “time to crime”). With this data we can examine flows of crime guns from their registered states to where they were eventually used in a crime.\nTo do this, I take the raw ATF files from the osf repository and process them in a little bit of code (see below). This code loops through the files from 2019 to 2021, strips off some unnecessary values, and then stacks them in a single data frame. We also do some basic computations to get the number and proportion of crime guns in a given state that were originally registered in another state.\n\n\nCode\n# params\nyears &lt;- c(2019,2020,2021)\nstate_recovery_list &lt;- list()\n\n# function to pull data from specific year\npull_data &lt;- function(year){\n  recoveries &lt;- read_csv(\n    sprintf(\".\\gun_recovery_%s.csv\", year)\n  ) %&gt;%\n    fill(REGISTERED_STATE) %&gt;%\n    filter(TIME_RANGE != 'Average Time-to-Crime in Years') %&gt;%\n    mutate(across(everything(), ~ str_replace(., \",\", \"\")))\n  \n  return(recoveries)\n}\n\nidx = 0\nfor(year in years) {\n  idx = idx+1\n  \n  # add state recoveries\n  state_recovery_list[[idx]] &lt;-  pull_data(year) %&gt;%\n    select(-TOTAL) %&gt;%\n    pivot_longer(c(-REGISTERED_STATE, -TIME_RANGE)) %&gt;%\n    mutate(\n      value = as.numeric(value),\n      reg_state = tolower(REGISTERED_STATE),\n      recov_state = tolower(name),\n      year = year\n    ) %&gt;%\n    group_by(year, reg_state, recov_state) %&gt;%\n    summarise(guns = sum(value, na.rm = T))\n}\n\n# now combine\n\n# all recoveries\nstate_recoveries &lt;- do.call(rbind, state_recovery_list) %&gt;%\n  group_by(reg_state, recov_state) %&gt;%\n  summarise(guns = sum(guns))\n\n# recoveries from other states only\nstate_recoveries_else &lt;- state_recoveries %&gt;%\n  group_by(recov_state) %&gt;%\n  mutate(prop = 1 - guns/sum(guns)) %&gt;%\n  filter(reg_state == recov_state) %&gt;%\n  ungroup()\n\n\nAll of this code above gives us two long-form data frames that define the number of crime guns seized in each state, compared to the state that gun was registered in. As an example, we can print out some values below:\n\n\n\nCrime Guns Recovered in Alabama\n\n\nRecovery State\nRegistered State\nCrime Guns\n\n\n\n\nalabama\nalabama\n16371\n\n\nalabama\nalaska\n1\n\n\nalabama\narizona\n59\n\n\nalabama\narkansas\n36\n\n\nalabama\ncalifornia\n430\n\n\nalabama\ncolorado\n77\n\n\nalabama\nconnecticut\n59\n\n\nalabama\ndelaware\n22\n\n\nalabama\ndistrict of columbia\n85\n\n\nalabama\nflorida\n1290\n\n\n\n\n\nFor example, below we see that 16,371 guns were recovered in Alabama that were registered in Alabama, 1 gun that was registered in Alaska, and 59 that were registered in Arizona, and so on. We can also condense this table and find out the total proportion of crime guns that were registered in a state other than the state in which it was recovered:\n\n\n\nProportion of Crime Guns Recovered in Other State\n\n\nState\nCrime Guns\nProportion\n\n\n\n\nalabama\n16371\n0.170\n\n\nalaska\n1776\n0.154\n\n\narizona\n21908\n0.164\n\n\narkansas\n5957\n0.212\n\n\ncalifornia\n47192\n0.448\n\n\ncolorado\n10884\n0.288\n\n\nconnecticut\n1229\n0.560\n\n\ndelaware\n2325\n0.321\n\n\ndistrict of columbia\n196\n0.958\n\n\nflorida\n55685\n0.203\n\n\n\n\n\nAbove we see that about 17% of crime guns seized in Alabama were registered outside the state, about 15% in Alaska, and for every other state. We can use this information to get a more comprehensive look at the sources and flows of guns."
  },
  {
    "objectID": "posts/crime-guns/crime-guns.html#tracing-the-flow-of-crime-guns",
    "href": "posts/crime-guns/crime-guns.html#tracing-the-flow-of-crime-guns",
    "title": "Where Do Crime Guns in Your State Come From?",
    "section": "",
    "text": "For this post I rely on some data posted by David Johnson. He kindly made ATF gun trace data freely available here, which I learned about via a post on Twitter:\n\nGoing to have a new paper out soon that uses ATF gun trace data. Data is state level (2010-21) and has info on exported and imported crime guns … for example, exported crime guns are, from the context of FL, guns from FL but recovered in GA. Imported crime guns are guns from GA but recovered in FL.\n\nThe ATF data details the source of guns seized by the agency that were involved in crimes. This includes the state that the gun was originally registered in, the state that the gun was seized in, and the number of days from the date of registration to the gun’s involvement in a crime (a statistic referred to as “time to crime”). With this data we can examine flows of crime guns from their registered states to where they were eventually used in a crime.\nTo do this, I take the raw ATF files from the osf repository and process them in a little bit of code (see below). This code loops through the files from 2019 to 2021, strips off some unnecessary values, and then stacks them in a single data frame. We also do some basic computations to get the number and proportion of crime guns in a given state that were originally registered in another state.\n\n\nCode\n# params\nyears &lt;- c(2019,2020,2021)\nstate_recovery_list &lt;- list()\n\n# function to pull data from specific year\npull_data &lt;- function(year){\n  recoveries &lt;- read_csv(\n    sprintf(\".\\gun_recovery_%s.csv\", year)\n  ) %&gt;%\n    fill(REGISTERED_STATE) %&gt;%\n    filter(TIME_RANGE != 'Average Time-to-Crime in Years') %&gt;%\n    mutate(across(everything(), ~ str_replace(., \",\", \"\")))\n  \n  return(recoveries)\n}\n\nidx = 0\nfor(year in years) {\n  idx = idx+1\n  \n  # add state recoveries\n  state_recovery_list[[idx]] &lt;-  pull_data(year) %&gt;%\n    select(-TOTAL) %&gt;%\n    pivot_longer(c(-REGISTERED_STATE, -TIME_RANGE)) %&gt;%\n    mutate(\n      value = as.numeric(value),\n      reg_state = tolower(REGISTERED_STATE),\n      recov_state = tolower(name),\n      year = year\n    ) %&gt;%\n    group_by(year, reg_state, recov_state) %&gt;%\n    summarise(guns = sum(value, na.rm = T))\n}\n\n# now combine\n\n# all recoveries\nstate_recoveries &lt;- do.call(rbind, state_recovery_list) %&gt;%\n  group_by(reg_state, recov_state) %&gt;%\n  summarise(guns = sum(guns))\n\n# recoveries from other states only\nstate_recoveries_else &lt;- state_recoveries %&gt;%\n  group_by(recov_state) %&gt;%\n  mutate(prop = 1 - guns/sum(guns)) %&gt;%\n  filter(reg_state == recov_state) %&gt;%\n  ungroup()\n\n\nAll of this code above gives us two long-form data frames that define the number of crime guns seized in each state, compared to the state that gun was registered in. As an example, we can print out some values below:\n\n\n\nCrime Guns Recovered in Alabama\n\n\nRecovery State\nRegistered State\nCrime Guns\n\n\n\n\nalabama\nalabama\n16371\n\n\nalabama\nalaska\n1\n\n\nalabama\narizona\n59\n\n\nalabama\narkansas\n36\n\n\nalabama\ncalifornia\n430\n\n\nalabama\ncolorado\n77\n\n\nalabama\nconnecticut\n59\n\n\nalabama\ndelaware\n22\n\n\nalabama\ndistrict of columbia\n85\n\n\nalabama\nflorida\n1290\n\n\n\n\n\nFor example, below we see that 16,371 guns were recovered in Alabama that were registered in Alabama, 1 gun that was registered in Alaska, and 59 that were registered in Arizona, and so on. We can also condense this table and find out the total proportion of crime guns that were registered in a state other than the state in which it was recovered:\n\n\n\nProportion of Crime Guns Recovered in Other State\n\n\nState\nCrime Guns\nProportion\n\n\n\n\nalabama\n16371\n0.170\n\n\nalaska\n1776\n0.154\n\n\narizona\n21908\n0.164\n\n\narkansas\n5957\n0.212\n\n\ncalifornia\n47192\n0.448\n\n\ncolorado\n10884\n0.288\n\n\nconnecticut\n1229\n0.560\n\n\ndelaware\n2325\n0.321\n\n\ndistrict of columbia\n196\n0.958\n\n\nflorida\n55685\n0.203\n\n\n\n\n\nAbove we see that about 17% of crime guns seized in Alabama were registered outside the state, about 15% in Alaska, and for every other state. We can use this information to get a more comprehensive look at the sources and flows of guns."
  },
  {
    "objectID": "posts/crime-guns/crime-guns.html#gun-control-and-crime-guns",
    "href": "posts/crime-guns/crime-guns.html#gun-control-and-crime-guns",
    "title": "Where Do Crime Guns in Your State Come From?",
    "section": "Gun Control and Crime Guns",
    "text": "Gun Control and Crime Guns\nThe first thing we can do is visually assess which states have the greatest proportion of seized crime guns registered in other states. Visually, the map below has some fairly striking patterns. New England states like Massachusetts, Connecticut, New York, and New Jersey all have very high proportions of crime guns imported from other states. In the Midwest we see Illinois is a bit of an outlier, wedged in between Wisconsin and Indiana. And out in the West we see California also has many crime guns imported from elsewhere.\n\n\n\n\n\nMany states in New England, as well as Illinois and California, have a high proportion of crime guns registered in other states.\n\n\n\n\nIf we compare this map above to a map showing the number of restrictive gun laws passed (that is, any legislation that limits gun ownership, or repeals a previously permissive gun law) we see a fairly close overlap. California is noteworthy for having the largest raw number of restrictive gun legislation passed since 1990. States like Illinois, New York, Connecticut, New Jersey, and Massachusetts also have much higher numbers of restrictive laws passed.\n\n\n\n\n\nStates in the South, Southwest, and Northern New England pass fewer restrictive gun laws relative to other parts of the county.\n\n\n\n\nSo does increased gun control cause the importation of guns outside that state? Well, a very basic comparison of the number of restrictive gun laws on the proportion of imported crime guns does seem to support that:\n\n\n\n\n\n\n\n\n\nIn the bottom-left corner there are a lot of states with relatively permissive gun laws who also import very few crime guns from other states. On the upper-right quadrant we see states with much more restrictive gun laws who have a much higher proportion of guns imported from other states. To be clear, this isn’t a huge revelation to many. There are a lot of studies showing how crime guns are imported through gun traffickers, out-of-state dealers, and straw purchasers (Cook et al. 2014) which bypass the restrictive state laws.\nWith this data we can also look at some state-by-state patterns to observe where these flows of guns originate from. Logically, we would expect to see guns purchased in more permissive states to flow to less permissive ones. Below we have some examples from Illinois, New York, and Massachusetts.\n\nIllinois\nWhile I currently live in Connecticut, my home state of Illinois has had a long history with gun violence. Despite having more restrictive gun laws compared to much of the country, gun violence has been a serious and ongoing issue.\nIndeed, it is not much of a secret that many guns used in crimes come from nearby states with less restrictive laws.Looking below we see that just under half of seized crime guns in Illinois are registered inside the state. Neighboring Indiana comprises about 17%, and then 5% and 4% from Missouri and Wisconsin, respectively.\n\n\n\n\n\nThe majority of crime guns not registered in Illinois come from neighboring Indiana.\n\n\n\n\n\n\n\n\n\nRegistered State\nCrime Guns\nProportion\n\n\n\n\nillinois\n18332\n0.488\n\n\nindiana\n6327\n0.169\n\n\nmissouri\n1961\n0.052\n\n\nwisconsin\n1489\n0.040\n\n\nkentucky\n1053\n0.028\n\n\n\n\n\n\n\nNew York\nIn contrast to Illinois, many crime guns are imported from much farther away. The so called “iron pipeline” represents the interstate flow of guns from Southern states with relatively lax gun laws to the more restrictive states in New England (Braga et al. 2012). In the map below it is quite clear that most of the out of state guns used in crimes come from Georgia, Virginia, and South Carolina.\n\n\n\n\n\nThe ‘Iron Pipeline’ draws guns from South Atlantic states into New York.\n\n\n\n\n\n\n\n\n\nRegistered State\nCrime Guns\nProportion\n\n\n\n\nnew york\n2982\n0.180\n\n\ngeorgia\n2133\n0.129\n\n\nvirginia\n1577\n0.095\n\n\nsouth carolina\n1453\n0.088\n\n\npennsylvania\n1228\n0.074\n\n\n\n\n\n\n\nMassachusetts\nInterestingly, while Massachusetts has similarly strict gun laws as New York, most of the crime guns actually flow from New Hampshire and Maine. While these states lie near the cluster of states with more restrictive laws, both New Hampshire and Maine are much more permissive with gun ownership. Here, their proximity is likely a larger factor in the proportion of imported guns relative to New York.\n\n\n\n\n\nCrime guns in Massachusetts come primarily from nearyby Maine and New Hampshire, as well as from the deep South.\n\n\n\n\n\n\n\n\n\nRegistered State\nCrime Guns\nProportion\n\n\n\n\nmassachusetts\n1234\n0.268\n\n\nnew hampshire\n621\n0.135\n\n\nmaine\n394\n0.085\n\n\ngeorgia\n359\n0.078\n\n\nflorida\n330\n0.072"
  },
  {
    "objectID": "posts/crime-guns/crime-guns.html#what-does-this-mean-for-gun-control",
    "href": "posts/crime-guns/crime-guns.html#what-does-this-mean-for-gun-control",
    "title": "Where Do Crime Guns in Your State Come From?",
    "section": "What Does This Mean For Gun Control?",
    "text": "What Does This Mean For Gun Control?\nI’ve done a lot of work on guns and gun violence in my academic career. To be clear, I believe gun ownership is a right that many owners take quite seriously (indeed, I am a gun owner as well). However, it’s also clear that the proliferation of guns in the United States is a serious public health problem (Bauchner et al. 2017). Laws restricting gun ownership in many cases makes sense - for example among perpetrators of intimate partner violence (Zeoli, Malinski, and Turchan 2016). What is less clear is how we make this work in a country with a patchwork of laws and a highly politicized environment around gun ownership. While I think gun legislation is a key part of reducing the burden that gun violence plays in the US, I also think a more “holistic” approach is also needed to address other parts of the problem - like access to mental health and addressing concentrated disadvantage in many places. Given the environment we live in, simply passing more restrictive laws are unlikely to fix everything."
  },
  {
    "objectID": "posts/crime-guns/crime-guns.html#comments",
    "href": "posts/crime-guns/crime-guns.html#comments",
    "title": "Where Do Crime Guns in Your State Come From?",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/duck-db/duckdb.html",
    "href": "posts/duck-db/duckdb.html",
    "title": "Dear Crime Analysts: Why You Should Use SQL Inside of R",
    "section": "",
    "text": "When I was in grad school working on my Ph.D. I learned a lot about math, statistics, research methods, and experimental design (among a LOT of other things). For a good part of my time as a grad student I also worked doing crime analysis at the Detroit Police Department for Ceasefire and Project Green Light. However, looking back, I realize one skill I never learned, that has become invaluable today, is something I never would have guessed: SQL. Yes, that SQL.\nFor my academic friends who aren’t in the know, SQL stands for “Structured Query Language” and is the number one way that analysts interface with data stored in databases. SQL is great because it is a fast and efficient way to pull data out of very large and complex tables. In addition it doesn’t require you to read an entire table into memory. For reference, at my day job I typically work with medical claims data tables with billions of records. It is simply not possible (nor recommended) to work with the entire table in-memory.\nDuring grad school my typical workflow was to try and load a single large data table into R and work with it directly, or manually break it into smaller .csv files. Not only is this highly inefficient, it also makes it difficult to replicate the workflow later. I think being able to work with large complex datasets is increasingly important for researchers who want to take control of their workflow."
  },
  {
    "objectID": "posts/duck-db/duckdb.html#the-one-big-thing-i-didnt-learn-in-grad-school",
    "href": "posts/duck-db/duckdb.html#the-one-big-thing-i-didnt-learn-in-grad-school",
    "title": "Dear Crime Analysts: Why You Should Use SQL Inside of R",
    "section": "",
    "text": "When I was in grad school working on my Ph.D. I learned a lot about math, statistics, research methods, and experimental design (among a LOT of other things). For a good part of my time as a grad student I also worked doing crime analysis at the Detroit Police Department for Ceasefire and Project Green Light. However, looking back, I realize one skill I never learned, that has become invaluable today, is something I never would have guessed: SQL. Yes, that SQL.\nFor my academic friends who aren’t in the know, SQL stands for “Structured Query Language” and is the number one way that analysts interface with data stored in databases. SQL is great because it is a fast and efficient way to pull data out of very large and complex tables. In addition it doesn’t require you to read an entire table into memory. For reference, at my day job I typically work with medical claims data tables with billions of records. It is simply not possible (nor recommended) to work with the entire table in-memory.\nDuring grad school my typical workflow was to try and load a single large data table into R and work with it directly, or manually break it into smaller .csv files. Not only is this highly inefficient, it also makes it difficult to replicate the workflow later. I think being able to work with large complex datasets is increasingly important for researchers who want to take control of their workflow."
  },
  {
    "objectID": "posts/duck-db/duckdb.html#duckdb-and-r",
    "href": "posts/duck-db/duckdb.html#duckdb-and-r",
    "title": "Dear Crime Analysts: Why You Should Use SQL Inside of R",
    "section": "DuckDB and R",
    "text": "DuckDB and R\nThere are a lot of different ways to interface with SQL. In earlier projects I’ve used a SQLite database to manage a very large dataset and then query it from R. However, this approach requires you to create a .sqlite database and adds a bit of up-front work. Often times I might just have one or two very large tables where this approach is a bit overkill. For example, working with raw NIBRS data entails only a few important tables (victim, offender, offense) but each table is far too large to work with directly.\nDuckDB is a great option here because it has a ton of very useful functions that allow you to read directly from a .csv or other type of file (JSON, Parquet,etc…). In addition this can work directly in R using a client API. For an academic, we often only have simple tables like these to work with and so having an option that we can easily integrate into our workflow is really appealing.\n\nSetting it up\nAs an example, I have some crime data from Detroit that I used for a project a few years back. The size of this file is large (although not that large). However it is big enough that it might be reasonable to pull only a subset of the data into memory at a time. Here’s a perfect use-case for duckDB. Below, I start by loading the duckdb library and setting up the working directory of the file location dir as a string. This will make it a bit cleaner to read and pass in to our queries when we start working.\n\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(sf)\n\nwd &lt;- \"../Documents/blog/data/\"\nfile_name &lt;- \"crime.csv\"\ndir &lt;- paste0(wd,file_name)\n\nWe then set up a connection object via dbConnect and use duckdb() as the connector. After we do that all we need to do is built a SQL string and pass it in. The script below reads all the columns (SELECT *) from the table listed in the directory crime.csv and pulls only the top 10 rows. And this builds the connection and executes the query:\n\ncon = dbConnect(duckdb())\ndbGetQuery(con, sprintf(\"SELECT * FROM read_csv('%s') LIMIT 10\", dir))\n\n         category       offense       crno       date           address\n1           FRAUD FRAUD (OTHER) 1501070062 2015-01-07     02600 E 8MILE\n2  STOLEN VEHICLE VEHICLE THEFT 1501310051 2015-01-31     15300 VAUGHAN\n3           FRAUD FRAUD (OTHER) 1503160171 2015-02-01     10000 GEORGIA\n4  STOLEN VEHICLE VEHICLE THEFT 1502020211 2015-02-02 14900 E JEFFERSON\n5           FRAUD FRAUD (OTHER) 1504090191 2015-02-02    20500 SAN JUAN\n6         ASSAULT  INTIMIDATION 1502090035 2015-02-09    14400 FREELAND\n7  STOLEN VEHICLE VEHICLE THEFT 1502240058 2015-02-23   15700 KENTFIELD\n8           FRAUD FRAUD (OTHER) 1502240128 2015-02-24   19300 PINEHURST\n9           FRAUD FRAUD (OTHER) 1502270221 2015-02-25      00100 SEWARD\n10          FRAUD FRAUD (OTHER) 1502270182 2015-02-25    19100 KEYSTONE\n        lon     lat     yr_mon crime_type\n1  -83.0759 42.4466 2015-01-01   property\n2  -83.2380 42.4021 2015-01-01   property\n3  -83.0042 42.3961 2015-02-01   property\n4  -82.9388 42.3746 2015-02-01   property\n5  -83.1480 42.4453 2015-02-01   property\n6  -83.1846 42.3945 2015-02-01   disorder\n7  -83.2407 42.4055 2015-02-01   property\n8  -83.1670 42.4333 2015-02-01   property\n9  -83.0786 42.3748 2015-02-01   property\n10 -83.0510 42.4341 2015-02-01   property\n\n\nVoila! As an aside, if you are more familiar with dplyr’s syntax, the equivalent code would be. This is a bit less verbose, but requires you to read in the entire table before selecting just the top 10 rows. It is vastly less efficient and slow in cases where the table sizes become very large.\n\nread_csv(dir) %&gt;%\n  slice(1:10)\n\nRows: 321983 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): category, offense, address, crime_type\ndbl  (3): crno, lon, lat\ndate (2): date, yr_mon\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 10 × 9\n   category  offense   crno date       address   lon   lat yr_mon     crime_type\n   &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;     \n 1 FRAUD     FRAUD … 1.50e9 2015-01-07 02600 … -83.1  42.4 2015-01-01 property  \n 2 STOLEN V… VEHICL… 1.50e9 2015-01-31 15300 … -83.2  42.4 2015-01-01 property  \n 3 FRAUD     FRAUD … 1.50e9 2015-02-01 10000 … -83.0  42.4 2015-02-01 property  \n 4 STOLEN V… VEHICL… 1.50e9 2015-02-02 14900 … -82.9  42.4 2015-02-01 property  \n 5 FRAUD     FRAUD … 1.50e9 2015-02-02 20500 … -83.1  42.4 2015-02-01 property  \n 6 ASSAULT   INTIMI… 1.50e9 2015-02-09 14400 … -83.2  42.4 2015-02-01 disorder  \n 7 STOLEN V… VEHICL… 1.50e9 2015-02-23 15700 … -83.2  42.4 2015-02-01 property  \n 8 FRAUD     FRAUD … 1.50e9 2015-02-24 19300 … -83.2  42.4 2015-02-01 property  \n 9 FRAUD     FRAUD … 1.50e9 2015-02-25 00100 … -83.1  42.4 2015-02-01 property  \n10 FRAUD     FRAUD … 1.50e9 2015-02-25 19100 … -83.1  42.4 2015-02-01 property  \n\n\n\n\nOther Tricks - Aggregations and Plotting\nOf course SQL is a very robust scripting language that allows for both simple and complex operations. We can do any kind of reporting and aggregations. For example, if we wanted some basic information about crime at the year-month level we could do:\n\nqu = \n  \"SELECT\n      crime_type,\n      yr_mon,\n      COUNT(crime_type) AS N\n  FROM\n      read_csv('%s')\n  GROUP BY\n      crime_type,\n      yr_mon\n  ORDER BY\n      crime_type,\n      yr_mon\"\n\ntab &lt;- dbGetQuery(con, sprintf(qu, dir))\nhead(tab)\n\n  crime_type     yr_mon    N\n1   disorder 2015-01-01 1518\n2   disorder 2015-02-01 1447\n3   disorder 2015-03-01 1797\n4   disorder 2015-04-01 1884\n5   disorder 2015-05-01 2125\n6   disorder 2015-06-01 1839\n\n\nThis performs the group-by and counts out of memory and then moves the aggregated table right into R as a dataframe. And since the result is just a dataframe, we can pipe it directly into a ggplot visualization, like:\n\ndbGetQuery(con, sprintf(qu, dir)) %&gt;%\n  ggplot() +\n  geom_line(aes(x = yr_mon, y = N, color = crime_type), linewidth = 1) +\n  facet_wrap(~crime_type, scales = \"free\", ncol = 2) +\n  labs(x = \"Year-Month\", y = \"Count\") +\n  scale_color_manual(values = c('#004488', '#DDAA33', '#BB5566')) +\n  theme_minimal() +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nSimilarly, this applies to other functions like creating spatial objects. What if we wanted to plot only the violent crimes from the first month of 2015?\n\nqu =\n  \"SELECT *\n  FROM \n      read_csv('%s')\n  WHERE \n      crime_type = 'violent'\n  AND\n      datepart('year', yr_mon) = 2015\n  AND\n      datepart('month', yr_mon) = 1\n\"\n\nst_as_sf(dbGetQuery(con, sprintf(qu, dir)),\n         coords = c('lon', 'lat'),\n         crs = 4326) %&gt;%\n  ggplot() +\n  geom_sf(shape = 21, alpha = .2, fill = '#BB5566') +\n  theme_void()\n\n\n\n\n\n\n\n\nThis is pretty cool too, because we have access to all the base functions available in duckDB. For example, there are a bunch of date handling functions that make these types of queries a lot easier than other base SQL languages. Here, the datepart function lets us split up date objects very easily within SQL."
  },
  {
    "objectID": "posts/hbos-anomaly/hbos_anomaly.html",
    "href": "posts/hbos-anomaly/hbos_anomaly.html",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "This is the third part of a 3-part series. In the first two posts I described how I built a principal components analysis anomaly detector and a k-nearest neighbors anomaly detector as components for a ensemble model. This third post will discuss the last piece, which is a histogram-based anomaly detector.\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector"
  },
  {
    "objectID": "posts/hbos-anomaly/hbos_anomaly.html#part-3-the-histogram-based-anomaly-detector",
    "href": "posts/hbos-anomaly/hbos_anomaly.html#part-3-the-histogram-based-anomaly-detector",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "",
    "text": "This is the third part of a 3-part series. In the first two posts I described how I built a principal components analysis anomaly detector and a k-nearest neighbors anomaly detector as components for a ensemble model. This third post will discuss the last piece, which is a histogram-based anomaly detector.\n\n“Soft” principal components anomaly detector\nK-nearest neighbors anomaly detector\nIsolation forest or histogram-based anomaly detector"
  },
  {
    "objectID": "posts/hbos-anomaly/hbos_anomaly.html#building-a-histogram-based-outlier-detector",
    "href": "posts/hbos-anomaly/hbos_anomaly.html#building-a-histogram-based-outlier-detector",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Building a Histogram-Based Outlier Detector",
    "text": "Building a Histogram-Based Outlier Detector\n\nDefining sub-space density\nThe core of the idea behind a histogram-based outlier detector is that it is a method to efficiently explore subspaces of the data by binning observations into discrete groups, then weighting each bin inversely by the number of observations (more on this in a moment). To start, we can provide a quick example showing how we can use histograms to partition the data into bins. Below, I create two histograms for the features representing stay length and average cost per-stay.\n\n\nCode\n# define breaks\nh1 &lt;- hist(df$stay_len, breaks = 5, plot = FALSE)\nh2 &lt;- hist(df$cost_per_stay, breaks = 5, plot = FALSE)\n\n# append to dataframe\nhdf &lt;- df %&gt;%\n  mutate(d1 = findInterval(stay_len, h1$breaks),\n         d2 = findInterval(cost_per_stay, h2$breaks),\n         space = paste0(d1,\"-\",d2))\n\n# Create a data frame with a grid of values for the predictor variables\ngrid &lt;- expand.grid(stay_len = seq(min(hdf$stay_len), max(hdf$stay_len), length.out = 10),\n                    cost_per_stay = seq(min(hdf$cost_per_stay), max(hdf$cost_per_stay), length.out = 10)) %&gt;%\n  mutate(d1 = findInterval(stay_len, h1$breaks),\n         d2 = findInterval(cost_per_stay, h2$breaks),\n         space = paste0(d1,\"-\",d2)) %&gt;%\n    mutate(space = ifelse(space %in% hdf$space, space, NA)) %&gt;%\n    fill(space)\n\n\nIf we plot each of these histograms, we can observe that most values concentrate in a few bins, while a small number of values are in more sparsely-populated bins. Obviously this shows us that the majority of stay lengths are between 0-10 days, and the average cost per-stay is around $4,000.\n\n\nCode\npar(mfrow=c(1,2))\nplot(h1, main = \"Stay Length\", xlab = \"Days\", col = '#004488', border = 'white')\nplot(h2, main = \"Cost Per Stay\", xlab = \"Cost\", col = '#004488', border = 'white')\n\n\n\n\n\nA histogram’s bins are proportional to the number of observations.\n\n\n\n\nWe can plot this in 2 dimensions to see how the feature space distribution is subdivided based on histogram bins. As we would expect, the majority of observations fall into a few regions, while potential outliers exist in much more sparsely populated bins. This is actually fairly similar to a decision tree, where we classify observations based on a set of rules. For example, the lone observation on the far right of the plot is in a region where stay_length &gt;= 44 and cost_per_stay &gt;= 3367 and cost_per_stay &lt;= 4837.\n\n\nCode\nggplot() +\n  geom_tile(data = grid, aes(x = stay_len, y = cost_per_stay, fill = space), alpha = .6) +\n  geom_point(data = df, aes(x = stay_len, y = cost_per_stay), color = '#004488') +\n  labs(x = 'Stay Length', y = 'Cost Per Stay') +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n2D histogram partitioning of observations. Colored regions represent different bin partitions. Note the sparse regions at the top left and lower right quadrants.\n\n\n\n\n\n\nScoring observations\nWith this in mind, we are essentially going to do the above, but in \\(d\\) dimensions (where \\(d\\) is the number of input features). To give each observation an anomaly score, we will follow a very simple scoring mechanism proposed by the original authors of the method where:\n\\[HBOS(p) = \\sum^d_{i=0}log(\\frac{1}{hist_i(p)})\\] Which states that the histogram-based anomaly score is the sum of the log of inverse histogram densities. More simply, for each feature \\(d\\) we compute a histogram density, and each observation is scored based on the inverse of its bin density (Goldstein and Dengel 2012). This means observations in sparsely populated bins receive higher scores, and vice-versa. One of the trade-offs here is that we have to assume feature independence (which is a tenuous assumption in a lot of cases), but even violations of this might not be too bad.\n\n\nA brief aside: choosing the optimal bin size\nOne challenge with this approach is that before we calculate histogram densities we need to define the number of bins for our histograms ahead of time. Now, one simple method might just be to choose a very rough rule-of-thumb (e.g. the “Sturges” rule of \\(1+log2(N)\\)) or to just choose a constant number like 5 or 10. A more principled way, however, would be to derive the optimal number of bins based on some properties of the input data.\nThere are a lot of proposed options out here, but the one that makes a lot of sense to me (and, incidentally, is also used in the pyod implementation of this function) is to iteratively fit histograms, calculate a penalized maximum likelihood estimate for each histogram \\(D\\), and then select the number of bins corresponding to the maximum likelihood estimate (Birgé and Rozenholc 2006). A rough R implementation of this is shown below:\n\n  # internal function: compute optimal bins\nopt_bins &lt;- function(X, upper_bound = 15)\n  {\n    \n    epsilon = 1\n    n &lt;- length(X)\n    \n    # maximum likelihood estimate for bin\n    maximum_likelihood &lt;- array(0, dim = c(upper_bound - 1, 1))\n    \n    # rule of thumb for upper bound\n    if (is.null(upper_bound)) {\n      upper_bound &lt;- as.integer(sqrt(length(X)))\n    }\n    \n    for (i in seq_along(1:(upper_bound - 1))) {\n      b &lt;- i + 1\n      histogram &lt;- hist(X, breaks = b, plot = FALSE)$counts\n      \n      maximum_likelihood[i] &lt;-\n        sum(histogram * log(b * histogram /  n + epsilon) - (b - 1 + log(b) ^ 2.5))\n    }\n    \n    return(which.max(maximum_likelihood))\n  }\n\nSo, running this for the first feature stay_len, we get:\n\nopt_bins(df$stay_len)\n\n[1] 5\n\n\n\n\nBuilding the detector\nWith the issue of histogram bins out of the way, we can procede with the rest of the model. The last bit is really quite simple. For each feature \\(d\\) we compute the optimal number of bins (using the function we just defined above). We then build a histogram for that feature and identify which points fall within each bin. We then score each point according to the formula above, which is the log of the inverse histogram density (making outlying observations have correspondingly higher anomaly scores). The last thing we do after running this algorithm over all \\(d\\) features is to scale their scores (here, I use min-max normalization) and sum them together. The code to do this is below:\n\n# run HBOS\n  for(d in 1:d){\n    \n    h &lt;- hist(X[,d], breaks = opt_bins(X[,d]), plot = FALSE)\n    fi &lt;- findInterval(X[,d], h$breaks)\n    \n    hbos[[d]] &lt;- log(1/h$density[fi])\n  }\n  \n  # minmax scale feature vectors\n  hbos &lt;- lapply(hbos, function(x){(x- min(x)) /(max(x)-min(x))})\n  \n  # return score\n  return(apply(do.call(cbind, hbos), 1, sum))"
  },
  {
    "objectID": "posts/hbos-anomaly/hbos_anomaly.html#running-the-model",
    "href": "posts/hbos-anomaly/hbos_anomaly.html#running-the-model",
    "title": "Building an Outlier Ensemble from ‘Scratch’",
    "section": "Running the Model",
    "text": "Running the Model\nNow we’re ready to run everything. For simplicity, I wrap all this code into a single adHBOS function that contains the optimal histogram binning and the scoring (see: Section 3.1). For flagging anomalies we will just identify the highest 5% (a rough, but arguably acceptable heuristic).\n\nX &lt;- df[,2:7]\n\ndf$anom &lt;-  adHBOS(X)\ndf$flag &lt;- ifelse(df$anom &gt;= quantile(df$anom, .95),1,0)\n\nIf we look at a histogram of we see most scores are low, while the outliers are clearly visible on the right-hand side.\n\n\nCode\nggplot(df) +\n  geom_histogram(aes(x = anom), bins = 10, color = 'white', fill = '#004488')+\n  labs(x = \"Anomaly Score\", y = \"Observations\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nHistogram-based anomaly scores. More anomalous observations have higher scores\n\n\n\n\nComparing this to our earlier plot we see:\n\n\nCode\nggplot() +\n  geom_tile(data = grid, aes(x = stay_len, y = cost_per_stay, fill = space), alpha = .6) +\n  geom_point(data = df, aes(x = stay_len, y = cost_per_stay), color = '#004488') +\n  geom_point(data = df[df$flag == 1,], aes(x = stay_len, y = cost_per_stay, color = '#BB5566'), size = 2) +\n  labs(x = 'Stay Length', y = 'Cost Per Stay') +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  theme(axis.text = element_text(size = 10),\n        axis.title = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\nPlotting inliers (blue) and outliers (red) in 2D space.\n\n\n\n\nAs we expect, most of the observations that are flagged as outliers reside in bins with few other observations. This is pretty consistent with the other two methods we used before (PCA and KNN anomaly detectors). One specific advantage of the HBOS method is that it is very fast for even large datasets. However, with higher levels of dimensionality it is very likely that the assumption of feature independence is tenuous at best. Other methods, like the isolation forest can often perform better in higher dimensions. However, the simplicity of the method makes it easy to explain, which can be a benefit in many cases!\n\nHBOS Anaomaly Detector: Example Function\nHere’s a minimal working example of the procedure above. As we build our ensemble, we’ll come back to this function later.\n\n\nCode\n# Run a principal components anomaly detector\nadHBOS &lt;- function(X, ub = 15){\n  \n  # scale input features, define list to hold scores\n  X &lt;- scale(X)\n  j &lt;- dim(X)[2]\n  hbos &lt;- vector(\"list\",j)\n  \n  # internal function: compute optimal bins\n  opt_bins &lt;- function(X, upper_bound = ub)\n  {\n    \n    epsilon = 1\n    n &lt;- length(X)\n    \n    # maximum likelihood estimate for bin\n    maximum_likelihood &lt;- array(0, dim = c(upper_bound - 1, 1))\n    \n    # rule of thumb for upper bound\n    if (is.null(upper_bound)) {\n      upper_bound &lt;- as.integer(sqrt(length(X)))\n    }\n    \n    for (i in seq_along(1:(upper_bound - 1))) {\n      b &lt;- i + 1\n      histogram &lt;- hist(X, breaks = b, plot = FALSE)$counts\n      \n      maximum_likelihood[i] &lt;-\n        sum(histogram * log(b * histogram /  n + epsilon) - (b - 1 + log(b) ^ 2.5))\n    }\n    \n    return(which.max(maximum_likelihood))\n  }\n  \n  # run HBOS\n  for(j in 1:j){\n    \n    h &lt;- hist(X[,j], breaks = opt_bins(X[,j]), plot = FALSE)\n    fi &lt;- findInterval(X[,j], h$breaks)\n    \n    hbos[[j]] &lt;- log(1/h$density[fi])\n  }\n  \n  # minmax scale feature vectors\n  hbos &lt;- lapply(hbos, function(x){(x- min(x)) /(max(x)-min(x))})\n  \n  # return score\n  return(apply(do.call(cbind, hbos), 1, sum))\n  \n}"
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html",
    "href": "posts/hlm-osha/osha_shrinkage.html",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "",
    "text": "My friend, Andy Wheeler, just recently posted on his blog about reported injuries at Amazon warehouses. As he rightly points out, the apparent high number of injuries at these warehouses is primarily a function of the size of the locations.\nIn criminology we often deal with similar issues (namely, why we use crime rates rather than raw counts when comparing geographies of different populations). While I don’t have much to add to Andy’s post, one thing did stand out to me - the issue of low base counts.\n\nBut note that I don’t think Bonded Logistics is a terribly dangerous place. One thing you need to watch out for when evaluating rate data is that places with smaller denominators (here lower total hours worked) tend to be more volatile.(“Injury Rates at Amazon Warehouses” 2022)\n\nThis is also a very common problem across many different disciplines. Andrew Gelman discusses the problem in this paper about issues arising from mapping county-level cancer rates. Similarly, he points out that very variable rates arise from very low sample sizes. For example: imagine a single murder occurs in the city of Union, CT. With a population of 854, that gives us a murder rate per 1,000 of \\(\\frac{1}{854} * 1,000 = 1.17\\). This would potentially make it one of the highest-rate small towns in the state! Logically this doesn’t make sense, because rare events can happen - but it doesn’t imply a single region is especially unusual.\n\n\n\nCounties with low population appear to have very high rates of kidney cancer. However, much of this is an illusion due to higher variance relative to higher population counties."
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#injuries-and-low-base-counts",
    "href": "posts/hlm-osha/osha_shrinkage.html#injuries-and-low-base-counts",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "",
    "text": "My friend, Andy Wheeler, just recently posted on his blog about reported injuries at Amazon warehouses. As he rightly points out, the apparent high number of injuries at these warehouses is primarily a function of the size of the locations.\nIn criminology we often deal with similar issues (namely, why we use crime rates rather than raw counts when comparing geographies of different populations). While I don’t have much to add to Andy’s post, one thing did stand out to me - the issue of low base counts.\n\nBut note that I don’t think Bonded Logistics is a terribly dangerous place. One thing you need to watch out for when evaluating rate data is that places with smaller denominators (here lower total hours worked) tend to be more volatile.(“Injury Rates at Amazon Warehouses” 2022)\n\nThis is also a very common problem across many different disciplines. Andrew Gelman discusses the problem in this paper about issues arising from mapping county-level cancer rates. Similarly, he points out that very variable rates arise from very low sample sizes. For example: imagine a single murder occurs in the city of Union, CT. With a population of 854, that gives us a murder rate per 1,000 of \\(\\frac{1}{854} * 1,000 = 1.17\\). This would potentially make it one of the highest-rate small towns in the state! Logically this doesn’t make sense, because rare events can happen - but it doesn’t imply a single region is especially unusual.\n\n\n\nCounties with low population appear to have very high rates of kidney cancer. However, much of this is an illusion due to higher variance relative to higher population counties."
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#hierarchical-models",
    "href": "posts/hlm-osha/osha_shrinkage.html#hierarchical-models",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\nAll of this discussion made me think about some issues I had addressed when studying crime - namely rare events (homicides or shootings) that are aggregated to small areas (census blocks or street segments). In these previous examples I had applied hierarchical models to help adjust for these issues we commonly observe with rare events. Let’s work with the same data that Andy used in his example. First, we’ll load the OSHA data for 2021 and isolate just the warehouses in North Carolina.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(knitr)\n\n# load osha data\nosha &lt;- read_csv(unzip(\"C:/Users/gioc4/Dropbox/blogs/hlm_osha/ITA-data-cy2021.zip\"))\n\n# isolate NC injuries at warehouses\ninj_wh &lt;- osha %&gt;%\n  filter(naics_code == '493110',\n         state == 'NC') %&gt;%\n  mutate(inj_rate = (total_injuries/total_hours_worked)*2080)\n\n\nIf we plot the distribution of injury rates per-person work hour year we see that the majority of warehouses are quite low, and very few exceed 0.2. However on the far right we see a single extreme example - the outlier that is the Bonded Logistics warehouse.\n\n\nShow code\nggplot(inj_wh) +\n  geom_histogram(aes(x = inj_rate), \n                 fill = \"#004488\", \n                 color = \"white\",\n                 bins = 20,\n                 linewidth =1.5) +\n  labs(x = \"(total_injuries/total_hours_worked)*2080\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nSorting by the top 10 we see that Bonded Logistics has an injury rate nearly 4 times the next highest warehouse. But they also have only a single employee who worked 1,686 hours that year! Is this really a fair comparison? Following what we already know, almost certainly not.\n\n\nShow code\ninj_wh %&gt;%\n  select(company_name, inj_rate, annual_average_employees, total_hours_worked) %&gt;%\n  arrange(desc(inj_rate)) %&gt;%\n  slice(1:10) %&gt;%\n  kable(caption = \"Top 10 Warehouses, by Injury Rate\", \n        digits = 2,\n        col.names = c(\"Company\",\"Injury Rate\",\"Employees\",\"Total Hours Worked\"))\n\n\n\nTop 10 Warehouses, by Injury Rate\n\n\n\n\n\n\n\n\nCompany\nInjury Rate\nEmployees\nTotal Hours Worked\n\n\n\n\nBonded Logistics\n1.23\n1\n1686\n\n\nTechnimark\n0.34\n4\n6154\n\n\nBonded Logistics\n0.30\n4\n6913\n\n\nCAPSTONE LOGISTICS, LLC\n0.29\n36\n43137\n\n\nRH US LLC\n0.27\n7\n7703\n\n\nAldi (N.C.) L.L.C.\n0.22\n385\n569274\n\n\nBrame Specialty Company Inc\n0.17\n6\n12513\n\n\nConn Appliances Inc\n0.16\n15\n26634\n\n\nBlue Line Distribution\n0.16\n31\n53287\n\n\nTosca Services, LLC\n0.16\n41\n66891\n\n\n\n\n\nTo address this issue, we’ll fit a (very) simple Bayesian hierarchical linear model where we give each warehouse its own intercept. We then partially pool estimates from the model toward the group-level means. In short, we’ll model this as the number of injuries \\(y\\) at each warehouse \\(j\\) as a Poisson process, where each warehouse is modeled with its own (varying) intercept. In a minute we will see the advantage of this.\n\\[y_{j} \\sim Poisson(\\lambda_{j})\\] \\[ln(\\lambda{j}) = \\beta_{0j}\\]\nUsing brms we’ll fit a Poisson regression estimating the total number of injuries at any warehouse weighted by the logged number of hours worked. Because the model is extremely simple, we’ll just keep the default priors with this model which are student_t(3,0,3).\n\n\nCode\n# fit the hierarchical model w/ default priors\n\nfit &lt;- brm(total_injuries ~ 1 + (1|id) + \n             offset(log(total_hours_worked)), \n           family = poisson(), \n           data = inj_wh,\n           file = \"C:/Users/gioc4/Dropbox/blogs/hlm_osha/brmfit\",\n           chains = 4, cores = 4, iter = 2000)\n\n\nAfter the model fits, it’s generally a good idea to make sure the predictions from the model correspond with the observed distribution of the data. Our posterior predictive checks show that we have fairly well captured the observed process, with our posterior simulations \\(\\hat{y}\\) largely in line with the observed \\(y\\).\n\nShow code\npp_check(fit, \"hist\") + theme_bw()\npp_check(fit, \"scatter_avg\") + theme_bw()"
  },
  {
    "objectID": "posts/hlm-osha/osha_shrinkage.html#shrinkage",
    "href": "posts/hlm-osha/osha_shrinkage.html#shrinkage",
    "title": "Injuries at Amazon Warehouses - A Bayesian Approach",
    "section": "Shrinkage!",
    "text": "Shrinkage!\nHere’s where things get interesting. One of the benefits of a hierarchical model is that estimates from the model are partially pooled (shrunk) toward the group-level means. In a typical no-pooling model, estimates from very sparse clusters can be extreme or even undefined. In our hierarchical example we are applying regularization to the estimates by trading higher bias for lower variance(Gelman et al. 1995). In a Bayesian framework our application of a prior distribution helps set a reasonable boundary for our model estimates.\nTo illustrate this, we can see the difference between the predicted (blue circles) and observed (empty circles) below. For warehouses with very few worked hours we see that the estimates are pulled strongly toward the global mean. For warehouses with more hours, however, there is considerably less shrinkage.\n\n\nShow code\n# predicted vs actual\ninj_wh_pred &lt;- inj_wh %&gt;%\n  select(id, company_name, inj_rate, annual_average_employees, total_hours_worked) %&gt;%\n  mutate(yhat = predict(fit, type = 'response')[,1],\n         inj_rate_pred = (yhat/total_hours_worked) * 2080)\n\n# Plot all values\nggplot(inj_wh_pred, aes(x = total_hours_worked)) +\n  geom_hline(yintercept = mean(inj_wh_pred$inj_rate), linewidth = 1, color = \"#004488\", alpha = .7)+\n  geom_point(aes(y = inj_rate), shape = 21, size = 2)+\n  geom_point(aes(y = inj_rate_pred), shape = 21, fill = \"#004488\", size = 2, alpha = .5) +\n  labs(x = \"Total Hours Worked\", y = \"Total Injuries/Hours Worked\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nIf we constrain ourselves to the left-hand side of the plot we can view this even more clearly. The estimated value for the unusual Bonded Warehouse is 0.1 compared to the observed value of 1.23. While this estimate is farther off from the observed value, it is probably much more reasonable based on the observed values of other warehouses.\n\n\nShow code\ninj_wh_pred %&gt;%\n  filter(total_hours_worked &lt; 1e5) %&gt;%\n  ggplot(aes(x = total_hours_worked)) +\n  geom_hline(yintercept = mean(inj_wh_pred$inj_rate), linewidth = 1, color = \"#004488\", alpha = .7)+\n  geom_point(aes(y = inj_rate), shape = 21, size = 2)+\n  geom_point(aes(y = inj_rate_pred), shape = 21, fill = \"#004488\", size = 2, alpha = .5) +\n  labs(x = \"Total Hours Worked\", y = \"Total Injuries/Hours Worked\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nIf we compare the predicted injury rates to the observed ones, we can see the differences in shrinkage as well. Larger warehouses have estimates quite close to the observed counts (like The Aldi warehouse which has a relatively high rate of injuries for its size).\n\n\nCode\ninj_wh_pred %&gt;%\n  select(company_name, inj_rate, inj_rate_pred, annual_average_employees, total_hours_worked) %&gt;%\n  arrange(desc(inj_rate)) %&gt;%\n  slice(1:10) %&gt;%\n  kable(caption = \"Top 10 Warehouses, by Injury Rate\", \n        digits = 2,\n        col.names = c(\"Company\",\"Injury Rate\",\"(Pred) Injury Rate\", \"Employees\",\"Total Hours Worked\"))\n\n\n\nTop 10 Warehouses, by Injury Rate\n\n\n\n\n\n\n\n\n\nCompany\nInjury Rate\n(Pred) Injury Rate\nEmployees\nTotal Hours Worked\n\n\n\n\nBonded Logistics\n1.23\n0.10\n1\n1686\n\n\nTechnimark\n0.34\n0.08\n4\n6154\n\n\nBonded Logistics\n0.30\n0.08\n4\n6913\n\n\nCAPSTONE LOGISTICS, LLC\n0.29\n0.19\n36\n43137\n\n\nRH US LLC\n0.27\n0.07\n7\n7703\n\n\nAldi (N.C.) L.L.C.\n0.22\n0.21\n385\n569274\n\n\nBrame Specialty Company Inc\n0.17\n0.07\n6\n12513\n\n\nConn Appliances Inc\n0.16\n0.08\n15\n26634\n\n\nBlue Line Distribution\n0.16\n0.10\n31\n53287\n\n\nTosca Services, LLC\n0.16\n0.11\n41\n66891"
  },
  {
    "objectID": "posts/mmm-bayesian/mmm.html",
    "href": "posts/mmm-bayesian/mmm.html",
    "title": "An Outsider’s Perspective On Media Mix Modelling",
    "section": "",
    "text": "I’m trying something a bit new this time. Typically how I learn is that I see something interesting (either in a blog post, an academic article, or through something a co-worker is working on). I’ll then try and work through the problem via code on my own to see how I can make it work. It’s not always perfect, but it gets me started. Today I’m going to go out of my comfort zone and try my hand at Media Mix Modelling (MMM).\nIn general, the stated goal of MMM is to determine the optimal distribution of advertising money, given \\(n\\) different venues. For instance, this could determine how much to spend on internet, TV, or radio advertising given the costs of running ads and the expected return for each venue. Typically this is done using a regression to try and parse out the effect of each type of advertising net of many other factors (e.g. seasonal and trend effects, costs of the product, demand, etc…).\n\n\nGetting reliable open-source data for MMM is actually a bit more difficult than you think. There are a number of very trivial simulated datasets scattered about on places like Kaggle, but these aren’t terribly useful. I was able to find a strange mostly undocumented set of data from a git repo here. Per the author, the data purports:\n\n“…data contain information on demand, sales, supply, POS data, advertisiment expenditure and different impressions recorded across multiple channels for calculating the advertising campaign effectiveness such as Mobile SMS, Newspaper Ads, Radio, TV, Poster, Internet etc.in the form of GRP (Gross Rating Point) in Shenzhen city”\n\nGood enough for me."
  },
  {
    "objectID": "posts/mmm-bayesian/mmm.html#media-mix-modelling",
    "href": "posts/mmm-bayesian/mmm.html#media-mix-modelling",
    "title": "An Outsider’s Perspective On Media Mix Modelling",
    "section": "",
    "text": "I’m trying something a bit new this time. Typically how I learn is that I see something interesting (either in a blog post, an academic article, or through something a co-worker is working on). I’ll then try and work through the problem via code on my own to see how I can make it work. It’s not always perfect, but it gets me started. Today I’m going to go out of my comfort zone and try my hand at Media Mix Modelling (MMM).\nIn general, the stated goal of MMM is to determine the optimal distribution of advertising money, given \\(n\\) different venues. For instance, this could determine how much to spend on internet, TV, or radio advertising given the costs of running ads and the expected return for each venue. Typically this is done using a regression to try and parse out the effect of each type of advertising net of many other factors (e.g. seasonal and trend effects, costs of the product, demand, etc…).\n\n\nGetting reliable open-source data for MMM is actually a bit more difficult than you think. There are a number of very trivial simulated datasets scattered about on places like Kaggle, but these aren’t terribly useful. I was able to find a strange mostly undocumented set of data from a git repo here. Per the author, the data purports:\n\n“…data contain information on demand, sales, supply, POS data, advertisiment expenditure and different impressions recorded across multiple channels for calculating the advertising campaign effectiveness such as Mobile SMS, Newspaper Ads, Radio, TV, Poster, Internet etc.in the form of GRP (Gross Rating Point) in Shenzhen city”\n\nGood enough for me."
  },
  {
    "objectID": "posts/mmm-bayesian/mmm.html#the-adstock-function",
    "href": "posts/mmm-bayesian/mmm.html#the-adstock-function",
    "title": "An Outsider’s Perspective On Media Mix Modelling",
    "section": "The Adstock Function",
    "text": "The Adstock Function\nThe paper Bayesian Methods for Media Mix Modeling with Carryover and Shape Effects is a pretty clear and concise introduction to media mix modelling. This paper is a pretty good introduction into many of the issues related to MMM. One of the biggest issues is the need to transform the ad spend variables to better represent how they behave in real life. For example, we don’t necessarily expect an ad campaign to have an instantaneous lift, nor do we expect its effect to end immediately either. Hence, the need to model appropriate carryover effects.\nTo do this I’ll borrow a function Kylie Fu to calculate the weights for the delayed adstock function. The goal here is to define a function that can transform the ad spend variables to reflect our belief that they have delays of decay, delays of peak effect, and an upper maximum carryover effect. Below the function takes a vector of ad spend data and creates the transformation given the parameters lambda, theta, and L.\n\nDelayedSimpleAdstock &lt;- function(advertising, lambda, theta, L){\n  N &lt;- length(advertising)\n  weights &lt;- matrix(0, N, N)\n  for (i in 1:N){\n    for (j in 1:N){\n      k = i - j\n      if (k &lt; L && k &gt;= 0){\n        weights[i, j] = lambda ** ((k - theta) ** 2)\n      }\n    }\n  }\n  \n  adstock &lt;- as.numeric(weights %*% matrix(advertising))\n  \n  return(adstock)  \n}\n\n\nSetting up an adstock transformation\nNow we can choose the parameters for the adstock transformation. Ideally, we want a transformation that captures the decay of the advertising program (lambda), its delayed peak onset (theta), and its maximum effect duration (L). With a bit of simulation we can see what each parameter does across a value of different lags. The goal here is to have a function that matches what we believe the actual effect of ad spending looks like for different media regions (e.g. TV, radio, internet). Below, we can see that increasing lambda increases the decay of the effect up, while varying theta sets the peak onset of the ad campaign to later lags. The value of L simply sets the maximum effect to a specific lag.\n\n# set up grid of params to iterate over\n\nx &lt;- c(1, rep(0, 15))\nlambda = seq(0,1, by = .1)\ntheta = seq(0,10, by = 1)\nL = seq(1,12, by = 1)\n\n\nLambdaThetaL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on a visual assesment I just chose an adstock function with a lambda of .8 (suggesting moderate decay of the initial ad effect), a theta of 2 (implying a peak onset of 2 weeks), and an L of 13 which is a rule-of-thumb that makes the maximum effect quite large.\n\n\n\n\n\nAdstock function (Lambda = .8, theta = 2, L = 13)\n\n\n\n\nThe code below applies our adstock function to each of the spend variables. For simplicity here I am making the assumption that all of the modes have similar adstock functions, but this can (and should) vary per modality based on expert prior information. We convert the daily data (which is quite noisy) to a more commonly utilized weekly format. We then limit the focus of our analysis to a 4-year time span.\n\n\nCode\n# setup weekly data\n# setup weekly data\nmmm_weekly_data &lt;-\n  mmm_raw %&gt;%\n  mutate(date = as.Date(DATE, \"%m/%d/%Y\"),\n         year = year(date),\n         month = month(date),\n         week = week(date)) %&gt;%\n  select(\n    date,\n    year,\n    month,\n    week,\n    sales = `SALES ($)`,\n    spend_sms = `Advertising Expenses (SMS)`,\n    spend_news = `Advertising Expenses(Newspaper ads)`,\n    spend_radio = `Advertising Expenses(Radio)`,\n    spend_tv = `Advertising Expenses(TV)`,\n    spend_net = `Advertising Expenses(Internet)`,\n    demand = DEMAND,\n    supply = `POS/ Supply Data`,\n    price = `Unit Price ($)`\n  ) %&gt;%\n  filter(year %in% 2014:2017)\n\n\nweekly_spend &lt;-\n  mmm_weekly_data %&gt;%\n  group_by(year, month, week) %&gt;%\n  summarise(across(sales:spend_net, sum), across(demand:price, mean), .groups = 'drop') %&gt;%\n  mutate(index = 1:nrow(.))\n\n\n# Apply transformation to advertising variables, scale dollar values to per $1,000\nX &lt;-\n  weekly_spend %&gt;%\n  mutate(across(spend_sms:spend_net,~ DelayedSimpleAdstock(.,lambda = .8,theta = 2,L = 13)),\n         across(spend_sms:price, function(x) x/1e3),\n         trend = 1:nrow(.)/nrow(.))\n\n\n\n\nSetting up the model\nBefore we fit the model we can plot out the primary variables of interest, along with our dependent variable sales. Looking below we can see a few potential issues. One which should jump out immediately is that there is a very high correlation between several of our ad spend categories. For example, the correlation between TV spending and news spending is almost 1. In the case of MMM this is a common problem, which makes unique identification of the effect of ad spends much more difficult. More troubling, by just eyeballing these plots there doesn’t seem to be a terribly strong relationship between any of the advertising venues and sales.\n\nplot(X[, c('sales',\n           'spend_sms',\n           'spend_news',\n           'spend_radio',\n           'spend_tv',\n           'spend_net')], col = '#004488')\n\n\n\n\nPairwise relationships between sales ~ ad venues\n\n\n\n\nNor do the pairwise correlations seem to be very high either (in fact, they are very nearly zero). Regardless, we’ll continue by fitting a simple set of models.\n\n\nCode\nround(cor(X[, c('sales',\n           'spend_sms',\n           'spend_news',\n           'spend_radio',\n           'spend_tv',\n           'spend_net')]),2)\n\n\n            sales spend_sms spend_news spend_radio spend_tv spend_net\nsales        1.00     -0.01      -0.04       -0.03    -0.04     -0.07\nspend_sms   -0.01      1.00       0.89        0.91     0.90     -0.06\nspend_news  -0.04      0.89       1.00        0.86     1.00      0.30\nspend_radio -0.03      0.91       0.86        1.00     0.87     -0.09\nspend_tv    -0.04      0.90       1.00        0.87     1.00      0.28\nspend_net   -0.07     -0.06       0.30       -0.09     0.28      1.00"
  },
  {
    "objectID": "posts/mmm-bayesian/mmm.html#fitting-a-model",
    "href": "posts/mmm-bayesian/mmm.html#fitting-a-model",
    "title": "An Outsider’s Perspective On Media Mix Modelling",
    "section": "Fitting A Model",
    "text": "Fitting A Model\nOne of the biggest challenges with MMM is that many of the model coefficients including the advertising venues, are likely to be very highly correlated. For example, the advertising spend on TV ads is almost perfectly correlated with the spend on news ads. We can set some moderately strong priors on the ad spend coefficients to ensure that the estimates don’t explode due to multicollinearity. Here I’m just placing a normal(0, .5) prior which is still pretty wide for a coefficient on the logrithmic scale.\n\n\nCode\n# set up priors\nbprior &lt;- c(prior(normal(0,.5), class = \"b\", coef = \"spend_sms\"),\n            prior(normal(0,.5), class = \"b\", coef = \"spend_news\"),\n            prior(normal(0,.5), class = \"b\", coef = \"spend_radio\"),\n            prior(normal(0,.5), class = \"b\", coef = \"spend_tv\"),\n            prior(normal(0,.5), class = \"b\", coef = \"spend_net\"))\n\n# just intercept\nbrm_fit_0 &lt;- brm(log(sales) ~ 1, data = X,\n                 chains = 4,\n                 cores = 4,\n                 family = gaussian(),\n                 file = \"C:/Users/gioc4/Documents/blog/data/brms_models/mmm_fit0.Rdata\")\n\n# no random effects, linear trend effect\nbrm_fit_1 &lt;- brm(log(sales) ~                 \n                   demand +\n                   supply +\n                   price +\n                   as.factor(month) +\n                   as.factor(week) +\n                   trend +\n                   spend_sms +\n                   spend_news +\n                   spend_radio +\n                   spend_tv +\n                   spend_net,\n                 data = X,\n                 chains = 4,\n                 cores = 4,\n                 prior = bprior,\n                 family = gaussian(),\n                 file = \"C:/Users/gioc4/Documents/blog/data/brms_models/mmm_fit1.Rdata\")\n\n# random effects for month+week\n# smoothing spline for trend\nbrm_fit_2 &lt;- brm(log(sales) ~                 \n                   demand +\n                   supply +\n                   price +\n                   s(trend) +\n                   spend_sms +\n                   spend_news +\n                   spend_radio +\n                   spend_tv +\n                   spend_net +\n                   (1|month) +\n                   (1|week),\n                 data = X,\n                 chains = 4,\n                 cores = 4,\n                 prior = bprior,\n                 family = gaussian(),\n                 control = list(adapt_delta = .9),\n                 file = \"C:/Users/gioc4/Documents/blog/data/brms_models/mmm_fit2.Rdata\")\n\n\n\nModel Evaluation\nNow we can evaluate the models. Here, I evaluate a model with random effects for month and week, and a smoothing spline for the trend component against a fixed effects model with a linear trend, and a “null” model with just an intercept. The model with a smooth trend spline appears to beat out the linear trend, which makes sense given the non-linear bump in sales observed in the raw data.\n\n\nCode\n# evaluate models using leave-out-out criterion\n# looks like smoothing spline is slightly better\nloo_eval &lt;- loo(brm_fit_0, brm_fit_1, brm_fit_2)\n\n\nWarning: Found 12 observations with a pareto_k &gt; 0.7 in model 'brm_fit_1'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 15 observations with a pareto_k &gt; 0.7 in model 'brm_fit_2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nCode\nloo_eval$diffs\n\n\n          elpd_diff se_diff\nbrm_fit_2    0.0       0.0 \nbrm_fit_1  -16.2       8.3 \nbrm_fit_0 -291.7      18.9 \n\n\nWe can (and should) also check out the predictions from the model using a posterior predictive check. In Bayesian terms what this means is we take a sample of draws from our fitted model and compare them against the observed data. If our model is capturing the process well, the predicted values should generally follow the observed process. Below we see that our model does a fairly decent job.\n\n\nCode\npp_check(brm_fit_2)\n\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\nPosterior predictive check, smoothing spline model\n\n\n\n\nFinally, we can look at some of the model coefficients\n\nsummary(brm_fit_2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log(sales) ~ demand + supply + price + s(trend) + spend_sms + spend_news + spend_radio + spend_tv + spend_net + (1 | month) + (1 | week) \n   Data: X (Number of observations: 197) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(strend_1)     1.53      0.73     0.54     3.35 1.00     1739     2509\n\nMultilevel Hyperparameters:\n~month (Number of levels: 12) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.86      0.22     0.55     1.38 1.00     1413     2226\n\n~week (Number of levels: 53) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.79      0.09     0.64     1.00 1.00      721     1142\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      13.88      0.48    12.94    14.83 1.00     2447     3039\ndemand          0.01      0.03    -0.06     0.07 1.00     4078     3029\nsupply          0.19      0.04     0.12     0.26 1.00     4860     3017\nprice           2.88      0.99     0.83     4.86 1.00     4064     2908\nspend_sms       0.12      0.17    -0.22     0.45 1.00     5927     3202\nspend_news     -0.01      0.51    -1.01     0.99 1.00     6425     2879\nspend_radio     0.18      0.14    -0.10     0.46 1.00     4534     2610\nspend_tv       -0.02      0.02    -0.05     0.01 1.00     5170     2835\nspend_net       0.00      0.00    -0.00     0.01 1.00     4820     3243\nstrend_1       -2.42      1.55    -5.84     0.34 1.00     2711     2636\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.15      0.01     0.13     0.17 1.00     1381     2410\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nModel Predictions\nAnd here’s the estimated lift for Radio by week. While there is definitely some lift, it is pretty tiny here\n\n\nCode\n# get predictions \nX_no_radio &lt;- X %&gt;% mutate(spend_radio= 0)\n\npred1 &lt;- predict(brm_fit_2)\npred1_no_radio &lt;- predict(brm_fit_2, newdata = X_no_radio)\n\npred_dataframe &lt;-\n  cbind.data.frame(week = 1:197,\n                   obs = pred1[, 1],\n                   pred = pred1_no_radio[, 1]) %&gt;%\n  pivot_longer(-week) %&gt;%\n  group_by(week) %&gt;%\n  mutate(diff = value - lead(value, 1))\n\n# predicted (all) vs predicted (no radio)\nggplot(pred_dataframe) +\n  geom_line(aes(x = week, y = value, color = name)) +\n  theme_bw() +\n  scale_color_manual(values = c(\"#0077BB\", \"#EE7733\")) +\n  labs(y = \"(log) Sales\", x = \"Week\") +\n  theme(legend.position = 'none')\n# predicted lift from radio\npred_dataframe %&gt;%\n  na.omit() %&gt;%\n  ggplot() +\n  geom_line(aes(x = week, y = diff)) +\n  theme_bw() +\n  labs(y = \"(log) Sales\", x = \"Week\")\n\n\n\n\n\n\n\n\n\n\n\n\nIf we average the estimated mean lift across the entire time frame we get an additional value of about 2%.\n\nmean((pred1[,1] - pred1_no_radio[,1])/pred1[,1])\n\n[1] 0.02156586\n\n\nAt this point there is a lot of additional work that can be done. Most applied uses of MMM apply some optimization algorithms to determine the best ad spend mix given a fixed budget. The data I have here isn’t really good enough to delve any deeper into - but its important to note that fitting the model is really only the beginning."
  },
  {
    "objectID": "posts/mmm-bayesian/mmm.html#in-closing-my-take",
    "href": "posts/mmm-bayesian/mmm.html#in-closing-my-take",
    "title": "An Outsider’s Perspective On Media Mix Modelling",
    "section": "In Closing: My Take",
    "text": "In Closing: My Take\nMy biggest problem with Mixed Media Modelling is that it seems like it is easy to implement badly, but much harder to do well. Not only do you have to appropriately model an adstock function for your advertising venues, you also have very high correlation between your variables. This, in turn, makes the choice of model specification even more important because the coefficients with be highly sensitive. Personally, a true experiment or even quasi-experiment would be preferable to this - although I’m all too aware that this is often impossible. Like everything there is no magic bullet, and choosing one approach over another will always introduce trade offs."
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html",
    "title": "The Power of Ensembles",
    "section": "",
    "text": "It’s no secret that ensemble methods are extremely powerful tools in statistical inference, data science, and machine learning. It’s long been known that many “imperfect” models combined together can often perform better than any single model.\nFor example, in the M5 forecasting competition almost all of the top performers used some element of model averaging or ensembling. Indeed, the very foundations of some of the most commonly used tools in machine learning, like random forests and boosting, work by averging across many highly biased models to create a single more powerful model. The success of this method is relies on the fact that averaging across many high-variance models generally results in a single, lower-variance model. This is most evident in the idea of the “wisdom of the crowd”, where large groups of individuals are often better at predicting something compared to a single expert. However, one area which hasn’t received much attention is outlier detection. When we say “outliers” we’re generally referring to observations that are exceptionally unusual compared to the rest of the sample. A rather consise definition by Hawkins (1980) states:\nThis rather broad definition fits well with the general application of outlier detection. It can be used for identifying fraud in insurance or healthcare datasets, intrusion detection for computer networks, or flagging anomalies in time-series data - among many others. The specific challenge I want to address in this mini-blog is an ensembling approach for unsupervised outlier detection. This is doubly interesting because unsupervised learning presents many more issues compared to supervised learning. Below, I’ll contrast some of these differences and then describe an interesting ensemble approach.\nCode\n# library and data imports\nimport random\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom ensamble_funcs import ALSOe\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import StandardScaler\n\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\n\n# set plotting theme\ncparams = {\n            \"axes.spines.left\": False,\n            \"axes.spines.right\": False,\n            \"axes.spines.top\": False,\n            \"axes.spines.bottom\": False,\n            \"grid.linestyle\": \"--\"\n            }\n\nsns.set_style(\"whitegrid\", rc = cparams)\nsns.set_palette([\"#0077BB\",\"#EE7733\"])\n\n# define some helper functions\ndef eval_preds(yobs, ypred):\n    \"\"\" Print AUC and average precision\n    \"\"\"\n\n    auc = roc_auc_score(yobs, ypred)\n    pre = average_precision_score(yobs, ypred)\n\n    print(f'Roc:{np.round(auc,3)}') \n    print(f'Prn:{np.round(pre,3)}')\n\n# set seed\nrandom.seed(46098)\n\n# load data\ndata = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/6_cardio.npz\")\nX, y = data['X'], data['y']\n\n# Scale input features to mean 0, sd 1\nX = StandardScaler().fit_transform(X)\n\n# Train-test split\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y)"
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#outlier-detection-with-supervision",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#outlier-detection-with-supervision",
    "title": "The Power of Ensembles",
    "section": "Outlier Detection with Supervision",
    "text": "Outlier Detection with Supervision\nTo start, let’s look at an example using the cardio dataset sourced from the pyod benchmark set. In this case we have 1831 observations with 21 variables, of which about 9.6% of them are considered anomalous. These are conviently labeled for us, where a value of 1 indicates an anomalous reading. If we fit a simple random forest classifier we see that it is trivial to get a very high AUC on the test data (let’s also not get ahead of ourselves here, as this is a toy dataset with a target that is quite easy to predict). Below we see an example of the fairly strong separation between the inliers and outliers. Our random forest works well in this case - giving us a test AUC of .99 and an average precision of .98. While this is an overly simple example, it does expose how easy some models can be (in many cases) when there is a definite target variable.\n\n\nCode\n# fit a random forest classifier\nrf = RandomForestClassifier()\nrf.fit(Xtrain, ytrain)\n\n# extract the predictions, calculate AUC\nrf_preds = rf.predict_proba(Xtest)[:,1]\n\neval_preds(ytest, rf_preds)\n\n\nRoc:0.997\nPrn:0.977\n\n\n\n\nCode\nsns.scatterplot(x = X[:,7], y = X[:,18], hue = y)\n(\n    plt.xlabel(\"Feature 7\"),\n    plt.ylabel(\"Feature 18\"),\n    plt.title(\"Cardio Scatterplot, Inliers and Outliers\")\n)\nplt.show()\n\n\n\n\n\nScatterplot of inliers (0) and outliers (1), displaying strong separation"
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#unsupervised-outlier-detection",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#unsupervised-outlier-detection",
    "title": "The Power of Ensembles",
    "section": "Unsupervised Outlier Detection",
    "text": "Unsupervised Outlier Detection\nLet’s talk about unsupervised outlier detection. Unlike the situation above in an unsupervised setting we don’t have the convenience of a set of labels identifying whether a given observation is anomalous or not. And because we’re lacking this ground truth, it makes things a lot more complicated for choosing both our model(s) and the parameters for those model(s). Let’s talk about why.\n\nHow do we select a best model?\nIn classic supervised learning we can choose a metric to optimize (say, root-mean squared error or log-loss), then fit a model which attempts to minimize that metric. In the simplest case, think about ordinary least squares. In that case we have a simple target of minimizing the sum of squared errors. We can validate the fit of the model by looking at evalution metrics (RMSE, R-Squared, standardized residuals).However, when we lack a way to identify if a given observation is anomalous or not we don’t have any meaningful way to know if one model is doing better than another.\nYou might also be thinking about optimizing a specific parameter in a model (like the number of nearest neighbors \\(K\\) in a K-nearest neighbors model) using some criterion that doesn’t rely on a target variable (like the ‘elbow’ method). However, optimizing this parameter doesn’t guarantee that the model itself is useful. Simply put, we’re often left groping around in the dark trying to decide what the optimal model or set of parameters is.\nLet’s consider this example: Say we’re looking at the same cardio dataset from above and trying to decide what unsupervised outlier detector we want to use. Maybe we’re deciding between a distance-based one like exact K-nearest neighbors (KNN) or a density-based one like local outlier factor (LOF). Let’s also say we’re agnostic to parameter choices, so we stick with the default ones provided by pyod.\n\n# initalize and fit using default params\nknn = KNN()\nlof = LOF()\n\nknn.fit(X)\nlof.fit(X)\n\n# extract the predictions, calculate AUC\nknn_pred = knn.decision_function(X)\n\neval_preds(y, knn_pred)\n\nRoc:0.686\nPrn:0.286\n\n\n\n\nCode\n# extract the predictions, calculate AUC\nlof_pred = lof.decision_function(X)\n\neval_preds(y, lof_pred)\n\n\nRoc:0.546\nPrn:0.154\n\n\nHere we see that the KNN model performs better than the LOF model - however we didn’t adjust any of the \\(K\\) parameters for either model. Because, in practice, we can’t see this, we don’t know a-priori which model or set of parameters will work best in a given case. This is in stark contrast to our first attempt when we could simply focus on decreasing out-of-sample bias."
  },
  {
    "objectID": "posts/outlier-ensemble/alsoe_ensembling.html#an-unsupervised-ensambling-approach",
    "href": "posts/outlier-ensemble/alsoe_ensembling.html#an-unsupervised-ensambling-approach",
    "title": "The Power of Ensembles",
    "section": "An Unsupervised Ensambling Approach",
    "text": "An Unsupervised Ensambling Approach\nSo, because we can’t easily decrease bias (due to the lack of ground truth values) what are our options? Well, as we saw above, there is a large source of variance implicit in these models. This variance can come from multiple sources:\n\nChoice of model. There is considerable variance in how different models perform under different kinds of anomolies. For example, absent some evaluation metric how do you meaningfully choose between KNN, LOF, or one of many other methods. The pyod benchmark page shows that many models perform quite differently under different types of dimensionality and outlier proportion.\nChoice of parameters. Almost all anomaly detection models have added uncertaintly based on the choice of parameters. For example, in K-nearest neighbors we need to specify the parameter \\(K\\). One-class support vector machines (SVM) are notoriously difficult to tune in part due to the number of parameters to choose (choice of kernel, polynomial degree, ect…).\n\nTherefore, in an unsupervised model our best option is to try to reduce the variance implicit in both the sources above. Rather than staking our whole model on a single choice of model, or a single set of parameters, we can ensemble over a wide number of choices to avoid the risk of choosing a catastrophically bad combination. Since we don’t have access to ground truth, this ends up being the safest option (and as we will see, generally produces good results).\n\nALSO: A regression-based approach\nFor this specific post I’m going to focus on an unsupervised ensemble algothrim proposed by Paulheim & Meusel (2015) and further discussed in Aggarwal & Sathe (2017). The authors dub this method “attribute-wise learning for scoring outliers” or ALSO. The approach we are going to use extends the logic of the ALSO model to an ensemble-based approach (hence ALSO-E).\nLet’s talk a bit about the logic here. The general idea of the algothrim is that we iteratively choose a target feature \\(X_j\\) from the full set of features \\(X\\). The chosen \\(j\\) value is used as the target and the remaining \\(X - j\\) features are used to predict \\(j\\). We repeat this for all features in \\(X\\), collecting the standardized model residuals at each step. We then average the residuals across all the models and use them to identify “anomalous” observations. In this case, more anomalous observations will likely be ones whose residuals are substantially larger than the rest of the sample. The beauty of this method is that for the modelling portion we can choose any base model for prediction (e.g. linear regression, random forests, etc…).\nTo avoid models that are very overfit, or have virtually no predictive ability, we define weights for each model using cross-validated RMSE. The goal here is to downweight models that have low predictive ability so they have a proportionally lower effect on the final outlier score. This is defined as\n\\[w_k = 1 - min(1, RMSE(M_k))\\]\nwhich simply means that models that perform worse than predicting the mean (which would give us an RMSE of 1) are weighted to 0, while a theoretically “perfect” model would be weighted 1. This gives us the added benefit of downweighting features that have little or no predictive value, which helps in cases when we might have one or more irrelevant variables.\n\n\nAdding an E to ALSO\nThe base algothrim above fits \\(X\\) models using all \\(n\\) observations in the data. However, we can extend this model to an ensambling method by applying some useful statistical tools - namely variable subsambling. The idea proposed by Aggarwal & Sathe (2017) is to define a number of iterations (say, 100), and for each iteration randomly subsamble the data from between \\(min(1, \\frac{50}{n})\\) and \\(min(1, \\frac{1000}{n})\\). This means that each model is fit on a minimum of 50 observations and up to 1000 (or, \\(n\\) if \\(n\\) is less than 1000). In addition, we randomly choose a feature \\(j\\) to be predicted. Combined, this ensambling approach makes a more efficient use of the available data and results in a more diverse set of models.\nTo my knowledge, there is no “official” implementation of the ALSO-E algothrim, and it is not present in any large libraries (e.g. pyod or sklearn). However the method is generic enough that it is not difficult to code from scrach. Using the notes available I implemented the method myself using a random forest regressor as the base detector. The code below defines a class with a fit() and predict() function. The fit() function handles all the subsampling and fits each sample on a very shallow random forest regressor. The predict() function does the work of getting the residuals and re-weighting them according to their CV-error. While my implementation is certainly not “feature complete”, it’s good enough to try out:\n\n\nCode\n# code to fit an ALSOe anomaly detector\n\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nclass ALSOe():\n    \"\"\" Initializes a regression-based outlier detector\n    \"\"\"\n\n    def __init__(self, N = 100) -&gt; None:\n\n        self.param_list = []\n        self.model_list = []\n        self.anom_list = []\n        self.wt = []\n    \n        self.N = N\n        self.std_scaler = StandardScaler()\n\n    def fit(self, data):\n        \"\"\" Fit an ensemble detector\n        \"\"\"\n\n        # standardize data\n        self.std_scaler = self.std_scaler.fit(X = data)\n        data = self.std_scaler.transform(data)\n\n        # fit N models\n        for i in range(0, self.N):\n\n            # define sample space\n            n = data.shape[0]\n            p = data.shape[1]\n            s = [min([n, 50]), min(n,1000)]\n\n            # draw s random samples from dataframe X\n            s1 = np.random.randint(low = s[0], high = s[1])\n            p1 = np.random.randint(low = 0, high = p)\n            ind = np.random.choice(n, size = s1, replace = False)\n\n            # define random y and X \n            df = data[ind]\n            y = df[:,p1]\n            X = np.delete(df, p1, axis=1)\n\n            # initalize RF regressor\n            rf = RandomForestRegressor(n_estimators=10)\n\n            # fit & predict\n            rf.fit(X, y)\n\n            # add fitted models & y param to list\n            self.model_list.append(rf)\n            self.param_list.append(p1)\n\n    def predict(self, newdata):\n\n        \"\"\" Get anomaly scores from fitted models\n        \"\"\"\n\n        # standardize data\n        newdata = self.std_scaler.transform(newdata)    \n\n        for i,j in zip(self.model_list, self.param_list):\n\n            # define X, y\n            y = newdata[:,j]\n            X = np.delete(newdata, j, axis=1)\n\n            # get predictions on model i, dropping feature j\n            yhat = i.predict(X)\n\n            # rmse\n            resid = np.sqrt(np.square(y - yhat))\n            resid = (resid - np.mean(resid)) / np.std(resid) \n\n            # compute and apply weights\n            cve = cross_val_score(i, X, y, cv=3, scoring='neg_root_mean_squared_error')\n            w = 1 - min(1, np.mean(cve)*-1)\n\n            resid = resid*w\n\n            # add weights and preds to lists\n            self.wt.append(w)\n            self.anom_list.append(resid)\n\n        # export results as min-max scaled\n        anom_score = np.array(self.anom_list).T\n        anom_score = np.mean(anom_score, axis = 1)\n\n        # rescale and export\n        anom_score = StandardScaler().fit_transform(anom_score.reshape(-1,1))\n        anom_score = anom_score.flatten()\n\n        return anom_score\n\n\n\nFitting the model\nWith all this in mind, fitting the actual model is quite simple. As stated above, the ALSO-E approach is largely parameter free, which means there isn’t much for the user to worry about. Here we’ll just initialize a model with 100 iterations, fit all of the random forest regressors, then get the weighted standardized residuals. This whole process can be condensed into basically 3 lines of code:\n\n# Fit an ALSOe regression ensemble\n# using 100 random forests\nad = ALSOe(N = 100)\nad.fit(X)\n\n# extract predictions from the models and combine\n# the standardized outlier scores\nad_preds = ad.predict(X)\n\n\n\nEvaluating the predictions\nNow that we have the predictions, we can look at the distribution of outlier scores. Below we see a histogram of the ensembled scores which, to recall, are rescaled to mean 0 and standard deviation 1. Therefore, the most anomalous observations will have large positive values. Consistent with what we would expect to see, there is a long tail of large residuals which correspond to the outliers, while the bulk of the data corresponds to a mostly “normal” set of values centered around zero.\n\n\nCode\nsns.histplot(x = ad_preds)\n(\n    plt.xlabel(\"Anomaly Score\"),\n    plt.ylabel(\"Observations\"),\n    plt.title(\"ALSO-E Anomaly Scores\")\n)\nplt.show()\n\n\n\n\n\nHistogram of anomaly scores. The characteristic long tail highlights potential anomalous observations\n\n\n\n\nWe can evaluate the performance of our method by bootstrapping the original dataset 10 times, then running our model on each of the boostrap replicates. This is because there is bound to be some degree of randomness based on the chosen samples and variables for each iteration. Averaging over the bootstrap replicates helps give us some idea of how this model might perform “in the wild” so to speak. Below I define a little helper function to resample the dataset, fit the model, and then extract the relevant evaluation metrics. We then loop through the function and put the fit statistics in a set of lists. For evaluation we’ll look at the averages for each set.\n\n# Bootstrap sample from base dataset and evaluate metrics\ndef boot_eval(df):\n    X, y = resample(df['X'], df['y'])\n\n    ad = ALSOe(N = 100)\n    ad.fit(X)\n    ad_preds = ad.predict(X)\n\n    auc = roc_auc_score(y, ad_preds)\n    pre = average_precision_score(y, ad_preds)\n\n    return [auc, pre]\n\n# run models\nroc_list = []\nprn_list = []\n\nfor i in range(10):\n    roc, prn = boot_eval(data)\n\n    roc_list.append(roc)\n    prn_list.append(prn)\n\nShown below, we see we have a decent AUC and average precision score of about 0.71 and 0.26 respectively. While this is substantially lower than the supervised model, it is still better than the base KNN and LOF models above. The ensambling process also makes it easy because we don’t have to specify any parameters other than the number of iterations to run. In testing, the default 100 works well as a starting point, and there aren’t huge performance gains by increasing it substantially.\n\n\nCode\nprint(f'Avg. ROC: {np.round(np.mean(roc_list),3) }\\nAvg. Prn: {np.round(np.mean(prn_list),3)}')\n\n\nAvg. ROC: 0.693\nAvg. Prn: 0.261\n\n\n\n\n\nComparing performance across datasets\nWe can also evaluate its performance on a variety of other datasets. Here I randomly chose another four datasets from the pyod benchmarks page and compared its performance over 10 bootstrap resamplings to the other benchmarked methods in the pyod ecosystem. Looking at the results we see that we get median RocAUC scores of between .7 to .85 and average precision scores between .2 to .85. For an unsupervised model this isn’t too bad, and largely falls within the range of other detectors.\nWe should note that while its performance is never the best, it is also never the worst either. For example: the .707 we achieved on the cardio dataset is lower than some of the best methods (in this case, PCA and Cluster-Based LOF). However, we avoid extremely bad results like with Angle-Based Outlier Detection or LOF. This underscores our goals with the ensemble model: we prefer a more conservative model that tends to perform consistently across many types of anomalies. We also avoid issues related to choosing optimal parameters but simply ensambling over many detectors. In an unsupervised case this decrease in variance is especially desirable.\n\n\nCode\n# load additional datasets\nd1 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/14_glass.npz\")\nd2 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/18_Ionosphere.npz\")\nd3 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/20_letter.npz\")\nd4 = np.load(\"C:/Users/gioc4/Documents/blog/data/Classical/21_Lymphography.npz\")\n\ndlist = [d1,d2,d3,d4]\nmname = ['Cardio', 'Glass','Ionosphere','Letter','Lympho']\n\nroc_list_m = []\nprn_list_m = []\n\n# run models\nfor j in dlist:\n \n    for i in range(10):\n        roc, prn = boot_eval(j)\n\n        roc_list_m.append(roc)\n        prn_list_m.append(prn)\n\n\n# Plot evaluation metrics across datasets\nevaldf = pd.DataFrame({'RocAUC' : roc_list +roc_list_m, \n                       'Precision' : prn_list + prn_list_m,\n                       'Dataset': sorted([x for x in mname*10])})\\\n            .melt(id_vars = 'Dataset', var_name = 'Metric', value_name = 'Score')\n\n# facet plot across datasets\ng = sns.FacetGrid(evaldf, col = 'Metric', sharey = False, col_wrap=1, aspect = 2)\n(\n    g.map(sns.boxplot, 'Dataset','Score', order = mname),\n    g.fig.subplots_adjust(top=0.9),\n    g.fig.suptitle('ALSO-E Model Evaluation', fontsize=16)\n)\nplt.show()\n\n\n\n\n\nensemble models often are often not as good as the best method, but can achieve consistently decent performance."
  },
  {
    "objectID": "posts/pca-timeseries/pca_ts.html",
    "href": "posts/pca-timeseries/pca_ts.html",
    "title": "Anomaly Detection for Time Series",
    "section": "",
    "text": "Identifying outliers in time series is one of the more common applications for unsupervised anomaly detection. Some of the most common examples come from network intrusion detection, mechanical processes, and other types of high-volume streaming data.\nOf course, there are just as many proposed ways of identifying outliers from the simple (basic Z-scores) to the complex (convolutional neural networks). There are also some approaches that rely on more conventional tabular approaches. Rob Hyndman proposed a few approaches here and here showing how many high-volume time series can be compressed into a tabular dataset. The general idea is that you can decompose many time series into tabular observations by creating a large variety of features describing each series.\n\n\nThe data we’ll use is on hourly power usage for a large power company (American Electric Power). From this dataset we can perform some basic aggregation (to ensure that all timestamped values are on the same day-hour), then separate each set of 24 hours into their individual days. The goal here is to make it easier to look at hours within each day. The code below does a bit of this processing. Of course, working with dates is still always a pain, despite the improvements in R libraries.\n\n\nCode\n# Read data, convert to zoo\nelec &lt;- read_csv(\"AEP_hourly.csv\") %&gt;%\n  group_by(Datetime) %&gt;%\n  summarise(AEP_MW = sum(AEP_MW)) %&gt;%\n  filter(year(Datetime) %in% 2017)\n\nelec_ts &lt;- zoo(x = elec$AEP_MW, order.by = elec$Datetime, frequency = 24)\n\n# Split the hourly time series into daily time series\ndaily_ts_list &lt;- split(elec_ts, as.Date(index(elec_ts)))\n\n# Extract the first 24 observations of each daily time series\n# dropping days with missing values\ndaily_24_ts_list &lt;- lapply(daily_ts_list, function(x) {\n  if (length(x) &gt;= 24) {\n    return(x[1:24])\n  } else {\n    return(NA)\n  }\n})\n\n# Convert from list to dataframe\ndaily_24_ts_list &lt;- purrr::discard(daily_24_ts_list, ~any(is.na(.)))\n\n\nAfter converting the list of values to a data frame, we can proceed with the featurization. As we said before, we can use the tsfeatures library to decompose each day’s hourly values into a single observation. We can see this creates a data frame with 17 features, which correspond to various measures, including: autocorrelation, seasonality, entropy and other ad-hoc measures of time series behavior.\n\n# Convert from list to dataframe, extract TS features\ndaily_24_ts_list &lt;- purrr::discard(daily_24_ts_list, ~ any(is.na(.)))\n\n# create time series features using `tsfeatures`\ndf &lt;- daily_24_ts_list %&gt;%\n  tsfeatures(\n    features = c(\n      \"acf_features\",\n      \"stl_features\",\n      \"entropy\",\n      \"lumpiness\",\n      \"stability\",\n      \"max_level_shift\"\n    )\n  ) %&gt;%\n  select(-nperiods,-seasonal_period)\n\n\n\nCode\nglimpse(df, width = 65)\n\n\nRows: 364\nColumns: 13\n$ x_acf1      &lt;dbl&gt; 0.8139510, 0.9318718, 0.9123398, 0.9173901,…\n$ x_acf10     &lt;dbl&gt; 1.771656, 2.081091, 1.868137, 1.889912, 1.7…\n$ diff1_acf1  &lt;dbl&gt; 0.6284980, 0.6181949, 0.6534759, 0.6194560,…\n$ diff1_acf10 &lt;dbl&gt; 1.4141518, 0.7771648, 0.6910469, 1.0491669,…\n$ diff2_acf1  &lt;dbl&gt; 0.3426413, 0.2082615, 0.3164917, 0.2786098,…\n$ diff2_acf10 &lt;dbl&gt; 0.5009215, 0.2550913, 0.4214296, 0.2577332,…\n$ trend       &lt;dbl&gt; 0.7256036, 0.9374949, 0.9377550, 0.9478148,…\n$ spike       &lt;dbl&gt; 1.946527e-04, 4.715699e-06, 6.706533e-06, 4…\n$ linearity   &lt;dbl&gt; 1.8002798, 3.6310069, 3.3908752, 4.1128389,…\n$ curvature   &lt;dbl&gt; 0.4988011, -1.2265007, -2.0788821, -0.66100…\n$ e_acf1      &lt;dbl&gt; 0.6719203, 0.6507832, 0.6513875, 0.6636410,…\n$ e_acf10     &lt;dbl&gt; 1.4588480, 1.0862781, 0.9523931, 1.1326925,…\n$ entropy     &lt;dbl&gt; 0.3585884, 0.5225069, 0.4962494, 0.3090901,…\n\n\n\n\n\nAfter doing this, we can proceed as a normal tabular data problem. The PCA anomaly detector that was detailed in an earlier post is an easy plug in here and is a natural fit for the problem. We have a lot of highly correlated measures that likely share a large amount of variance across a few dimensions. We can then weight the lower-variance dimensions higher to identify anomalous series. We’ll use the \\(\\chi^2\\) distribution to derive a p-value, which we can then threshold for flagging outliers.\n\n# Perform anomaly detection\nanom &lt;- adPCA(df)\np = sapply(anom, pchisq, df=ncol(df), ncp = mean(anom), lower.tail=F)\n\n\n\n\nWe can see which series were flagged by the model by highlighting the series which were flagged at the \\(p &lt; .01\\)\n\n\nCode\n# flag observations at p &lt; 0.01\nelec_plot &lt;- elec %&gt;%\n  mutate(date = as.Date(format(Datetime, \"%Y-%m-%d\")),\n         hour = hour(Datetime)) %&gt;%\n  left_join(scored_data) %&gt;%\n  mutate(flag = ifelse(p &lt; 0.01,1,0))\n\nggplot(data = elec_plot, aes(x = Datetime, y = AEP_MW, group = date)) +\n  geom_line(color = '#004488', alpha = .125) +\n  geom_line(data = elec_plot[elec_plot$flag == 1,], color = '#BB5566') +\n  labs(x = \"Date\", y = \"Megawatt Hours\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nAnamolous days are highlighted in red. Note the unusually high spike in late 2017.\n\n\n\n\nWe can also see what these anomalous series look like compared to the other series on a hourly basis. This plot clearly shows one series with a unusual early-morning spike, and several series with flatter trajectories compared to more normally expected seasonality - in particular, they are days with low power consumption in the afternoon when consumption is usually at its highest.\n\n\nCode\nggplot(data = elec_plot, aes(x = hour, y = AEP_MW, group = date)) +\n  geom_line(color = '#004488', alpha = .075) +\n  geom_line(data = elec_plot[elec_plot$flag == 1,], color = '#BB5566', size = .7) +\n  labs(x = \"Hour of Day\", y = \"Megawatt Hours\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nAnomalous days often have flatter curves and dip during high-load hours of the day.\n\n\n\n\nArguably this isn’t an ideal approach because each sub-series is only comprised of 24 observations. That means reliably identifying seasonality via the stl_features is questionable. In addition, this approach loses some information that comes from day-to-day correlations. It would probably be worthwhile testing this approach against something like STL decomposition."
  },
  {
    "objectID": "posts/pca-timeseries/pca_ts.html#an-unsupervised-approach-to-time-series-anomalies",
    "href": "posts/pca-timeseries/pca_ts.html#an-unsupervised-approach-to-time-series-anomalies",
    "title": "Anomaly Detection for Time Series",
    "section": "",
    "text": "Identifying outliers in time series is one of the more common applications for unsupervised anomaly detection. Some of the most common examples come from network intrusion detection, mechanical processes, and other types of high-volume streaming data.\nOf course, there are just as many proposed ways of identifying outliers from the simple (basic Z-scores) to the complex (convolutional neural networks). There are also some approaches that rely on more conventional tabular approaches. Rob Hyndman proposed a few approaches here and here showing how many high-volume time series can be compressed into a tabular dataset. The general idea is that you can decompose many time series into tabular observations by creating a large variety of features describing each series.\n\n\nThe data we’ll use is on hourly power usage for a large power company (American Electric Power). From this dataset we can perform some basic aggregation (to ensure that all timestamped values are on the same day-hour), then separate each set of 24 hours into their individual days. The goal here is to make it easier to look at hours within each day. The code below does a bit of this processing. Of course, working with dates is still always a pain, despite the improvements in R libraries.\n\n\nCode\n# Read data, convert to zoo\nelec &lt;- read_csv(\"AEP_hourly.csv\") %&gt;%\n  group_by(Datetime) %&gt;%\n  summarise(AEP_MW = sum(AEP_MW)) %&gt;%\n  filter(year(Datetime) %in% 2017)\n\nelec_ts &lt;- zoo(x = elec$AEP_MW, order.by = elec$Datetime, frequency = 24)\n\n# Split the hourly time series into daily time series\ndaily_ts_list &lt;- split(elec_ts, as.Date(index(elec_ts)))\n\n# Extract the first 24 observations of each daily time series\n# dropping days with missing values\ndaily_24_ts_list &lt;- lapply(daily_ts_list, function(x) {\n  if (length(x) &gt;= 24) {\n    return(x[1:24])\n  } else {\n    return(NA)\n  }\n})\n\n# Convert from list to dataframe\ndaily_24_ts_list &lt;- purrr::discard(daily_24_ts_list, ~any(is.na(.)))\n\n\nAfter converting the list of values to a data frame, we can proceed with the featurization. As we said before, we can use the tsfeatures library to decompose each day’s hourly values into a single observation. We can see this creates a data frame with 17 features, which correspond to various measures, including: autocorrelation, seasonality, entropy and other ad-hoc measures of time series behavior.\n\n# Convert from list to dataframe, extract TS features\ndaily_24_ts_list &lt;- purrr::discard(daily_24_ts_list, ~ any(is.na(.)))\n\n# create time series features using `tsfeatures`\ndf &lt;- daily_24_ts_list %&gt;%\n  tsfeatures(\n    features = c(\n      \"acf_features\",\n      \"stl_features\",\n      \"entropy\",\n      \"lumpiness\",\n      \"stability\",\n      \"max_level_shift\"\n    )\n  ) %&gt;%\n  select(-nperiods,-seasonal_period)\n\n\n\nCode\nglimpse(df, width = 65)\n\n\nRows: 364\nColumns: 13\n$ x_acf1      &lt;dbl&gt; 0.8139510, 0.9318718, 0.9123398, 0.9173901,…\n$ x_acf10     &lt;dbl&gt; 1.771656, 2.081091, 1.868137, 1.889912, 1.7…\n$ diff1_acf1  &lt;dbl&gt; 0.6284980, 0.6181949, 0.6534759, 0.6194560,…\n$ diff1_acf10 &lt;dbl&gt; 1.4141518, 0.7771648, 0.6910469, 1.0491669,…\n$ diff2_acf1  &lt;dbl&gt; 0.3426413, 0.2082615, 0.3164917, 0.2786098,…\n$ diff2_acf10 &lt;dbl&gt; 0.5009215, 0.2550913, 0.4214296, 0.2577332,…\n$ trend       &lt;dbl&gt; 0.7256036, 0.9374949, 0.9377550, 0.9478148,…\n$ spike       &lt;dbl&gt; 1.946527e-04, 4.715699e-06, 6.706533e-06, 4…\n$ linearity   &lt;dbl&gt; 1.8002798, 3.6310069, 3.3908752, 4.1128389,…\n$ curvature   &lt;dbl&gt; 0.4988011, -1.2265007, -2.0788821, -0.66100…\n$ e_acf1      &lt;dbl&gt; 0.6719203, 0.6507832, 0.6513875, 0.6636410,…\n$ e_acf10     &lt;dbl&gt; 1.4588480, 1.0862781, 0.9523931, 1.1326925,…\n$ entropy     &lt;dbl&gt; 0.3585884, 0.5225069, 0.4962494, 0.3090901,…\n\n\n\n\n\nAfter doing this, we can proceed as a normal tabular data problem. The PCA anomaly detector that was detailed in an earlier post is an easy plug in here and is a natural fit for the problem. We have a lot of highly correlated measures that likely share a large amount of variance across a few dimensions. We can then weight the lower-variance dimensions higher to identify anomalous series. We’ll use the \\(\\chi^2\\) distribution to derive a p-value, which we can then threshold for flagging outliers.\n\n# Perform anomaly detection\nanom &lt;- adPCA(df)\np = sapply(anom, pchisq, df=ncol(df), ncp = mean(anom), lower.tail=F)\n\n\n\n\nWe can see which series were flagged by the model by highlighting the series which were flagged at the \\(p &lt; .01\\)\n\n\nCode\n# flag observations at p &lt; 0.01\nelec_plot &lt;- elec %&gt;%\n  mutate(date = as.Date(format(Datetime, \"%Y-%m-%d\")),\n         hour = hour(Datetime)) %&gt;%\n  left_join(scored_data) %&gt;%\n  mutate(flag = ifelse(p &lt; 0.01,1,0))\n\nggplot(data = elec_plot, aes(x = Datetime, y = AEP_MW, group = date)) +\n  geom_line(color = '#004488', alpha = .125) +\n  geom_line(data = elec_plot[elec_plot$flag == 1,], color = '#BB5566') +\n  labs(x = \"Date\", y = \"Megawatt Hours\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nAnamolous days are highlighted in red. Note the unusually high spike in late 2017.\n\n\n\n\nWe can also see what these anomalous series look like compared to the other series on a hourly basis. This plot clearly shows one series with a unusual early-morning spike, and several series with flatter trajectories compared to more normally expected seasonality - in particular, they are days with low power consumption in the afternoon when consumption is usually at its highest.\n\n\nCode\nggplot(data = elec_plot, aes(x = hour, y = AEP_MW, group = date)) +\n  geom_line(color = '#004488', alpha = .075) +\n  geom_line(data = elec_plot[elec_plot$flag == 1,], color = '#BB5566', size = .7) +\n  labs(x = \"Hour of Day\", y = \"Megawatt Hours\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(face = \"bold\"))\n\n\n\n\n\nAnomalous days often have flatter curves and dip during high-load hours of the day.\n\n\n\n\nArguably this isn’t an ideal approach because each sub-series is only comprised of 24 observations. That means reliably identifying seasonality via the stl_features is questionable. In addition, this approach loses some information that comes from day-to-day correlations. It would probably be worthwhile testing this approach against something like STL decomposition."
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html",
    "href": "posts/rtm-spatial/rtm.html",
    "title": "Generating Spatial Risk Features using R",
    "section": "",
    "text": "In criminology there is a considerable research on the role that fixed spatial features in the environment have on crime. These spatial risk factors have criminogenic qualities that make them “attractors” or “generators”(Brantingham and Brantingham 1995). Absent some change, these places typically contribute a disproportionate share of crime that is largely stable over time(Sherman, Gartin, and Buerger 1989). The classic example is a bar or night club. Alcohol plays a large role in a lot of crime, and locations where many people congregate and become intoxicated also have higher incidences of crime. We can use information about the environment to help solve problems or prioritize patrol areas.\nOne challenge in research is obtaining the point locations for these features. Generally when we perform some kind of spatial analysis we have a study area (e.g. a city or other boundary file) and a set of labeled point features corresponding to the locations of interest. However, reliable places to get this information is often hard to come by. Some cities provide open data portals with commercial information, but these are typically limited to larger cities. In my work I’ve had people ask how to get spatial risk factors for their research, often times for something related to the “Risk Terrain Modeling” approach of spatial analysis. I’ve worked on a few projects now where I’ve had to generate these myself, and have had some success using open data sources like Google to help with it."
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#spatial-features",
    "href": "posts/rtm-spatial/rtm.html#spatial-features",
    "title": "Generating Spatial Risk Features using R",
    "section": "",
    "text": "In criminology there is a considerable research on the role that fixed spatial features in the environment have on crime. These spatial risk factors have criminogenic qualities that make them “attractors” or “generators”(Brantingham and Brantingham 1995). Absent some change, these places typically contribute a disproportionate share of crime that is largely stable over time(Sherman, Gartin, and Buerger 1989). The classic example is a bar or night club. Alcohol plays a large role in a lot of crime, and locations where many people congregate and become intoxicated also have higher incidences of crime. We can use information about the environment to help solve problems or prioritize patrol areas.\nOne challenge in research is obtaining the point locations for these features. Generally when we perform some kind of spatial analysis we have a study area (e.g. a city or other boundary file) and a set of labeled point features corresponding to the locations of interest. However, reliable places to get this information is often hard to come by. Some cities provide open data portals with commercial information, but these are typically limited to larger cities. In my work I’ve had people ask how to get spatial risk factors for their research, often times for something related to the “Risk Terrain Modeling” approach of spatial analysis. I’ve worked on a few projects now where I’ve had to generate these myself, and have had some success using open data sources like Google to help with it."
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#querying-google-places",
    "href": "posts/rtm-spatial/rtm.html#querying-google-places",
    "title": "Generating Spatial Risk Features using R",
    "section": "Querying Google Places",
    "text": "Querying Google Places\nGoogle has a lot of paid API services which are quite useful for researchers. In most cases there is a free tier, and for smaller one-off projects this makes their API services attractive for research. Let’s walk through an example of how we might do this. For our example we will use the Swedish city of Malmö (which, incidentially is a very lovely city I’ve been lucky enough to visit). We have a shapefile that looks like this:\n\n\n\n\n\n\n\n\n\nOur goal is to query theGoogle Places API to get the locations of criminogenic spatial risk factors (here, bars and gas stations). One significant limitation with the Google Places API is that there is a limit to the number of locations that will show up for a single query. This means if you ran the query on the entire city, it would only return up to 20 locations. However, we can bypass this by running multiple queries on smaller spatial regions. Other bloggers have provided similar advice as well (see here and here).\nTo do the actual interfacing with the Google Places API we will use the very handy googleway package.\n\nSplitting up into a grid\n\n\nCode\n# GOOGLE PLACES API CODE\n# ================================================= #\n# Giovanni Circo\n# gmcirco42@gmail.com\n#\n# Code to query google place api\n# Divides a boundry shapefile into grid cells\n# of radius r, then queries the api for each cell\n#\n# NOTE:\n# Only requires a free version of the API. Doesn't\n# incur any costs.\n# ================================================= #\n\nlibrary(googleway)\nlibrary(sf)\nlibrary(tidyverse)\n\n# API Key\n# instructions here:\n# https://developers.google.com/maps/documentation/places/web-service/get-api-key\nmykey &lt;- \"[INSERT GOOGLE PLACES API KEY HERE]\"\n\n# Location shapefile\nboundry &lt;- st_read(\"...\\DeSo_Malmö.shp\")\n\n# specify grid size (meters)\nr &lt;- 1200\n\n# Make a grid\nboundry_grid &lt;- st_make_grid(boundry, cellsize = r)\nboundry_grid &lt;- boundry_grid[boundry]\n\n# Transform to lat\\lon\n# Extract coords\narea_coords &lt;- st_transform(boundry_grid, crs = 4326) %&gt;%\n  st_centroid() %&gt;%\n  st_coordinates() %&gt;%\n  data.frame() %&gt;%\n  select(lat = Y, lon = X)\n\n\nWe can divide the city into a series of grids, then iterate through each grid cell and query within it. This way we are more likely to obtain all of the relevant features in that grid cell without hitting the limit. Here, I create 150 1200 square meter grid cells, which gives us something like this:\n\n\n\n\n\n\n\n\n\nIn addition we extract the X-Y coordinates for the grid centroid, which we will use as our location for the API query. This means we hit the API 150 times, once for each grid cell. This is well within the free number that Google allows.\n\n\nQuerying our features\n\n\nCode\n# EXTRACT FEATURES ON GRID\n#----------------------------#\n\n# Supported place type names:\n# https://developers.google.com/maps/documentation/places/web-service/supported_types\n# Features: gas_station, bar, liquor_store, night_club, pharmacy, restaurant\n\n# Specify feature type\n# number of grid cells\nfeature &lt;- \"bar\"\nn &lt;- nrow(area_coords)\n\n# First, set up function to query google places\n# for each grid centroid\n# add to a list\n\narea_list &lt;- list()\nfor(i in 1:n){\n  \n  area_list[[i]] &lt;-\n    google_places(location = unlist(area_coords[i,]),\n                  place_type = feature,\n                  radius = r,\n                  key = mykey)\n}\n\n# Function to convert results from above\n# to a dataframe suitable for rbinding\n# then conversion to an sf object\nconvert_to_dataframe &lt;-\n  function(x) {\n    \na &lt;- x$results\n\nb &lt;- tibble(\n  lat = a$geometry$location$lat,\n  lon = a$geometry$location$lng,\n  name = a$name,\n  types = a$types,\n  address = a$vicinity,\n  place_id = a$place_id\n)\n\nreturn(b)\n  }\n\n# Rbind the results, and then un-nest on feature type\n# This creates a long-form dataframe that you can then filter\n# based on feature type\narea_dataframe &lt;-\n  do.call(rbind, lapply(area_list, convert_to_dataframe)) %&gt;%\n  distinct(place_id, .keep_all = TRUE) %&gt;%\n  unnest(types)\n\n# Get just feature requested\nfeature_out &lt;- area_dataframe %&gt;%\n  filter(types %in% feature) %&gt;%\n  distinct(address, .keep_all = TRUE)\n\n\nWe can use the code above to iterate through each grid cell, hit the API, and then store the results in a list. I include a few helper functions to assist with pulling out the names and coordinates, binding them into a dataframe, and setting them up to export. The key bit of code is below, which is the part that queries the API for each of the grid cells:\n\narea_list &lt;- list()\nfor(i in 1:n){\n  \n  area_list[[i]] &lt;-\n    google_places(location = unlist(area_coords[i,]),\n                  place_type = feature,\n                  radius = r,\n                  key = mykey)\n}"
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#calculating-grid-cell-distances",
    "href": "posts/rtm-spatial/rtm.html#calculating-grid-cell-distances",
    "title": "Generating Spatial Risk Features using R",
    "section": "Calculating Grid Cell Distances",
    "text": "Calculating Grid Cell Distances\n\n\nCode\ncompute_distance &lt;- function(grid, feature){\n  # get nearest point from grid to feature\n  nearest &lt;- st_nearest_feature(grid,feature)\n  nearest_dist &lt;- st_distance(grid, feature[nearest,], by_element = TRUE)\n  \n  return(nearest_dist)\n}\n\n# specify grid size (meters)\nr &lt;- 250\n\n# Make a city grid\ncity &lt;- st_make_grid(boundry, cellsize = r, square = FALSE)\ncity &lt;- city[boundry]\n\n# get distances\nbar_dist &lt;- compute_distance(city, bar)\ngas_dist &lt;- compute_distance(city, gas)\n  \n# create long-form dataframe\ntbl &lt;- tibble(city) %&gt;%\n  mutate(bar = bar_dist,\n         gas_station = gas_dist) %&gt;%\n  pivot_longer(-city, \n               names_to = \"feature\", \n               values_to = \"dist\") %&gt;%\n  mutate(dist = as.numeric(dist)) %&gt;%\n  st_as_sf()\n\n\nNow that we have our city boundary and our spatial risk factors, all we need to do now is compute the distance from each grid cell to its nearest risk factor. In the end, what we will want is a dataframe with grid cell ids, and columns corresponding to distance to the nearest feature. After merging them, we can create a nice map like this - showing the location sand distances of our risk factors.\n\n\n\n\n\n\n\n\n\nYou can then use these features in other kinds of spatial risk models (for a great walk through, see (Wheeler and Steenbeek 2021).The big advantage of this approach is that you have the flexibility to implement any kind of model you want at this point - whether it is a conventional RTM model, or a boosted tree model."
  },
  {
    "objectID": "posts/rtm-spatial/rtm.html#full-code",
    "href": "posts/rtm-spatial/rtm.html#full-code",
    "title": "Generating Spatial Risk Features using R",
    "section": "Full Code",
    "text": "Full Code\n\n\nCode\n# GOOGLE PLACES API CODE\n# ================================================= #\n# Giovanni Circo\n# gmcirco42@gmail.com\n#\n# Code to query google place api\n# Divides a boundry shapefile into grid cells\n# of radius r, then queries the api for each cell\n#\n# NOTE:\n# Only requires a free version of the API. Doesn't\n# incur any costs.\n# ================================================= #\n\nlibrary(googleway)\nlibrary(sf)\nlibrary(tidyverse)\n\n# API Key\n# instructions here:\n# https://developers.google.com/maps/documentation/places/web-service/get-api-key\nmykey &lt;- \"[INSERT GOOGLE PLACES API KEY HERE]\"\n\n# Location shapefile\nboundry &lt;- st_read(\"DeSo_Malmö.shp\")\n\n# specify grid size (meters)\nr &lt;- 1200\n\n# Make a grid\nboundry_grid &lt;- st_make_grid(boundry, cellsize = r)\nboundry_grid &lt;- boundry_grid[boundry]\n\n# Transform to lat\\lon\n# Extract coords\narea_coords &lt;- st_transform(boundry_grid, crs = 4326) %&gt;%\n  st_centroid() %&gt;%\n  st_coordinates() %&gt;%\n  data.frame() %&gt;%\n  select(lat = Y, lon = X)\n\nplot(boundry_grid)\n  \n# EXTRACT FEATURES ON GRID\n#----------------------------#\n\n# Supported place type names:\n# https://developers.google.com/maps/documentation/places/web-service/supported_types\n# Features: gas_station, bar, liquor_store, night_club, pharmacy, restaurant\n\n# Specify feature type\n# number of grid cells\nfeature &lt;- \"bar\"\nn &lt;- nrow(area_coords)\n\n# First, set up function to query google places\n# for each grid centroid\n# add to a list\n\narea_list &lt;- list()\nfor(i in 1:n){\n  \n  area_list[[i]] &lt;-\n    google_places(location = unlist(area_coords[i,]),\n                  place_type = feature,\n                  radius = r,\n                  key = mykey)\n}\n\n# Function to convert results from above\n# to a dataframe suitable for rbinding\n# then conversion to an sf object\nconvert_to_dataframe &lt;-\n  function(x) {\n    \na &lt;- x$results\n\nb &lt;- tibble(\n  lat = a$geometry$location$lat,\n  lon = a$geometry$location$lng,\n  name = a$name,\n  types = a$types,\n  address = a$vicinity,\n  place_id = a$place_id\n)\n\nreturn(b)\n  }\n\n# Rbind the results, and then un-nest on feature type\n# This creates a long-form dataframe that you can then filter\n# based on feature type\narea_dataframe &lt;-\n  do.call(rbind, lapply(area_list, convert_to_dataframe)) %&gt;%\n  distinct(place_id, .keep_all = TRUE) %&gt;%\n  unnest(types)\n\n# Get just feature requested\nfeature_out &lt;- area_dataframe %&gt;%\n  filter(types %in% feature) %&gt;%\n  distinct(address, .keep_all = TRUE)\n\n# Now export as a shapefile\n# re-assign the crs of the boundry shapefile\nfeature_out %&gt;%\n  st_as_sf(coords = c('lon','lat'), crs = 4326) %&gt;%\n  st_transform(crs = st_crs(boundry)) %&gt;%\n  st_write(paste0(\"Desktop\\\\\",feature,\".shp\"))"
  },
  {
    "objectID": "posts/synth-impact/synth_impact.html",
    "href": "posts/synth-impact/synth_impact.html",
    "title": "Synthetic Controls and Small Areas",
    "section": "",
    "text": "Andrew Gelman recently covered a mildly controversial paper in criminology that suggested that a policy of “de-prosecution” by the Philadelphia District Attorney’s office resulted in an increase in homicides. This has sparked a lot of back-and-forth discussion on the appropriateness of the analysis and the kind of synthetic control method used. I’m not here to discuss any of these things, as many other smart people have already debated this to death (plus, given Hogan is reticent to release his data or code we may never really know exactly what he did).\nHowever, what I do want to discuss is something else Gelman wrote about on his blog:\n\nHogan and the others make comparisons, but the comparisons they make are to that weighted average of Detroit, New Orleans, and New York. The trouble is . . . that’s just 3 cities, and homicide rates can vary a lot from city to city. It just doesn’t make sense to throw away the other 96 cities in your data. The implied counterfactual is that if Philadelphia had continued post-2014 with its earlier sentencing policy, that its homicide rates would look like this weighted average of Detroit, New Orleans, and New York…\n\nWhat Gelman is talking about here is the commonly-used ADH approach (short for Abadie, Diamond, and Hainmueller). In this method you typically have one large “treated” area - such as a city or state - that implements some kind of policy. You then use other comparably large or similar areas to construct a synthetic version of your treated unit to estimate the counterfactual. It’s an appealing method because it is relatively simple to calculate, fairly transparent about where the weights come from, and has good overlap with more conventional difference-in-differences methods (non-negative weights with a sum-to-one constraint). So in a way, I don’t necessarily have the same issues that Gelman does, but he brings up a good point. By using only large aggregate units there is an inherent loss of information. In the ADH method we sort of assume that by matching closely on the outcome variable we can average over a lot of the confounders. Although in the ADH method you can also match on other covariates - but in my experience the vast majority of the synthetic control weights are derived solely from the pre-treatment outcomes.\n\n\nGelman further writes:\n\nMy understanding of a synthetic controls analysis went like this. You want to compare Philadelphia to other cities, but there are no other cities that are just like Philadelphia, so you break up the city into neighborhoods and find comparable neighborhoods in other cities . . . and when you’re done you’ve created this composite “city,” using pieces of other cities, that functions as a pseudo-Philadelphia. In creating this composite, you use lots of neighborhood characteristics, not just matching on a single outcome variable. And then you do all of this with other cities in your treatment group (cities that followed a de-prosecution strategy).\n\nWhich describes another approach that has grown in popularity, especially among criminologists. This so-called “micro-synthetic” approach constructs synthetic controls from many small pieces to comprise a larger treated piece. The classic criminological example might be that you have a neighborhood in your city with 20 census blocks that gets some focused deterrence intervention. You can use the remaining “untreated” census blocks in the city to use as composite pieces as part of the synthetic control. This approach is especially appealing because so much criminological research has a focus on small, disaggregated regions."
  },
  {
    "objectID": "posts/synth-impact/synth_impact.html#synthetic-controls-in-statistics",
    "href": "posts/synth-impact/synth_impact.html#synthetic-controls-in-statistics",
    "title": "Synthetic Controls and Small Areas",
    "section": "",
    "text": "Andrew Gelman recently covered a mildly controversial paper in criminology that suggested that a policy of “de-prosecution” by the Philadelphia District Attorney’s office resulted in an increase in homicides. This has sparked a lot of back-and-forth discussion on the appropriateness of the analysis and the kind of synthetic control method used. I’m not here to discuss any of these things, as many other smart people have already debated this to death (plus, given Hogan is reticent to release his data or code we may never really know exactly what he did).\nHowever, what I do want to discuss is something else Gelman wrote about on his blog:\n\nHogan and the others make comparisons, but the comparisons they make are to that weighted average of Detroit, New Orleans, and New York. The trouble is . . . that’s just 3 cities, and homicide rates can vary a lot from city to city. It just doesn’t make sense to throw away the other 96 cities in your data. The implied counterfactual is that if Philadelphia had continued post-2014 with its earlier sentencing policy, that its homicide rates would look like this weighted average of Detroit, New Orleans, and New York…\n\nWhat Gelman is talking about here is the commonly-used ADH approach (short for Abadie, Diamond, and Hainmueller). In this method you typically have one large “treated” area - such as a city or state - that implements some kind of policy. You then use other comparably large or similar areas to construct a synthetic version of your treated unit to estimate the counterfactual. It’s an appealing method because it is relatively simple to calculate, fairly transparent about where the weights come from, and has good overlap with more conventional difference-in-differences methods (non-negative weights with a sum-to-one constraint). So in a way, I don’t necessarily have the same issues that Gelman does, but he brings up a good point. By using only large aggregate units there is an inherent loss of information. In the ADH method we sort of assume that by matching closely on the outcome variable we can average over a lot of the confounders. Although in the ADH method you can also match on other covariates - but in my experience the vast majority of the synthetic control weights are derived solely from the pre-treatment outcomes.\n\n\nGelman further writes:\n\nMy understanding of a synthetic controls analysis went like this. You want to compare Philadelphia to other cities, but there are no other cities that are just like Philadelphia, so you break up the city into neighborhoods and find comparable neighborhoods in other cities . . . and when you’re done you’ve created this composite “city,” using pieces of other cities, that functions as a pseudo-Philadelphia. In creating this composite, you use lots of neighborhood characteristics, not just matching on a single outcome variable. And then you do all of this with other cities in your treatment group (cities that followed a de-prosecution strategy).\n\nWhich describes another approach that has grown in popularity, especially among criminologists. This so-called “micro-synthetic” approach constructs synthetic controls from many small pieces to comprise a larger treated piece. The classic criminological example might be that you have a neighborhood in your city with 20 census blocks that gets some focused deterrence intervention. You can use the remaining “untreated” census blocks in the city to use as composite pieces as part of the synthetic control. This approach is especially appealing because so much criminological research has a focus on small, disaggregated regions."
  },
  {
    "objectID": "posts/synth-impact/synth_impact.html#an-example-operation-impact-2014",
    "href": "posts/synth-impact/synth_impact.html#an-example-operation-impact-2014",
    "title": "Synthetic Controls and Small Areas",
    "section": "An Example: Operation Impact (2014)",
    "text": "An Example: Operation Impact (2014)\nAs a quick demo, here’s an example I presented for part of NIJ’s Smart Suite. The research question posed here is whether a surge in police activity New York City’s 47th precinct reduced assaults or robberies. There were a number of previous iterations of Operation Impact which an evaluation found a general decrease in crime(MacDonald, Fagan, and Geller 2016). The example here looks at a much later surge in 2014:\n\nThe data I organized for this example contains block-level census data from the American Community Survey, as well as monthly counts of some major crime categories (here: assault, burglary, motor vehicle theft, robbery, and larceny-theft). This is organized in a long-form dataset, which is indexed by precinct * geoid * month.\n\n\n\n\n\nprecinct\ngeoid\nimpact\ntotal_pop\ntotal_hh\ntotal_male\ntotal_white\ntotal_hispan\ntotal_black\ntotal_poverty\ntotal_snap\nmonth\nassault\nburglary\nmvt\nrobbery\ntheft\n\n\n\n\n1\n3.6061e+10\n0\n7315\n1090\n3539\n4594\n479\n332\n586\n14\n1\n0\n0\n0\n0\n5\n\n\n1\n3.6061e+10\n0\n7315\n1090\n3539\n4594\n479\n332\n586\n14\n2\n1\n0\n0\n0\n1\n\n\n1\n3.6061e+10\n0\n7315\n1090\n3539\n4594\n479\n332\n586\n14\n3\n1\n3\n0\n1\n7\n\n\n1\n3.6061e+10\n0\n7315\n1090\n3539\n4594\n479\n332\n586\n14\n4\n0\n1\n0\n1\n6\n\n\n1\n3.6061e+10\n0\n7315\n1090\n3539\n4594\n479\n332\n586\n14\n5\n0\n1\n0\n0\n5\n\n\n1\n3.6061e+10\n0\n7315\n1090\n3539\n4594\n479\n332\n586\n14\n6\n0\n0\n0\n1\n3\n\n\n\n\n\n\n\nThe precinct we’re interested in, the 47th, is comprised of 44 census blocks which are each measured at 12 time points. The remainder of the census blocks in the dataset are part of our “donor” pool, which we can use for our synthetic control.\n\nApplying the ‘Microsynth’ approach\nThe R package microsynth does almost all of the heavy lifting(Robbins and Davenport 2021). Without getting too much into the weeds, the general idea here is that we want to re-weight all of our untreated (non-Operation Impact) census blocks in a way that makes them nearly - or exactly - identical to the census blocks in the 47th precinct. Microsynth accomplishes this much in the same way that surveys are weighted to approximate the population of interest. However, instead here we treat the 47th precinct as our “population” and estimate weights to apprxomiate the pre-treatment outcomes and covariates in the treated precinct. The full code to run the model is below:\n\nfit &lt;-\n  microsynth(\n    nyc_impact,\n    idvar = 'geoid',\n    timevar = 'month',\n    intvar = 'impact',\n    start.pre = 1,\n    end.pre = 9,\n    end.post = 12,\n    match.out = c('assault', 'robbery'),\n    match.covar = c(\n      'total_pop',\n      'total_black',\n      'total_hispan',\n      'total_poverty',\n      'total_snap'),\n    result.var = c('assault', 'robbery'),\n    omnibus.var = c('assault', 'robbery'))\n\nAs a start, we can assess whether our synthetic control is appropriately balanced on pre-treatment differences. As we saw above, we matched on both time-varying and non time-varying covariates. Looking at the balance table below we see that we achieved exact balance on all our variables - which is quite good! This should give us more confidence that the outcomes we observe in the post period are due to the intervention, and not a result of systematic differences between treated and control units.\n\n\n\n\n\n\nTargets\nWeighted.Control\nAll.scaled\n\n\n\n\nIntercept\n23\n23\n23.00\n\n\ntotal_pop\n77311\n77311\n89810.67\n\n\ntotal_black\n44079\n44079\n23373.48\n\n\ntotal_hispan\n15693\n15693\n26494.13\n\n\ntotal_poverty\n24022\n24022\n18865.98\n\n\ntotal_snap\n6558\n6558\n4875.89\n\n\nassault.9\n27\n27\n18.56\n\n\nassault.8\n35\n35\n20.69\n\n\nassault.7\n28\n28\n21.13\n\n\nassault.6\n26\n26\n21.10\n\n\nassault.5\n35\n35\n20.43\n\n\nassault.4\n30\n30\n17.44\n\n\nassault.3\n25\n25\n17.57\n\n\nassault.2\n15\n15\n15.14\n\n\nassault.1\n16\n16\n16.22\n\n\nrobbery.9\n30\n30\n15.89\n\n\nrobbery.8\n38\n38\n16.61\n\n\nrobbery.7\n31\n31\n16.19\n\n\nrobbery.6\n26\n26\n14.36\n\n\nrobbery.5\n23\n23\n15.60\n\n\nrobbery.4\n21\n21\n12.91\n\n\nrobbery.3\n18\n18\n13.50\n\n\nrobbery.2\n13\n13\n13.79\n\n\nrobbery.1\n31\n31\n16.39\n\n\n\n\n\n\n\nWe can print out the results as well. Here we see that the observed number of assaults and robberies in the post-period were 77 and 53 for the 47th precinct, and 77.7 and 73.5 for the synthetic control, respectively. In the case of robbery we estimate that Operation Impact resulted in about a 28% decrease in robberies for the 3-month period in 2014.\n\n\n\n\n\n\n\n\n\n\n\n\nTrt\nCon\nPct.Chng\nLinear.pVal\nLinear.Lower\nLinear.Upper\n\n\n\n\nassault\n77\n77.70\n-0.01\n0.94\n-0.19\n0.22\n\n\nrobbery\n53\n73.54\n-0.28\n0.03\n-0.45\n-0.05\n\n\nOmnibus\nNA\nNA\nNA\n0.04\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nIt’s also helpful to visualize what this looks like. Looking at the results we see that most of the decrease in robberies occurred immediately after the start of the program. For assaults there’s a slight dip early on, but the overall results are more mixed.\n\n\nCode\nplot_microsynth(fit)"
  },
  {
    "objectID": "posts/synth-impact/synth_impact.html#full-code",
    "href": "posts/synth-impact/synth_impact.html#full-code",
    "title": "Synthetic Controls and Small Areas",
    "section": "Full Code",
    "text": "Full Code\n\n\nCode\nlibrary(microsynth)\n\nset.seed(1)\n\n# note: a good vignette is provided here:\n# https://cran.r-project.org/web/packages/microsynth/vignettes/introduction.html\n\n\n# data comes from NYC open data and US Census bureau, compiled by me\n# dependent variable is the number of assaults and robberies in Q3 2014\n# when time &gt; 9\n\n# census-level variables from the ACS 2014 5-year estimates\n# and are presented as raw counts, to facilitate weighting\n# b/c microsynth uses survey weighting via survey::calibrate()\n\n# file url\ndf &lt;- url('https://www.dropbox.com/s/08owr5710bnvxn0/nyc_impact_long.csv?raw=1')\n\n# read into R\nnyc_impact &lt;- read.csv(df)\n\n# MICROSYNTH\n#-----------------------#\n\n# model 1, without permutation-based inference\n# test statistics are calculated as weighted linear model\n\n# each microsynth needs the following:\n# idvar = variable identifying observations\n# timevar = variable indexing observations by time\n# intvar = variable that takes on 1 for treated units, post treatment\n#   is 0 otherwise\n# start.pre, end.pre, end.post define the start of the study period, the end\n#   of the pre-period, and the end of the post period\n# match.out = the time-varying variables that are to be matched exactly\n# match.cov = the time-invariant variables to be matched exactly\n# result.var = the outcome variable(s) of interest\n# omnibus.var = the outcome variable(s) that should be used in the calculation \n#   of an omnibus p-value\n\nfit &lt;-\n  microsynth(\n    nyc_impact,\n    idvar = 'geoid',\n    timevar = 'month',\n    intvar = 'impact',\n    start.pre = 1,\n    end.pre = 9,\n    end.post = 12,\n    match.out = c('assault', 'robbery'),\n    match.covar = c(\n      'total_pop',\n      'total_black',\n      'total_hispan',\n      'total_poverty',\n      'total_snap'),\n    result.var = c('assault', 'robbery'),\n    omnibus.var = c('assault', 'robbery'))\n\n# get the model summary\nsummary(fit)\n\n# plot treated vs synthetic\n# and gap plot of treated - synthetic\nplot_microsynth(fit)\n\n\n# PLACEBO-BASED INFERENCE\n#-----------------------#\n\n# this model is same as above, except we are calculating permutation p-values\n# here, I set the number of permutations to just 100, but ideally you\n# can set this higher. The more permutations you set, the longer the run time\n\nfit2 &lt;-\n  microsynth(\n    nyc_impact,\n    idvar = 'geoid',\n    timevar = 'month',\n    intvar = 'impact',\n    start.pre = 1,\n    end.pre = 9,\n    end.post = 12,\n    match.out = c('assault', 'robbery'),\n    match.covar = c(\n      'total_pop',\n      'total_black',\n      'total_hispan',\n      'total_poverty',\n      'total_snap'),\n    result.var = c('assault', 'robbery'),\n    omnibus.var = c('assault', 'robbery'),\n    perm = 100)\n\nsummary(fit2)\nplot_microsynth(fit2)"
  },
  {
    "objectID": "posts/modern-bert/rank-rerank.html",
    "href": "posts/modern-bert/rank-rerank.html",
    "title": "Information Retrieval Using the Retrieve and Rerank Method",
    "section": "",
    "text": "The logic behind the “retrieve and rerank” method is that we have two sets of tools that excel at one specific task. Specifically we want to use a combination of a bi-encoder and cross-encoder to retrieve data based on an initial input query. The trade-off we have to deal with is that cross-enocder models are very slow, while bi-encoder models have performance that often falls short for retreval purposes.\nThe bi-encoder model (the “retrieve” part) creates seperate embeddings of the input query and corpus text and looks for the closest match based on the vector space. This is often done by finding the nearest cosine similarity. The retreval step is typically quite fast, with the trade-off that some information is lost because the query and search corpus are embedded seperately.\nOn the flip side, a cross encoder embeds the search query and corpus together. The major advantage of this, is that the cross-encoder uses cross-attention to create the similarity score, which pools information about the both inputs directly. However, the major trade off is that this requires the search query to be embedded with every query-corpus pair. In a very large dataset, this might create millions of pairs and is can be potentially very slow.\n\n\n\n\n\n\n\nflowchart LR\n  A(\"Sentence A\") ==&gt; BA ==&gt; SA ==&gt; C\n  B(\"Sentence B\") ==&gt; BB ==&gt; SB ==&gt; C\n  BA[\"BERT\"]\n  BB[\"BERT\"]\n  SA[\"Sentence Embedding\"]\n  SB[\"Sentence Embedding\"]\n  C[\"Cosine Similarity\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  A(\"Sentence A\") ==&gt; C\n  B(\"Sentence A\") ==&gt; C\n  C[\"BERT\"] ==&gt; D\n  D[\"Classifier\"] ==&gt; E  \n  E[\"0...1\"]\n\n\n\n\n\n\nTherefore, it makes sense to use both of these methods in tandem. We can quickly retrieve the top 100 or so records using the bi-encoder, then re-rank the retrieved records using the bi-encoder. This way we limit the number of paired records we have to run through it.\nTo do this in Python I create a RetrieveReranker class. The class is initialized with a bi-encoder and cross-encoder model, and a corpus of text to serve as the searchable data base. Most of the important work is handled by the query function, which takes an input query string, creates an embedding, then retrieves the 100 most similar documents based on cosine similarity. These 100 records are then passed to the bi-encoder which re-ranks them and returns the most similar ones.\nI should note, this is a pretty limited first attempt at “off-the-shelf” pre-trained models. I’m not doing any pre-training, nor am I doing any fine-tuning here. It’s quite clear that both would strongly improve performance, but this is too simple of an example to warrant the effort.\n\n\nCode\nimport torch\nimport numpy as np\nimport os\nimport pickle\n\n\nclass RetrieveReranker:\n    def __init__(\n        self,\n        corpus,\n        bi_encoder_model,\n        cross_encoder_model,\n        save_corpus=False,\n        corpus_path=None,\n    ):\n        self.bi_encoder_model = bi_encoder_model\n        self.cross_encoder_model = cross_encoder_model\n        self.save_corpus = save_corpus\n        self.corpus_path = corpus_path\n\n        self.corpus = corpus  # raw text\n        self.corpus_embed = self._embed_corpus()  # embedded text\n\n    def _embed_corpus(self):\n        \"Embed and save a corpus of searchable text, or load corpus if present\"\n        embedding = None\n\n        try:\n            if os.path.exists(self.corpus_path):\n                embedding = self._load_corpus()\n            else:\n                embedding = self.bi_encoder_model.encode(self.corpus)\n\n                if self.save_corpus:\n                    self._save_corpus(embedding)\n\n        except Exception as e:\n            print(f\"Error processing corpus: {e}\")\n\n        return embedding\n\n    def _save_corpus(self, embedding):\n        with open(self.corpus_path, \"wb\") as fOut:\n            pickle.dump(embedding, fOut)\n\n    def _load_corpus(self):\n        with open(self.corpus_path, \"rb\") as fIn:\n            return pickle.load(fIn)\n\n    def query(self, query_string, number_ranks=100, number_results=1):\n        \"\"\"Find the top N results matching the input string and returning the\n        matched string and the index.\"\"\"\n\n        ce_list = []\n\n        # embed query in bi-enocder, then get cosine similarities w/ corpus\n        query_embed = self.bi_encoder_model.encode(query_string)\n        sims = self.bi_encoder_model.similarity(query_embed, self.corpus_embed)\n        idx = np.array(torch.topk(sims, number_ranks).indices)[0]\n\n        # create a list of paired strings\n        for i in idx:\n            ce_list.append([query_string, self.corpus[i]])\n\n        # run cross-encoder, get top `number_results`\n        # convert to probabilities using invlogit\n        scores = self.cross_encoder_model.predict(ce_list)\n        probs = torch.sigmoid(torch.tensor(scores))\n        top_idx = np.argsort(scores)[-number_results:][::-1]\n            \n        # Retrieve the results based on top indices\n        res_idx = [int(idx[i]) for i in top_idx] \n        res_prb = torch.tensor([probs[i] for i in top_idx])\n        res_str = [ce_list[i][1] for i in top_idx] \n\n        return res_idx, res_prb, res_str"
  },
  {
    "objectID": "posts/modern-bert/demo.html",
    "href": "posts/modern-bert/demo.html",
    "title": "A Blog for Data Stuff",
    "section": "",
    "text": "from transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom src.transformer_funcs import CustomDataset, new_input_to_prediction\nfrom src.utils import injury_codes\nimport torch\nimport pandas as pd\nimport numpy as np\nimport evaluate\nimport random\n\nrandom.seed(35418)\n\nMODEL = \"answerdotai/ModernBERT-base\"\nTRAIN_DATA = \"C:/Users/gioc4/Documents/blog/data/falls/neis.csv\"\nMAX_TOKEN_LENGTH = 256\nDATA_SIZE = 2000\nTRAIN_SIZE = .90\n\n# init some values\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\naccuracy = evaluate.load(\"accuracy\")\nprecision = evaluate.load(\"precision\")\nlabel_encoder = LabelEncoder()\n\n# load data\nneis_data = pd.read_csv(TRAIN_DATA).head(DATA_SIZE)\n\n# get top 5 diagnoses\nvalues = neis_data.groupby('Diagnosis').size().sort_values(ascending=False)[:5]\ntrain_data = neis_data[neis_data['Diagnosis'].isin(values.index.values)]\n\nc:\\Users\\gioc4\\Anaconda3\\envs\\bert_models\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n# some local funcs\ndef prep_data(dataframe):\n    X = dataframe['Narrative_1'].to_list()\n    y = label_encoder.fit_transform(dataframe['Diagnosis'].map(injury_codes))\n\n    # return dict of encoded labels\n    keys = label_encoder.classes_\n    values = label_encoder.transform(label_encoder.classes_)\n    value_dict = dict(zip(keys, map(int, values)))\n\n\n    return X, y, value_dict\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n\n    # get preds using just the max predicted value\n    acc = accuracy.compute(predictions=predictions, references=labels)\n\n    return acc\n\n\n# set up data\n# prepare the text and labels, train-test split, and init torch datasets\n\nX, y, value_dict = prep_data(train_data)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=TRAIN_SIZE, random_state=42\n)\n\n\ntrain_dataset = CustomDataset(X_train, y_train, tokenizer, MAX_TOKEN_LENGTH)\ntest_dataset = CustomDataset(X_test, y_test, tokenizer, MAX_TOKEN_LENGTH)\n\n\n# set up model\n\n# set labels for inputs\nid2label = dict((v,k) for k,v in value_dict.items())\nlabel2id = value_dict\n\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=5, id2label=id2label, label2id=label2id)\n\ntraining_args = TrainingArguments(\n    output_dir=\"models\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\nSome weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n                                                \n 50%|█████     | 87/174 [20:16&lt;16:54, 11.66s/it]\n\n\n{'eval_loss': 0.8524370193481445, 'eval_accuracy': 0.6753246753246753, 'eval_runtime': 37.0549, 'eval_samples_per_second': 4.156, 'eval_steps_per_second': 0.27, 'epoch': 1.0}\n\n\n                                                 \n100%|██████████| 174/174 [40:56&lt;00:00, 11.84s/it]\n\n\n{'eval_loss': 0.5316950678825378, 'eval_accuracy': 0.7727272727272727, 'eval_runtime': 37.2713, 'eval_samples_per_second': 4.132, 'eval_steps_per_second': 0.268, 'epoch': 2.0}\n\n\n100%|██████████| 174/174 [40:58&lt;00:00, 14.13s/it]\n\n\n{'train_runtime': 2458.4675, 'train_samples_per_second': 1.126, 'train_steps_per_second': 0.071, 'train_loss': 0.8236960926275144, 'epoch': 2.0}\n\n\n\n\n\nTrainOutput(global_step=174, training_loss=0.8236960926275144, metrics={'train_runtime': 2458.4675, 'train_samples_per_second': 1.126, 'train_steps_per_second': 0.071, 'total_flos': 471618667438080.0, 'train_loss': 0.8236960926275144, 'epoch': 2.0})\n\n\n\n# load model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"models/checkpoint-174\", num_labels=5)\ntokenizer = AutoTokenizer.from_pretrained(\"models/checkpoint-174\")\n\n\nnewdata = pd.read_csv(TRAIN_DATA)\nnewdata = newdata.iloc[2500:3000]\nnewdata = newdata[newdata['Diagnosis'].isin(values.index)]\n\nnew_text_input = newdata['Narrative_1'].tolist()\n\n\n# to get new preds we pass the input through the tokenizer\n# and get the tokenized input and attention mask\noutputs = new_input_to_prediction(model, new_text_input, tokenizer, MAX_TOKEN_LENGTH)\n\n# convert predictions to probabilities, then get max probability as label\npredictions = torch.nn.functional.softmax(outputs.logits, dim=1)\ndf_preds = pd.DataFrame(\n    {\n        \"text\": new_text_input,\n        \"label\": newdata[\"Diagnosis\"],\n        \"pred\": predictions.argmax(1),\n    }\n)\n\n\ntorch.nn.functional.softmax(outputs.logits, dim=1)\n\ntensor([[2.3225e-02, 7.6291e-01, 1.9845e-01, 8.7046e-03, 6.7145e-03],\n        [5.4486e-01, 1.5052e-01, 4.9559e-02, 6.9284e-02, 1.8578e-01],\n        [3.4269e-02, 3.8225e-02, 8.2826e-01, 9.0455e-02, 8.7931e-03],\n        ...,\n        [3.1903e-03, 1.3829e-03, 1.7341e-03, 1.6485e-04, 9.9353e-01],\n        [1.9445e-03, 9.5668e-01, 3.6274e-02, 4.3934e-03, 7.1027e-04],\n        [9.6429e-05, 9.9820e-01, 8.8022e-04, 7.7958e-04, 4.7722e-05]])\n\n\n\ndf_preds['pred_LABEL'] = df_preds['pred'].map(model.config.id2label)\ndf_preds\n\n\n\n\n\n\n\n\ntext\nlabel\npred\npred_LABEL\n\n\n\n\n2501\n27 YOM FELL SKIING AND INJ HAND ON MOUNTAIN D...\n57\n1\nFracture\n\n\n2502\n7 YOF IN HOUSE AND FELL AND HIT HAND ON FURNIT...\n57\n0\nContusions, Abrasions\n\n\n2503\n5 MOM ROLLED OFF BED LANDING ON TILE FLOOR AND...\n53\n2\nInternal organ injury\n\n\n2504\n26 YOM FELL SNOWBOARDING ONTO KNEE INJ IT DX...\n57\n2\nInternal organ injury\n\n\n2505\n49 YOF PLAYING SOCCER AND KNOCKED TO GROUOND A...\n53\n0\nContusions, Abrasions\n\n\n...\n...\n...\n...\n...\n\n\n2992\n2 YOM FELL INTO DOORINJURED LIP DX LACERATION LIP\n59\n3\nLaceration\n\n\n2994\n25 YOM INJURED FINGER ON A BROKEN MIRROR DX LA...\n59\n3\nLaceration\n\n\n2995\n45 YOF CO PAIN RIGHT ANKLE AND SWELLING HURTS ...\n71\n4\nOther/Not Stated\n\n\n2997\n22 YOM FELL FROM A LADDER AND INJURED LEFT ANK...\n57\n1\nFracture\n\n\n2998\n95 YOF FELL IN FLOOR AND FRACTURED HIP DX FRAC...\n57\n1\nFracture\n\n\n\n\n378 rows × 4 columns"
  },
  {
    "objectID": "posts/modern-bert/sentence_transofrmers.html",
    "href": "posts/modern-bert/sentence_transofrmers.html",
    "title": "A Blog for Data Stuff",
    "section": "",
    "text": "from sentence_transformers import SentenceTransformer\nfrom sentence_transformers.cross_encoder import CrossEncoder\n\nimport numpy as np\nimport pandas as pd\nimport re\n\nfrom src.search_funcs import RetrieveReranker\n\n# local vars\nBI_ENCODER_MODEL = \"answerdotai/ModernBERT-base\"\nCROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"\nCORPUS = \"C:/Users/gioc4/Documents/blog/data/falls/neis.csv\"\nMAX_TOKEN_LENGTH = 512\nCORPUS_SIZE = 50000\n\n# we want the observations to be agnostic to patient age, so we remove those\n# define remappings of abbreviations\n# and strings to remove from narratives\n\nremap = {\n    \"FX\": \"FRACTURE\",\n    \"INJ\": \"INJURY\",\n    \"LAC\": \"LACERATION\",\n    \"LOC\": \"LOSS OF CONCIOUSNESS\",\n    \"CONT\": \"CONTUSION\",\n    \"CHI\" : \"CLOSED HEAD INJURY\",\n    \"ETOH\": \"ALCOHOL\",\n    \"SDH\": \"SUBDURAL HEMATOMA\",\n    \"AFIB\": \"ATRIAL FIBRILLATION\",\n    \"NH\": \"NURSING HOME\",\n    \"LTCF\": \"LONG TERM CARE FACILITY\",\n    \"C/O\": \"COMPLAINS OF\",\n    \"H/O\": \"HISTORY OF\",\n    \"S/P\": \"STATUS POST\",\n    \"DX:\": \"DIAGNOSIS\",\n    \"YOM\": \"YEAR OLD MALE\",\n    \"YOF\": \"YEAR OLD FEMALE\",\n    \"MOM\": \"MONTH OLD MALE\",\n    \"MOF\": \"MONTH OLD FEMALE\",\n    \"PT\": \"PATIENT\",\n    \"LT\": \"LEFT\",\n    \"RT\": \"RIGHT\",\n    \"&\" : \" AND \"\n}\n\ndef process_text(txt):\n\n    # remap leading age and sex info\n    txt = re.sub(r\"(\\d+)(YOM|YOF|MOM|MOF)\", lambda m: f\"{m.group(1)} {remap[m.group(2)]}\", txt)\n\n    words = txt.split()\n    new_words = [remap.get(word, word) for word in words]\n    txt = \" \".join(new_words)\n\n    return re.sub(r\"^\\s+\", \"\", txt)\n\n\n# strings to encode as searchable\n\n# load data\nneis_data = pd.read_csv(CORPUS).head(CORPUS_SIZE)\nnarrative_strings = neis_data['Narrative_1'].apply(process_text).tolist()\n\n# define models and ranker\nbiencoder = SentenceTransformer(BI_ENCODER_MODEL)\ncrossencoder = CrossEncoder(CROSS_ENCODER_MODEL)\n\nNo sentence-transformers model found with name answerdotai/ModernBERT-base. Creating a new one with mean pooling.\n\n\n\n# set up a Retriveal-Ranker class\nranker = RetrieveReranker(\n    corpus=narrative_strings,\n    bi_encoder_model=biencoder,\n    cross_encoder_model=crossencoder,\n    save_corpus=True,\n    corpus_path=\"C:/Users/gioc4/Documents/blog/data/corpus_large.pkl\"\n)\n\n\n# rag-ish thing\n# get the top 5 most similar cases, based on the query\nquery = \"80 MALE FELT DIZZY AND HIT HEAD ON TOLIET\"\n\nidx, output = ranker.query(process_text(query), number_results=5)\n\n\noutput\n\n['90 MALE WAS GETTING OUT OF BED AND FELL STRUCK FACE ON THE CLOSET DOOR DIAGNOSIS HEMATOMA TO HEAD',\n '92 MALE WITH A FALL DOWN THE STEPS STRIKING HIS HEAD +HEAD PAIN DX BRADYCARDIA, FALL',\n '16 MONTH OLD MALE WAS PLAYING AND FELL HITTING HIS HEAD ON A WOODEN CHAIR DX LACERATION OF HEAD',\n '3 MALE FELL OUT OF A CHAIR AND HIT HEAD ON A RADIATOR. DX FACE LACERATION',\n '85 MALE WITH FALL OUT OF CHAIR DIAGNOSIS CLOSED HEAD INJURY AND LACERATION TO FACE']\n\n\n\nneis_data.iloc[idx]\n\n\n\n\n\n\n\n\nCPSC_Case_Number\nTreatment_Date\nAge\nSex\nRace\nOther_Race\nHispanic\nBody_Part\nDiagnosis\nOther_Diagnosis\n...\nFire_Involvement\nProduct_1\nProduct_2\nProduct_3\nAlcohol\nDrug\nNarrative_1\nStratum\nPSU\nWeight\n\n\n\n\n545\n220111585\n1/2/2022\n219\n2\n2\nNaN\n2\n76\n53\nNaN\n...\n0\n4076\n0\n0\n0\n0\n19 MOF FELL OFF BED. DX FACE CONTUSION, HEAD ...\nC\n31\n5.8342\n\n\n837\n220114137\n1/3/2022\n205\n2\n2\nNaN\n2\n76\n53\nNaN\n...\n0\n4076\n0\n0\n0\n0\n5 MOF FELL OFF BED. DX FACE CONTUSION\nC\n31\n5.8342\n\n\n831\n220114126\n1/3/2022\n219\n2\n1\nNaN\n2\n76\n59\nNaN\n...\n0\n4076\n0\n0\n0\n0\n19 MOF FELL AND HIT FACE ON BED FRAME. DX LAC...\nC\n31\n5.8342\n\n\n234\n220108361\n1/1/2022\n219\n1\n2\nNaN\n2\n75\n62\nNaN\n...\n0\n1842\n0\n0\n0\n0\n19 MOM FELL DOWN STEPS. DX HEAD INJURY\nC\n31\n5.8342\n\n\n228\n220108350\n1/1/2022\n214\n2\n1\nNaN\n2\n75\n62\nNaN\n...\n0\n679\n1807\n0\n0\n0\n14 MOF STANDING ON A COUCH AND FELL OFF, HIT H...\nC\n31\n5.8342\n\n\n\n\n5 rows × 25 columns"
  },
  {
    "objectID": "posts/modern-bert/rank-rerank.html#querying-records",
    "href": "posts/modern-bert/rank-rerank.html#querying-records",
    "title": "Information Retrieval Using the Retrieve and Rerank Method",
    "section": "",
    "text": "The logic behind the “retrieve and rerank” method is that we have two sets of tools that excel at one specific task. Specifically we want to use a combination of a bi-encoder and cross-encoder to retrieve data based on an initial input query. The trade-off we have to deal with is that cross-enocder models are very slow, while bi-encoder models have performance that often falls short for retreval purposes.\nThe bi-encoder model (the “retrieve” part) creates seperate embeddings of the input query and corpus text and looks for the closest match based on the vector space. This is often done by finding the nearest cosine similarity. The retreval step is typically quite fast, with the trade-off that some information is lost because the query and search corpus are embedded seperately.\nOn the flip side, a cross encoder embeds the search query and corpus together. The major advantage of this, is that the cross-encoder uses cross-attention to create the similarity score, which pools information about the both inputs directly. However, the major trade off is that this requires the search query to be embedded with every query-corpus pair. In a very large dataset, this might create millions of pairs and is can be potentially very slow.\n\n\n\n\n\n\n\nflowchart LR\n  A(\"Sentence A\") ==&gt; BA ==&gt; SA ==&gt; C\n  B(\"Sentence B\") ==&gt; BB ==&gt; SB ==&gt; C\n  BA[\"BERT\"]\n  BB[\"BERT\"]\n  SA[\"Sentence Embedding\"]\n  SB[\"Sentence Embedding\"]\n  C[\"Cosine Similarity\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  A(\"Sentence A\") ==&gt; C\n  B(\"Sentence A\") ==&gt; C\n  C[\"BERT\"] ==&gt; D\n  D[\"Classifier\"] ==&gt; E  \n  E[\"0...1\"]\n\n\n\n\n\n\nTherefore, it makes sense to use both of these methods in tandem. We can quickly retrieve the top 100 or so records using the bi-encoder, then re-rank the retrieved records using the bi-encoder. This way we limit the number of paired records we have to run through it.\nTo do this in Python I create a RetrieveReranker class. The class is initialized with a bi-encoder and cross-encoder model, and a corpus of text to serve as the searchable data base. Most of the important work is handled by the query function, which takes an input query string, creates an embedding, then retrieves the 100 most similar documents based on cosine similarity. These 100 records are then passed to the bi-encoder which re-ranks them and returns the most similar ones.\nI should note, this is a pretty limited first attempt at “off-the-shelf” pre-trained models. I’m not doing any pre-training, nor am I doing any fine-tuning here. It’s quite clear that both would strongly improve performance, but this is too simple of an example to warrant the effort.\n\n\nCode\nimport torch\nimport numpy as np\nimport os\nimport pickle\n\n\nclass RetrieveReranker:\n    def __init__(\n        self,\n        corpus,\n        bi_encoder_model,\n        cross_encoder_model,\n        save_corpus=False,\n        corpus_path=None,\n    ):\n        self.bi_encoder_model = bi_encoder_model\n        self.cross_encoder_model = cross_encoder_model\n        self.save_corpus = save_corpus\n        self.corpus_path = corpus_path\n\n        self.corpus = corpus  # raw text\n        self.corpus_embed = self._embed_corpus()  # embedded text\n\n    def _embed_corpus(self):\n        \"Embed and save a corpus of searchable text, or load corpus if present\"\n        embedding = None\n\n        try:\n            if os.path.exists(self.corpus_path):\n                embedding = self._load_corpus()\n            else:\n                embedding = self.bi_encoder_model.encode(self.corpus)\n\n                if self.save_corpus:\n                    self._save_corpus(embedding)\n\n        except Exception as e:\n            print(f\"Error processing corpus: {e}\")\n\n        return embedding\n\n    def _save_corpus(self, embedding):\n        with open(self.corpus_path, \"wb\") as fOut:\n            pickle.dump(embedding, fOut)\n\n    def _load_corpus(self):\n        with open(self.corpus_path, \"rb\") as fIn:\n            return pickle.load(fIn)\n\n    def query(self, query_string, number_ranks=100, number_results=1):\n        \"\"\"Find the top N results matching the input string and returning the\n        matched string and the index.\"\"\"\n\n        ce_list = []\n\n        # embed query in bi-enocder, then get cosine similarities w/ corpus\n        query_embed = self.bi_encoder_model.encode(query_string)\n        sims = self.bi_encoder_model.similarity(query_embed, self.corpus_embed)\n        idx = np.array(torch.topk(sims, number_ranks).indices)[0]\n\n        # create a list of paired strings\n        for i in idx:\n            ce_list.append([query_string, self.corpus[i]])\n\n        # run cross-encoder, get top `number_results`\n        # convert to probabilities using invlogit\n        scores = self.cross_encoder_model.predict(ce_list)\n        probs = torch.sigmoid(torch.tensor(scores))\n        top_idx = np.argsort(scores)[-number_results:][::-1]\n            \n        # Retrieve the results based on top indices\n        res_idx = [int(idx[i]) for i in top_idx] \n        res_prb = torch.tensor([probs[i] for i in top_idx])\n        res_str = [ce_list[i][1] for i in top_idx] \n\n        return res_idx, res_prb, res_str"
  },
  {
    "objectID": "posts/modern-bert/rank-rerank.html#creating-a-document-retreval-model",
    "href": "posts/modern-bert/rank-rerank.html#creating-a-document-retreval-model",
    "title": "Information Retreval Using the Rank-Rerank Method",
    "section": "Creating A Document Retreval Model",
    "text": "Creating A Document Retreval Model\nNow that we have our class defined, we can import it below and utilize it. In order for it to work we need to pass in both a bi-encoder and a cross-encoder model. Recall, the bi-encoder will do the first pass to get the \\(N\\) most similar records, then pass these to the cross-encoder. Hence, the rank-rerank method. Below, we use ModernBERT in tandem with a SentenceTransformers model to do the embedding and first pass as the bi-encoder, and a MS Macro model as the cross encoder.\nNow, ideally we would fine-tune the cross-encoder model so that input queries would more closely match the retrieved documents. In addition, this would help in the case of asymmeterical queries (e.g. providing a short query to retrieve a much longer text). But right now we can rely on out-of-the box performance as a demonstration.\nOur corpus is relatively small. We take a sample of 50,000 records from the 2022 NEISS dataset. We use a max token length of 512 characters. Finally, we define some local functions to clean up the NEISS text entries a bit before we pass them into the model.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.cross_encoder import CrossEncoder\n\nimport numpy as np\nimport pandas as pd\nimport re\n\nfrom src.search_funcs import RetrieveReranker\n\n# local vars\nBI_ENCODER_MODEL = \"answerdotai/ModernBERT-base\"\nCROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\nCORPUS = \"C:/Users/gioc4/Documents/blog/data/falls/neis.csv\"\nMAX_TOKEN_LENGTH = 512\nCORPUS_SIZE = 50000\n\n# we want the observations to be agnostic to patient age, so we remove those\n# define remappings of abbreviations\n# and strings to remove from narratives\n\nremap = {\n    \"FX\": \"FRACTURE\",\n    \"INJ\": \"INJURY\",\n    \"LAC\": \"LACERATION\",\n    \"LOC\": \"LOSS OF CONCIOUSNESS\",\n    \"CONT\": \"CONTUSION\",\n    \"CHI\" : \"CLOSED HEAD INJURY\",\n    \"ETOH\": \"ALCOHOL\",\n    \"SDH\": \"SUBDURAL HEMATOMA\",\n    \"AFIB\": \"ATRIAL FIBRILLATION\",\n    \"NH\": \"NURSING HOME\",\n    \"LTCF\": \"LONG TERM CARE FACILITY\",\n    \"C/O\": \"COMPLAINS OF\",\n    \"H/O\": \"HISTORY OF\",\n    \"S/P\": \"STATUS POST\",\n    \"DX:\": \"DIAGNOSIS\",\n    \"YOM\": \"YEAR OLD MALE\",\n    \"YOF\": \"YEAR OLD FEMALE\",\n    \"MOM\": \"MONTH OLD MALE\",\n    \"MOF\": \"MONTH OLD FEMALE\",\n    \"PT\": \"PATIENT\",\n    \"LT\": \"LEFT\",\n    \"RT\": \"RIGHT\",\n    \"&\" : \" AND \"\n}\n\ndef process_text(txt):\n\n    # remap leading age and sex info\n    txt = re.sub(r\"(\\d+)(YOM|YOF|MOM|MOF)\", lambda m: f\"{m.group(1)} {remap[m.group(2)]}\", txt)\n\n    words = txt.split()\n    new_words = [remap.get(word, word) for word in words]\n    txt = \" \".join(new_words)\n\n    return re.sub(r\"^\\s+\", \"\", txt)\n\n\nNow that we’re ready, we can encode the corpus using the pre-defined models by passing it all into our RetrieveReranker class. Passing the corpus_path argument allows us to save the embeddings as a pickle file and reload it when it exists so we don’t have to go through the very time consuming process of re-embedding the corpus each time we do this.\n\n# strings to encode as searchable\n# load data\nneis_data = pd.read_csv(CORPUS).head(CORPUS_SIZE)\nnarrative_strings = neis_data['Narrative_1'].apply(process_text).tolist()\n\n# define models and ranker\nbiencoder = SentenceTransformer(BI_ENCODER_MODEL)\ncrossencoder = CrossEncoder(CROSS_ENCODER_MODEL)\n\n# set up a Retriveal-Ranker class\nranker = RetrieveReranker(\n    corpus=narrative_strings,\n    bi_encoder_model=biencoder,\n    cross_encoder_model=crossencoder,\n    save_corpus=True,\n    corpus_path=\"C:/Users/gioc4/Documents/blog/data/corpus_large.pkl\"\n)\n\n\nRetreiving similar records\nAfter that has processed we’re ready to query our corpus with an example text string. Let’s imagine we had a case involving an elderly fall at an elderly care facility (ECF) and we wanted to find 5 similar cases based on information provided in the narrative:\n\n“100 YOM RESIDENT AT ECF FELL BACKWARDS ON THE FLOOR. DX: CERVICAL STRAIN, LUMBAR STRAIN”\n\nWe directly pass this query into our fitted RetrieveReranker and specify the number of results we want. We get indices and matching strings as output.\n\nquery = \"100 YOM RESIDENT AT ECF FELL BACKWARDS ON THE FLOOR. DX: CERVICAL STRAIN, LUMBAR STRAIN\"\n\nidx, output = ranker.query(process_text(query), number_results=5)\n\nHere are the matching queries:\n\noutput\n\n['93 YEAR OLD FEMALE RESIDENT AT ECF LOST BALANCE AND FELL BACKWARDS ONTO THE FLOOR. DIAGNOSIS C-5 FRACTURE.',\n '87 YEAR OLD FEMALE RESIDENT AT ECF LOST BALANCE AND FELL BACKWARDS ON THE FLOOR. DIAGNOSIS SACRAL FRACTURE.',\n '84 YEAR OLD MALE RESIDENT AT ECF FELL ON THE FLOOR. DIAGNOSIS SUBDURAL HEMATOMA.',\n '71 YEAR OLD MALE RESIDENT AT ECF TRIPPED AND FELL ON THE FLOOR. DIAGNOSIS NASAL BONE FRACTURE.',\n '95 YEAR OLD FEMALE RESIDENT AT ECF FELL ON THE FLOOR. DIAGNOSIS CLOSED HEAD INJURY, LUMBAR STRAIN.']\n\n\nAnd are the matching records in the data frame:\n\nneis_data.iloc[idx]\n\n\n\n\n\n\n\n\nCPSC_Case_Number\nTreatment_Date\nAge\nSex\nRace\nOther_Race\nHispanic\nBody_Part\nDiagnosis\nOther_Diagnosis\n...\nFire_Involvement\nProduct_1\nProduct_2\nProduct_3\nAlcohol\nDrug\nNarrative_1\nStratum\nPSU\nWeight\n\n\n\n\n47104\n220505213\n2/24/2022\n93\n2\n1\nNaN\n2\n89\n57\nNaN\n...\n0\n1807\n0\n0\n0\n0\n93 YOF RESIDENT AT ECF LOST BALANCE AND FELL B...\nV\n95\n17.2223\n\n\n46029\n220410468\n2/6/2022\n87\n2\n1\nNaN\n2\n79\n57\nNaN\n...\n0\n1807\n0\n0\n0\n0\n87 YOF RESIDENT AT ECF LOST BALANCE AND FELL B...\nV\n95\n17.2223\n\n\n22886\n220371574\n1/28/2022\n84\n1\n1\nNaN\n2\n75\n62\nNaN\n...\n0\n1807\n0\n0\n0\n0\n84 YOM RESIDENT AT ECF FELL ON THE FLOOR. DX: ...\nV\n95\n17.2223\n\n\n47110\n220505221\n2/24/2022\n71\n1\n1\nNaN\n2\n76\n57\nNaN\n...\n0\n1807\n0\n0\n0\n0\n71 YOM RESIDENT AT ECF TRIPPED AND FELL ON THE...\nV\n95\n17.2223\n\n\n46661\n220432778\n2/12/2022\n95\n2\n1\nNaN\n2\n75\n62\nNaN\n...\n0\n1807\n0\n0\n0\n0\n95 YOF RESIDENT AT ECF FELL ON THE FLOOR. DX: ...\nV\n95\n17.2223\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\nAsymmetrical queries\nGiven we have done zero fine tuning on either the embedding model or the cross encoder, the resulst are are pretty surprising. However, a notable weakness of this is that the model is not robust for asymmetrical queries - that is, queries which are much shorter than one in the corpus. For example, let’s say we just wanted to find a case where an elderly person fell in a bathtub:\n\nshort_query = \"80YOM SLIPPED AND FELL IN BATHTUB\"\n\n_, output = ranker.query(process_text(short_query ), number_results=5)\n\noutput\n\n['16 YEAR OLD MALE SLIPPED AND FELL GETTING OUT OF BATHTUB. DX CONCUSSION',\n '30 YEAR OLD MALE FELL IN BATHTUB DX; BACK CONTUSION',\n '62 YEAR OLD MALE SLIPPED AND FELL IN THE SHOWER. DX:CERVICAL STRAIN.',\n '61 YEAR OLD MALE SLIPPED AND FELL GETTING OUT OF SHOWER DX; R ANKLE FRACTURE',\n '71 YEAR OLD MALE FELL IN SHOWER DX NECK PAIN']\n\n\nThe results here are ok (they give us cases involving slip and falls in a bathtub) but we’ll note they are similarly short to the input query. This is because the retreveal model matches close to queries with similar lengths. Fine-tuning could help improve this - although it is a time-consuming process. What we have built here isn’t really a “true” RAG search model, but more of a semantic search and retreval model. In the latter the model expects to see a more context-rich example to use for document retreval."
  },
  {
    "objectID": "posts/modern-bert/rank-rerank.html#creating-a-records-retrieval-model",
    "href": "posts/modern-bert/rank-rerank.html#creating-a-records-retrieval-model",
    "title": "Information Retrieval Using the Retrieve and Rerank Method",
    "section": "Creating A Records Retrieval Model",
    "text": "Creating A Records Retrieval Model\nNow that we have our class defined, we can import it below and utilize it. In order for it to work we need to pass in both a bi-encoder and a cross-encoder model. Recall, the bi-encoder will do the first pass to get the \\(N\\) most similar records, then pass these to the cross-encoder. Hence, the “retrieve and rerank” method. Below, we use ModernBERT in tandem with a SentenceTransformers model to do the embedding and first pass as the bi-encoder, and a MS Macro model as the cross encoder.\nNow, ideally we would fine-tune the cross-encoder model so that input queries would more closely match the medical narratives. This would have the added benefit of improvement performance for asymmeterical queries (e.g. providing a short query to retrieve a much longer text). But right now we can rely on out-of-the box performance as a demonstration.\nOur corpus is relatively small. We take a sample of 50,000 records from the 2022 NEISS dataset and use some local functions to clean up the NEISS text entries a bit before we pass them into the model. From these narratives we pass them through a SentenceTransformer model using ModernBert to embed them as a 50000x768 dimension array. Essentially this a fancy method of data compression, where we extract and store semantic meaning from the narratives as a vector of numeric values.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.cross_encoder import CrossEncoder\n\nimport numpy as np\nimport pandas as pd\nimport re\n\nfrom src.search_funcs import RetrieveReranker\n\n# local vars\nBI_ENCODER_MODEL = \"answerdotai/ModernBERT-base\"\nCROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\nCORPUS = \"C:/Users/gioc4/Documents/blog/data/falls/neis.csv\"\nCORPUS_SIZE = 50000\n\n# we want the observations to be agnostic to patient age, so we remove those\n# define remappings of abbreviations\n# and strings to remove from narratives\n\nremap = {\n    \"FX\": \"FRACTURE\",\n    \"INJ\": \"INJURY\",\n    \"LAC\": \"LACERATION\",\n    \"LOC\": \"LOSS OF CONCIOUSNESS\",\n    \"CONT\": \"CONTUSION\",\n    \"CHI\" : \"CLOSED HEAD INJURY\",\n    \"ETOH\": \"ALCOHOL\",\n    \"SDH\": \"SUBDURAL HEMATOMA\",\n    \"AFIB\": \"ATRIAL FIBRILLATION\",\n    \"NH\": \"NURSING HOME\",\n    \"LTCF\": \"LONG TERM CARE FACILITY\",\n    \"C/O\": \"COMPLAINS OF\",\n    \"H/O\": \"HISTORY OF\",\n    \"S/P\": \"STATUS POST\",\n    \"DX:\": \"DIAGNOSIS\",\n    \"YOM\": \"YEAR OLD MALE\",\n    \"YOF\": \"YEAR OLD FEMALE\",\n    \"MOM\": \"MONTH OLD MALE\",\n    \"MOF\": \"MONTH OLD FEMALE\",\n    \"PT\": \"PATIENT\",\n    \"LT\": \"LEFT\",\n    \"RT\": \"RIGHT\",\n    \"&\" : \" AND \"\n}\n\ndef process_text(txt):\n\n    # remap leading age and sex info\n    txt = re.sub(r\"(\\d+)(YOM|YOF|MOM|MOF)\", lambda m: f\"{m.group(1)} {remap[m.group(2)]}\", txt)\n\n    words = txt.split()\n    new_words = [remap.get(word, word) for word in words]\n    txt = \" \".join(new_words)\n\n    return re.sub(r\"^\\s+\", \"\", txt)\n\n\nNow that we’re ready, we can encode the corpus using the pre-defined models by passing it all into our RetrieveReranker class. Passing the corpus_path argument allows us to save the embeddings as a pickle file and reload it when it exists so we don’t have to go through the very time consuming process of re-embedding the corpus each time we do this. Without using a GPU embedding 50,000 narratives takes around 30-40 minutes.\n\n# strings to encode as searchable\n# load data\nneis_data = pd.read_csv(CORPUS).head(CORPUS_SIZE)\nnarrative_strings = neis_data['Narrative_1'].apply(process_text).tolist()\n\n# define models and ranker\nbiencoder = SentenceTransformer(BI_ENCODER_MODEL)\ncrossencoder = CrossEncoder(CROSS_ENCODER_MODEL)\n\n# set up a Retriveal-Ranker class\nranker = RetrieveReranker(\n    corpus=narrative_strings,\n    bi_encoder_model=biencoder,\n    cross_encoder_model=crossencoder,\n    save_corpus=True,\n    corpus_path=\"C:/Users/gioc4/Documents/blog/data/corpus_large.pkl\"\n)\n\n\nRetreiving similar records\nAfter that has processed we’re ready to query our corpus with an example text string. Let’s imagine we had a case involving an elderly fall at an elderly care facility (ECF) and we wanted to find 5 similar cases based on information provided in the narrative:\n\n“100 YOM RESIDENT AT ECF FELL BACKWARDS ON THE FLOOR. DX: CERVICAL STRAIN, LUMBAR STRAIN”\n\nWe directly pass this query into our fitted RetrieveReranker and specify the number of results we want. We get indices and matching strings as output.\n\nquery = \"100 YOM RESIDENT AT ECF FELL BACKWARDS ON THE FLOOR. DX: CERVICAL STRAIN, LUMBAR STRAIN\"\n\nidx, proba, output = ranker.query(process_text(query), number_results=5)\n\nHere are the matching queries:\n\noutput\n\n['93 YEAR OLD FEMALE RESIDENT AT ECF LOST BALANCE AND FELL BACKWARDS ONTO THE FLOOR. DIAGNOSIS C-5 FRACTURE.',\n '87 YEAR OLD FEMALE RESIDENT AT ECF LOST BALANCE AND FELL BACKWARDS ON THE FLOOR. DIAGNOSIS SACRAL FRACTURE.',\n '84 YEAR OLD MALE RESIDENT AT ECF FELL ON THE FLOOR. DIAGNOSIS SUBDURAL HEMATOMA.',\n '71 YEAR OLD MALE RESIDENT AT ECF TRIPPED AND FELL ON THE FLOOR. DIAGNOSIS NASAL BONE FRACTURE.',\n '95 YEAR OLD FEMALE RESIDENT AT ECF FELL ON THE FLOOR. DIAGNOSIS CLOSED HEAD INJURY, LUMBAR STRAIN.']\n\n\nThe probability scores:\n\nproba\n\ntensor([0.9996, 0.9996, 0.9996, 0.9995, 0.9995])\n\n\nAnd are the matching records in the data frame:\n\nneis_data.iloc[idx]\n\n\n\n\n\n\n\n\nCPSC_Case_Number\nTreatment_Date\nAge\nSex\nRace\nOther_Race\nHispanic\nBody_Part\nDiagnosis\nOther_Diagnosis\n...\nFire_Involvement\nProduct_1\nProduct_2\nProduct_3\nAlcohol\nDrug\nNarrative_1\nStratum\nPSU\nWeight\n\n\n\n\n47104\n220505213\n2/24/2022\n93\n2\n1\nNaN\n2\n89\n57\nNaN\n...\n0\n1807\n0\n0\n0\n0\n93 YOF RESIDENT AT ECF LOST BALANCE AND FELL B...\nV\n95\n17.2223\n\n\n46029\n220410468\n2/6/2022\n87\n2\n1\nNaN\n2\n79\n57\nNaN\n...\n0\n1807\n0\n0\n0\n0\n87 YOF RESIDENT AT ECF LOST BALANCE AND FELL B...\nV\n95\n17.2223\n\n\n22886\n220371574\n1/28/2022\n84\n1\n1\nNaN\n2\n75\n62\nNaN\n...\n0\n1807\n0\n0\n0\n0\n84 YOM RESIDENT AT ECF FELL ON THE FLOOR. DX: ...\nV\n95\n17.2223\n\n\n47110\n220505221\n2/24/2022\n71\n1\n1\nNaN\n2\n76\n57\nNaN\n...\n0\n1807\n0\n0\n0\n0\n71 YOM RESIDENT AT ECF TRIPPED AND FELL ON THE...\nV\n95\n17.2223\n\n\n46661\n220432778\n2/12/2022\n95\n2\n1\nNaN\n2\n75\n62\nNaN\n...\n0\n1807\n0\n0\n0\n0\n95 YOF RESIDENT AT ECF FELL ON THE FLOOR. DX: ...\nV\n95\n17.2223\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\nAsymmetrical queries\nGiven we have done zero fine tuning on either the embedding model or the cross encoder, the results are are pretty good. However, a notable weakness of this current approach is that the model is not robust for asymmetrical queries - that is, queries which are much shorter than the optimal one in the corpus. For example, let’s say we just wanted to find a case where an elderly person fell in a bathtub. Here I just type in a manual example:\n\nshort_query = \"80YOM SLIPPED AND FELL IN BATHTUB\"\n\n_, _, output = ranker.query(process_text(short_query ), number_results=5)\n\noutput\n\n['16 YEAR OLD MALE SLIPPED AND FELL GETTING OUT OF BATHTUB. DX CONCUSSION',\n '30 YEAR OLD MALE FELL IN BATHTUB DX; BACK CONTUSION',\n '62 YEAR OLD MALE SLIPPED AND FELL IN THE SHOWER. DX:CERVICAL STRAIN.',\n '61 YEAR OLD MALE SLIPPED AND FELL GETTING OUT OF SHOWER DX; R ANKLE FRACTURE',\n '71 YEAR OLD MALE FELL IN SHOWER DX NECK PAIN']\n\n\nThe results here are ok (they give us cases involving slip and falls in a bathtub) but we’ll note they are similarly short to the input query. For example, here is another narrative involving an elderly fall in the bathtub, but it is ranked much lower because its length and structure are asymmetrical to the input:\n\n“75YOM PT HAULING FIREWOOD 3 WKS AGO; DEVELOPED BACK PAIN. 2 NIGHTS AGO SLIPPED & FELL IN BATHTUB, COULDLN’T GET UP UNTIL MORNING WITH NEIGHBOR’S HELP DX: LOW BACK PAIN, SHINGLES, ELEVATED LIVER FUNCTION TESTS #”\n\nThis is because the retreveal model matches close to queries with similar lengths. In the case of a true querying model, we need to map questions to positive and negative inputs. Fine-tuning the cross-encoder could help improve this, although it is a time-consuming process. What I wanted to demonstrate here is not a “true” RAG search model, but more of a semantic search and retreval model. In the latter approach the model expects to see a more context-rich example to use for document retreval."
  }
]